---
title: "Schätzverfahren"
weight: 305
tags: ["schaetzung", "regression", "lernvideo"]
bibliography: [../../static/files/publications.bib]
suppress-bibliography: true
link-citations: true
# nocite: "@*" um einfach alle Angaben zu printen am Ende
csl: [../../static/files/apa.csl]
---



<p>{{% buttonGit href="https://gitlab.ub.uni-giessen.de/methoden-politik/einstieg-in-statistik/issues/new?issue[title]=" icon="fas fa-bug" %}} {{% /buttonGit %}}</p>
<p>{{% buttonGit href="mailto:?subject=Schau%20dir%20das%20mal%20an%3A%20" icon="fas fa-paper-plane" %}} {{% /buttonGit %}}</p>
<p>{{% buttonGit href="https://t.me/share/url?url=" icon="fab fa-telegram" %}} {{% /buttonGit %}}</p>
<p>{{% buttonGit href="https://api.whatsapp.com/send?text=" icon="fab fa-whatsapp" %}} {{% /buttonGit %}}</p>
<p>{{% buttonGit href="https://twitter.com/share?url=" icon="fab fa-twitter" %}} {{% /buttonGit %}}</p>
<p>{{% buttonGit href="https://www.facebook.com/sharer/sharer.php?u=" icon="fab fa-facebook" %}} {{% /buttonGit %}}</p>
<p>In diesem Lernvideo werden die relevanten Teile der Regressionsberechnung an einem weiteren Beispiel vereinfacht dargestellt und das mathematische Vorgehen der Berechnung der besten Regressionsgeraden kurz erläutert. Weitergehende Ausführungen sind unterhalb des Lernvideos im Text zu finden.</p>
<center>
<iframe src="https://player.vimeo.com/video/506077999?quality=720p&amp;dnt=1" width="640" height="360" frameborder="0" allow="autoplay; fullscreen; picture-in-picture" allowfullscreen referrerpolicy="noreferrer">
</iframe>
</center>
<p>Kommen wir zurück zur Modellschätzung. Die Gleichung einer Modellschätzung lautet:
<span class="math inline">\(Y = \beta_0 + \beta_1 \ast X + e\)</span>
Mit <span class="math inline">\(Y = (y_1, y_2, ..., y_n), X = (x_1, x_2, ..., x_n)\)</span> und <span class="math inline">\(e=(e_1, e_2, ..., e_n)\)</span>.</p>
<p>Dabei wird die Regressionsgerade über folgende Gleichung berechnet:</p>
<p><span class="math inline">\(\hat{Y} = \hat{\beta}_0 + \hat{\beta}_1 \ast X\)</span>, mit <span class="math inline">\(\hat{Y} = (\hat{y}_1, \hat{y}_2, ..., \hat{y}_n), X = (x_1, x_2, ..., x_n)\)</span></p>
<p>Um nun die beste Regressionsgerade zu finden, gibt es für die Schätzung einer Regressionsgerade verschiedene Verfahren. Für die <a href="../../glossar/lineareregression/index.html">lineare Regression</a> wird das <a href="../../glossar/ols/index.html">Ordinary-Least-Squares(OLS)-Verfahren</a> angewendet, dass nach dem Gauss-Markov-Theorem die <em>best linear unbiased estimation</em> (BLUE) ist. Für eine genauere Auseinandersetzung mit diesem Verfahren empfehlen wir folgende Literatur: <span class="citation">Urban &amp; Mayerl (<a href="#ref-Urban2011">2011</a>, Kapitel 3.1.2 BLUE-Schätzer)</span>.</p>
<p>Mit Rückbezug zur Grafik können wir im Beispiel der Regression von <em>Zufriedenheit mit der Demokratie</em> leicht verstehen, wie das Schätzverfahren <a href="../../glossar/ols/index.html">OLS</a> die beste Regressionsgerade berechnet.</p>
<div class="float">
<img src="../images/regplot2.png" alt="Residuen in der linearen Regression" />
<div class="figcaption">Residuen in der linearen Regression</div>
</div>
<p>Wie vorhin festgestellt, verursacht die Rechnung der Regressionsgerade Abweichungen der beobachteten Werte von den geschätzten Werten. In der Grafik stellen die blauen Punkte den gemessen <span class="math inline">\(y\)</span>-Wert dar und die roten Punkte den geschätzten <span class="math inline">\(y\)</span>-Wert (<span class="math inline">\(\hat{y}\)</span>). Der Abstand zwischen dem beobachteten blauen Punkt (<span class="math inline">\(y_i\)</span>) der Daten und den geschätzten roten Punkt (<span class="math inline">\(\hat{y}_i\)</span>) auf der Regressionsgerade ist der Wert des <a href="../../glossar/residuum/index.html">Residuums</a> (<span class="math inline">\(e_i\)</span>) des jeweiligen Falls <span class="math inline">\(i\)</span>. Die Summe der Residuen beinhaltet sozusagen den Anteil, der nicht über die Steigung von <span class="math inline">\(x\)</span> erklärt werden kann (<a href="../../glossar/varianz/index.html">Varianz</a>). Das OLS-Verfahren berechnet die beste Regressionsgerade über die kleinsten quadrierten Abstände von <span class="math inline">\(y\)</span> und <span class="math inline">\(\hat{y}\)</span> und minimiert diese. Das Ziel ist es, die Gerade zu finden, die die quadratischen Abstände zwischen den geschätzten (rote Punkte) und beobachteten (blaue Punkte) Werten minimiert. Also die Gerade, die durchschnittlich am wenigstens von den beobachteten Werten abweicht.</p>
<p>Für die einfache lineare Regression lautet der mathematische Ausdruck wie folgt:
OLS: <span class="math inline">\(\sum\limits_{i=1}^n e^2_i = \sum\limits_{i=1}^n (y_i - \hat{y}_i)^2 = \sum\limits_{i=1}^n (y_i - \beta_0 - \beta_i \ast x_i)^2 \rightarrow min.!\)</span></p>
<p>Steigung (slope): <span class="math inline">\(\hat{\beta}_1 = \frac{\sum_{i=1}^n {(x_i - \bar{x})(y_i - \bar{y})}}{\sum_{i=1}^n {(x_i-\bar{x})^2}}\)</span></p>
<p>Konstante (intercept): <span class="math inline">\(\hat{\beta}_0 = \bar{y} - \hat{\beta_1} \ast \bar{x}\)</span></p>
<p>Das OLS-Verfahren hat verschiedene Bedingungen, die hier nur kurz genannt werden und in der angegebenen Literatur vertieft werden können. Bei der Durchführung einer linearen Regression müssen diese kontrolliert werden.
Damit das OLS-Verfahren wirklich ein <em>blue</em>-Schätzer ist, müssen drei Bedingungen für die Residuen erfüllt sein:</p>
<ol style="list-style-type: decimal">
<li><p>Homoskedastizität <span class="math inline">\((VAR(\varepsilon_i)=\delta^2)\)</span></p></li>
<li><p><span class="math inline">\(E(\varepsilon_i )=0\)</span></p></li>
<li><p>Autokorrelation <span class="math inline">\(cov(\varepsilon_i; \varepsilon_j )=0\)</span> mit <span class="math inline">\(i \neq j\)</span></p></li>
</ol>
<p>Für die <a href="../../glossar/lineareregression/index.html">lineare Regressionsrechnung</a> gilt, dass es sich um eine einmalige Schätzung handelt und bei Erfüllung dieser Bedingungen das OLS-Verfahren als blue-Schätzer gilt. Diese Aussagen nach dem Gauss-Markov-Prinzip beziehen sich auf die <a href="../../glossar/stoervariable/index.html">Störvariable(n)</a> <span class="math inline">\(\varepsilon\)</span>, die in der <a href="../../glossar/grundgesamtheit/index.html">Grundgesamtheit</a> vorliegt/vorliegen. In der einzelnen <a href="../../glossar/stichprobe/index.html">Stichprobe</a> wird deshalb das Residuum (<span class="math inline">\(e\)</span>) überprüft. Zum einen (1) müssen die Varianzen der Residuen <a href="../../glossar/homoskedastizitaet/index.html">homoskedastisch</a> sein, also dürfen nicht sehr stark zu- oder abnehmen. Ebenfalls (2) muss der <a href="../../glossar/erwartungswert/index.html">Erwartungswert</a> des Fehlers einer Beobachtung (<span class="math inline">\(i\)</span>) 0 sein. Das bedeutet, dass im Mittel der Fehler für die Regressionsrechnung quasi wegfällt. Drittens (3) dürfen die Fehler einer Beobachtung (<span class="math inline">\(i\)</span>) nicht mit den Fehlern einer weiteren Beobachtung (<span class="math inline">\(j\)</span>) korrelieren. Die Überprüfung der Modellannahmen erfolgt im multivariaten Modell über die Residualanalyse (siehe dazu <span class="citation">Urban &amp; Mayerl (<a href="#ref-Urban2011">2011</a>, Kapitel 4)</span>).</p>
<p>Als zusätzliche Voraussetzung kommt in der <a href="../../glossar/lineareregression/index.html">linearen Regression</a> hinzu, dass wir mit dem Rückgriff auf die <a href="../../glossar/inferenzstatistik/index.html">Inferenzstatistik</a> von einer <a href="../../glossar/normalverteilung/index.html">Normalverteilung</a> der Störterme ausgehen müssen.</p>
<p>Normalverteilung <span class="math inline">\(\varepsilon \sim \mathcal{N}(0, \, \sigma^2)\)</span></p>
<p>Andernfalls können die statistischen Tests keine korrekten Ergebnisse liefern. In der Analyse bedeutet dies, dass man auch die Normalverteilung der Residuen der spezifischen Regressionsrechnung überprüfen muss.</p>

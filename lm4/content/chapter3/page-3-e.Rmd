---
title: "Schätzverfahren"
weight: 305
tags: ["schaetzung", "regression", "lernvideo"]
bibliography: [../../static/files/publications.bib]
suppress-bibliography: true
link-citations: true
# nocite: "@*" um einfach alle Angaben zu printen am Ende
csl: [../../static/files/apa.csl]
---

{{% buttonShare href=\"https\:\/\/gitlab.ub.uni-giessen.de\/methoden-politik\/einstieg-in-statistik\/issues\/new?issue[title]=\" icon=\"fas fa-bug\" %}} {{% /buttonShare %}} 

{{% buttonShare href=\"mailto\:?subject=Schau\%20dir\%20das\%20mal\%20an\%3A\%20\" icon=\"fas fa-paper-plane\" %}} {{% /buttonShare %}}

{{% buttonShare href=\"https\:\/\/t.me\/share\/url?url=\" icon=\"fab fa-telegram\" %}} {{% /buttonShare %}}

{{% buttonShare href=\"https\:\/\/api.whatsapp.com\/send?text=\" icon=\"fab fa-whatsapp\" %}} {{% /buttonShare %}}

{{% buttonShare href=\"https\:\/\/twitter.com\/share?url=\" icon=\"fab fa-x-twitter\" %}} {{% /buttonShare %}}

{{% buttonShare href=\"https\:\/\/www.facebook.com\/sharer\/sharer.php?u=\" icon=\"fab fa-facebook\" %}} {{% /buttonShare %}}

{{% button href=\"https\:\/\/bmc.link\/bpkleerw\" icon=\"fa-solid fa-beer-mug-empty\" %}} {{% /button %}}

In diesem Lernvideo werden die relevanten Teile der Regressionsberechnung an einem weiteren Beispiel vereinfacht dargestellt und das mathematische Vorgehen der Berechnung der besten Regressionsgeraden kurz erläutert. Weitergehende Ausführungen sind unterhalb des Lernvideos im Text zu finden.

<center>
<iframe src="https://player.vimeo.com/video/506077999?quality=720p&dnt=1" width="640" height="360" frameborder="0" allow="autoplay; fullscreen; picture-in-picture" allowfullscreen referrerpolicy="noreferrer"></iframe>
</center>

Kommen wir zurück zur Modellschätzung. Die Gleichung einer Modellschätzung lautet:
$Y = \beta_0 + \beta_1 \ast X + e$
Mit $Y = (y_1, y_2, ..., y_n), X = (x_1, x_2, ..., x_n)$ und $e=(e_1, e_2, ..., e_n)$.

Dabei wird die Regressionsgerade über folgende Gleichung berechnet:

$\hat{Y} = \hat{\beta}_0 + \hat{\beta}_1 \ast X$, mit $\hat{Y} = (\hat{y}_1, \hat{y}_2, ..., \hat{y}_n), X = (x_1, x_2, ..., x_n)$

Um nun die beste Regressionsgerade zu finden, gibt es für die Schätzung einer Regressionsgerade verschiedene Verfahren. Für die [lineare Regression](../../glossar/lineareregression/index.html) wird das [Ordinary-Least-Squares(OLS)-Verfahren](../../glossar/ols/index.html) angewendet, dass nach dem Gauss-Markov-Theorem die *best linear unbiased estimation* (BLUE) ist. Für eine genauere Auseinandersetzung mit diesem Verfahren empfehlen wir folgende Literatur: @Urban2011 [Kapitel 3.1.2 BLUE-Schätzer].

Mit Rückbezug zur Grafik können wir im Beispiel der Regression von *Zufriedenheit mit der Demokratie* leicht verstehen, wie das Schätzverfahren [OLS](../../glossar/ols/index.html) die beste Regressionsgerade berechnet.

![Residuen in der linearen Regression](../images/regplot2.png)

Wie vorhin festgestellt, verursacht die Rechnung der Regressionsgerade Abweichungen der beobachteten Werte von den geschätzten Werten. In der Grafik stellen die blauen Punkte den gemessen $y$-Wert dar und die roten Punkte den geschätzten $y$-Wert ($\hat{y}$). Der Abstand zwischen dem beobachteten blauen Punkt ($y_i$) der Daten und den geschätzten roten Punkt ($\hat{y}_i$) auf der Regressionsgerade ist der Wert des [Residuums](../../glossar/residuum/index.html) ($e_i$) des jeweiligen Falls $i$. Die Summe der Residuen beinhaltet sozusagen den Anteil, der nicht über die Steigung von $x$ erklärt werden kann ([Varianz](../../glossar/varianz/index.html)). Das OLS-Verfahren berechnet die beste Regressionsgerade über die kleinsten quadrierten Abstände von $y$ und $\hat{y}$ und minimiert diese. Das Ziel ist es, die Gerade zu finden, die die quadratischen Abstände zwischen den geschätzten (rote Punkte) und beobachteten (blaue Punkte) Werten minimiert. Also die Gerade, die durchschnittlich am wenigstens von den beobachteten Werten abweicht.

Für die einfache lineare Regression lautet der mathematische Ausdruck wie folgt:
OLS: $\sum\limits_{i=1}^n e^2_i = \sum\limits_{i=1}^n (y_i - \hat{y}_i)^2 = \sum\limits_{i=1}^n (y_i - \beta_0 - \beta_i \ast x_i)^2 \rightarrow min.!$

Steigung (slope): $\hat{\beta}_1 = \frac{\sum_{i=1}^n {(x_i - \bar{x})(y_i - \bar{y})}}{\sum_{i=1}^n {(x_i-\bar{x})^2}}$

Konstante (intercept): $\hat{\beta}_0 = \bar{y} - \hat{\beta_1} \ast \bar{x}$

Das OLS-Verfahren hat verschiedene Bedingungen, die hier nur kurz genannt werden und in der angegebenen Literatur vertieft werden können. Bei der Durchführung einer linearen Regression müssen diese kontrolliert werden.
Damit das OLS-Verfahren wirklich ein *blue*-Schätzer ist, müssen drei Bedingungen für die Residuen erfüllt sein:

1. Homoskedastizität $(VAR(\varepsilon_i)=\delta^2)$

2. $E(\varepsilon_i )=0$

3. Autokorrelation $cov(\varepsilon_i; \varepsilon_j )=0$ mit $i \neq j$

Für die [lineare Regressionsrechnung](../../glossar/lineareregression/index.html) gilt, dass es sich um eine einmalige Schätzung handelt und bei Erfüllung dieser Bedingungen das OLS-Verfahren als blue-Schätzer gilt. Diese Aussagen nach dem Gauss-Markov-Prinzip beziehen sich auf die [Störvariable(n)](../../glossar/stoervariable/index.html) $\varepsilon$, die in der [Grundgesamtheit](../../glossar/grundgesamtheit/index.html) vorliegt/vorliegen. In der einzelnen [Stichprobe](../../glossar/stichprobe/index.html) wird deshalb das Residuum ($e$) überprüft. Zum einen (1) müssen die Varianzen der Residuen [homoskedastisch](../../glossar/homoskedastizitaet/index.html) sein, also dürfen nicht sehr stark zu- oder abnehmen. Ebenfalls (2) muss der [Erwartungswert](../../glossar/erwartungswert/index.html) des Fehlers einer Beobachtung ($i$) 0 sein. Das bedeutet, dass im Mittel der Fehler für die Regressionsrechnung quasi wegfällt. Drittens (3) dürfen die Fehler einer Beobachtung ($i$) nicht mit den Fehlern einer weiteren Beobachtung ($j$) korrelieren. Die Überprüfung der Modellannahmen erfolgt im multivariaten Modell über die Residualanalyse (siehe dazu @Urban2011 [Kapitel 4]).

Als zusätzliche Voraussetzung kommt in der [linearen Regression](../../glossar/lineareregression/index.html) hinzu, dass wir mit dem Rückgriff auf die [Inferenzstatistik](../../glossar/inferenzstatistik/index.html) von einer [Normalverteilung](../../glossar/normalverteilung/index.html) der Störterme ausgehen müssen.

Normalverteilung $\varepsilon \sim \mathcal{N}(0, \, \sigma^2)$

Andernfalls können die statistischen Tests keine korrekten Ergebnisse liefern. In der Analyse bedeutet dies, dass man auch die Normalverteilung der Residuen der spezifischen Regressionsrechnung überprüfen muss.

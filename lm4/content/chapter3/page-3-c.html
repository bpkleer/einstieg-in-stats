---
title: "Bivariate Regression"
weight: 303
tags: ["regression", "bivariat"]
---



<p>{{% buttonGit href="https://gitlab.ub.uni-giessen.de/methoden-politik/einstieg-in-statistik/issues/new?issue[title]=" icon="fas fa-bug" %}} {{% /buttonGit %}}</p>
<p>{{% buttonGit href="mailto:?subject=Schau%20dir%20das%20mal%20an%3A%20" icon="fas fa-paper-plane" %}} {{% /buttonGit %}}</p>
<p>{{% buttonGit href="https://t.me/share/url?url=" icon="fab fa-telegram" %}} {{% /buttonGit %}}</p>
<p>{{% buttonGit href="https://api.whatsapp.com/send?text=" icon="fab fa-whatsapp" %}} {{% /buttonGit %}}</p>
<p>{{% buttonGit href="https://twitter.com/share?url=" icon="fab fa-twitter" %}} {{% /buttonGit %}}</p>
<p>{{% buttonGit href="https://www.facebook.com/sharer/sharer.php?u=" icon="fab fa-facebook" %}} {{% /buttonGit %}}</p>
<p>Bei der Berechnung einer <strong>linearen Regression</strong> in den Sozialwissenschaften lässt sich das Regressionsmodell nur mit einer Anpassung als lineare Darstellung durchführen. Dies liegt daran, dass keine <a href="../../glossar/abhaengigevariable/index.html">abhängige</a> Variable perfekt durch eine oder mehrere <a href="../../glossar/unabhaengigevariable/index.html">unabhängige</a> Variable(n) darstellbar ist. In der Modellannahme wird deshalb davon ausgegangen, dass es einen nicht-erklärbaren Anteil gibt, der mit der <a href="../../glossar/stoervariable/index.html">Störvariable</a> <span class="math inline">\(\varepsilon\)</span> (manchmal auch <span class="math inline">\(U\)</span>) angegeben wird. In den Sozialwissenschaften ergibt sich diese Störvariable aus der Forschungslogik heraus: Wir können in den Sozialwissenschaften niemals alle Variablen, die auf etwas wirken, berücksichtigen, da ein soziales Ereignis stets umfassend ist.</p>
<p>Aus dieser Überlegung heraus ergibt sich die Anpassung einer linearen Gleichung zur bivariaten <strong>linearen Regression</strong> für die <a href="../../glossar/grundgesamtheit/index.html">Grundgesamtheit</a>:
<span class="math inline">\((1) \quad y_i = \beta_0 + \beta_1 \ast x_i + \varepsilon_i\)</span></p>
<p>Mit <span class="math inline">\(Y = (y_1, y_2, ..., y_n), \thinspace X = (x_1, x_2, ..., x_n) \thinspace und \thinspace \varepsilon = (\varepsilon_1, \varepsilon_2, ..., \varepsilon_n)\)</span></p>
<p>Geschätzt werden im Modell die Regressionskoeffizienten <span class="math inline">\(\beta_0\)</span> und <span class="math inline">\(\beta_1\)</span> (bzw. weitere Regressionskoeffizienten im multivariaten Modell, dazu später mehr). Diese stellen mit den beobachteten Werten der unabhängigen Variablen die <strong>systematische Komponente</strong> dar. Die Störvariable <span class="math inline">\(\varepsilon\)</span> stellt die <strong>stochastische Komponente</strong> dar. Um die empirische Beobachtung der Wertepaare der zwei Variablen <span class="math inline">\(Y\)</span> und <span class="math inline">\(X\)</span> darzustellen, wird diese stochastische Komponente benötigt. Denn, wie oben bereits erwähnt, lassen sich niemals eine perfekt lineare Beziehungen in den Sozialwissenschaften darstellen. Die Störvariable <span class="math inline">\(\varepsilon\)</span> ist nicht beobachtbar und nicht messbar. <span class="math inline">\(\varepsilon\)</span> kann man sich inhaltlich als die Gesamtheit der nicht berücksichtigten Variablen vorstellen.</p>
<p>In der <a href="../../glossar/stichprobe/index.html">Stichprobe</a>, mit dessen Variablen und Beobachtungen wir die <strong>lineare Regression</strong> berechnen, können wir die Störvariable(n) nicht messen. Es können nie perfekte Beobachtungen entstehen: Die beobachteten Werte von <span class="math inline">\(y\)</span> (aus der Stichprobe) weichen von den durch die Regressionsgerade geschätzten Werten (<span class="math inline">\(\hat{y}\)</span>) ab. Diese Differenzen bezeichnet man im Modell (bzw. der Stichprobe) als <a href="../../glossar/residuum/index.html">Residuen</a> und sie werden mathematisch im Term <span class="math inline">\(e\)</span> berücksichtigt. Dies ist als die Differenz von beobachteten <span class="math inline">\(y\)</span>-Werten und geschätzten <span class="math inline">\(\hat{y}\)</span>-Werten zu verstehen und ist ein Nebenprodukt der Schätzung der Regressionskoeffizienten <span class="math inline">\(\hat{\beta}_0\)</span> und <span class="math inline">\(\hat{\beta}_1\)</span>.</p>
<p>Die Residuen im geschätzten Modell ergeben sich also wie folgt:</p>
<p><span class="math inline">\(e_i = y_i - \hat{y}_i = y_i - \hat{\beta}_0 - \hat{\beta}_1 * x_1\)</span></p>
<p>Für die geschätzte Regressionsrechnung einer Stichprobe müssen wir theoretisch also zwischen zwei Gleichungen unterscheiden: Einmal die Schätzung der Regressionsgleichung (später mehr zum Verfahren der Schätzung) und einmal die Modellgleichung.</p>
<p>Modellgleichungen der linearen Regressionen sehen in aller Regel wie folgt aus:</p>
<p><span class="math inline">\((2) \quad y_i = \beta_0 + \beta_1 \ast x_i + e_i\)</span></p>
<p>Mit <span class="math inline">\(Y = (y_1, y_2, ..., y_n), \thinspace X = (x_1, x_2, ..., x_n) \thinspace und \thinspace e= (e_1, e_2, ..., e_n) \thinspace bzw. \thinspace e_i=\hat{y}_i - y_i\)</span></p>
<p>Es wird der beobachtete Wert von <span class="math inline">\(Y\)</span> über die Konstante und die Steigerung der beobachteten unabhängigen Variablen (<span class="math inline">\(X\)</span>) errechnet. Da sich diese Gleichung in aller Regel nicht perfekt linear darstellt, wird über das Residuum (<span class="math inline">\(e\)</span>) ein Anpassungsterm errechnet.</p>
<p>Für die geschätzte Regressionsgleichung, bei der für <span class="math inline">\(Y\)</span> Werte geschätzt werden, sieht die Formel wie folgt aus:
<span class="math inline">\((3) \quad \hat{y_i} = \hat{\beta}_0 + \hat{\beta}_1 \ast x_i\)</span></p>
<p>Dieses Modell gilt mathematisch nur unter der Annahme, dass <span class="math inline">\(\hat{Y}\)</span> allein durch <span class="math inline">\(X\)</span> beeinflusst wird.</p>
<p>Die Schätzung der Regressionskoeffizienten <span class="math inline">\(\hat{\beta}_0\)</span> und <span class="math inline">\(\hat{\beta}_1\)</span> erfolgt über die beobachteten Werte von <span class="math inline">\(X\)</span> und <span class="math inline">\(Y\)</span>:</p>
<p><span class="math inline">\(\hat{\beta}_1\)</span> wird über die Differenz zum Mittelwert der beobachten Variablen berechnet: <span class="math inline">\(\hat{\beta}_1 = \frac{\sum_{i=1}^n {(x_i - \bar{x})(y_i - \bar{y})}}{\sum_{i=1}^n {(x_i-\bar{x})^2}}\)</span></p>
<p><span class="math inline">\(\hat{\beta}_0\)</span> wird dann unter Zuhilfenahme der Berechnung von <span class="math inline">\(\hat{\beta}_1\)</span> berechnet: <span class="math inline">\(\hat{\beta}_0 = \bar{y} - \hat{\beta}_1\bar{x} = \bar{y} - \frac{\sum_{i=1}^n {(x_i - \bar{x})(y_i - \bar{y})}}{\sum_{i=1}^n {(x_i-\bar{x})^2}}\)</span></p>
<p>Aus diesem Schätzmodell können dann geschätzte Werte von <span class="math inline">\(Y\)</span> berechnet werden. Ebenso können so die einzelnen Residuen der jeweiligen beobachteten Wertepaare von <span class="math inline">\(X\)</span> und <span class="math inline">\(Y\)</span> berechnet werden. Zur Erinnerung: Die Residuen in der Modellgleichung geben die Differenz zwischen beobachtetem <span class="math inline">\(y_i\)</span>-Wert und dem geschätztem <span class="math inline">\(\hat{y}_i\)</span>-Wert an (<span class="math inline">\(e_i=\hat{y}_i - y_i\)</span>).</p>
<p>Mithilfe dieser Gleichung (3) errechnen Statistikprogramme die beste Gerade unter Berücksichtigung des Residuums aus der vorherigen Gleichung (2). Das Ziel der Regressionsrechnung ist es, die Gerade zu finden, bei der das Residuum e minimal ist.</p>

[
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/chapter1/",
	"title": "Signifikanz-/Hypothesentests",
	"tags": [],
	"description": "",
	"content": "Kapitel 1 Signifikanz-/Hyposentests In diesem Teil des Web-Based-Trainings werden Signifikanz- bzw. Hypothesentests vorgestellt. Diese sind für die quantitative empirische Sozialforschung von besonderer Bedeutung, da nicht nur Aussagen über die Stichprobe geschlossen werden sollen, sondern auch über die Grundgesamtheit.\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/chapter1/page-1-a/",
	"title": "Einführung",
	"tags": ["signifikanz", "hypothesentest"],
	"description": "",
	"content": " Die Ergebnisse der Korrelationsberechnungen können wir bisher nur auf die Stichprobe beziehen.\nWir wissen also in unserem Beispiel der Pearson-Korrelation, dass die die Präferenz für die CDU mit dem Alter einer Person positiv korreliert (\\(r=0.97\\)). Es handelt sich hierbei um eine sehr starke (fast perfekte) lineare Korrelation.\nNun soll aber in den Sozialwissenschaften nicht nur eine Aussage über eine (Zufalls-)Stichprobe getroffen werden, sondern die Aussage soll auf die Grundgesamtheit übertragen werden. Deshalb werden in den Sozialwissenschaften nicht willkürliche oder bewusste Stichproben gezogen, sondern durch zufallsbasierte Verfahren Stichproben erhoben, um mithilfe der Wahrscheinlichkeitsrechnung Rückschlüsse auf die Grundgesamtheit führen zu können.\nUm nun den Inferenzschluss auf die Grundgesamtheit treffen zu können, müssen die Ergebnisse durch einen Hypothesentest auf Signifikanz überprüft werden. Deshalb werden diese Hypothesentests auch Signifikanztests genannt. In einem solchen Hypothesentest testen wir, wie wahrscheinlich ein angenommener Wert des Zusammenhangs zwischen zwei Variablen ist. Also mathematisch gesprochen, ob ein Effekt in der Stichprobe zufällig entstanden sein kann und dieser in der Grundgesamtheit gar nicht vorliegt.\nStatistische Testverfahren sind daher grundlegende Beurteilungsverfahren in der quantitativen Datenanalyse (Inferenzstatistik). Statistische Tests werden benutzt, um die Wahrscheinlichkeiten anzugeben, dass die gesichtete Beobachtung (hier im Beispiel die ermittelte Korrelation zwischen Alter und Präferenz für die CDU) der Stichprobe auch wahrscheinlich in der Grundgesamtheit vorzufinden ist. Für das weitere Vorgehen nehmen wir nun an, dass unsere Stichprobe zufallsbasiert ausgewählt wurde und ausreichend groß ist. Um das Verständnis zu erleichtern, wurde eine kleine Datenmenge ausgewählt.\nUm einen statistischen Test zu berechnen, muss zuerst eine Nullhypothese (\\(H_0\\)) sowie ihre Alternativhypothese (\\(H_A\\)) gebildet werden.\nDiese Hypothesen sind Annahmen über die Verteilung einer Variable auf Basis der Stichprobe. In den Sozialwissenschaften wird die Nullhypothese meist als Punkthypothese verwendet: Der vermutete Zusammenhang/die vermutete Ausprägung wird als nicht vorhanden formuliert.\nDie Alternativhypothese wird dann als Hypothese über die/den vermutete/n Differenz/Zusammenhang betrachtet. Dabei kann die Alternativhypothese als \\(\\neq H_0\\) formuliert sein oder als \\(\u0026gt;H_0 / \u0026lt;H_0\\). In Abhängigkeit der Formulierung der Alternativhypothese wird entweder ein einseitiger Test oder ein zweiseitiger Test berechnet. Es wird getestet, ob eine gewisse Aussage zutrifft.\nJe nach Verfahren und Verteilung einer Variablen können verschiedene Testverteilungen genutzt werden: zum Beispiel die Normalverteilung, t-Verteilung oder z-Verteilung. Das Verfahren zum Prüfen ist dabei immer gleich und wird exemplarisch an der Normalverteilung vorgestellt.\nEs wird nun zuerst der Unterschied zwischen einem einseitigen und zweiseitigen Test dargestellt, bevor die Begriffe der Irrtumswahrscheinlichkeit und Vertrauenswahrscheinlichkeit erläutert werden. Anschließend wird ein Anwendungsbeispiel berechnet.\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/chapter1/page-1-b/",
	"title": "Testsituationen",
	"tags": ["testsituation", "signifikanz", "hypothesentest"],
	"description": "",
	"content": " Nehmen wir folgendes Beispiel zur Formulierung einer Null- und Alternativhypothese: Wir möchten testen, ob das mittlere Einkommen von Männern in einer Stichprobe größer als 2300 Euro ist. Zur Erinnerung der Erwartungswert wird immer mit \\(\\mu\\) angegeben.\nDiese Aussage formulieren wir in einer Alternativhypothese, die wie folgt lautet: \\(H_A: \\mu \u0026gt; 2300\\)\nIn der Nullhypothese nehmen wir an, dass die vermutete Beobachtung nicht zutrifft. Aus unserer getroffenen Alternativhypothese ergibt sich die Nullhypothese: \\(H_0: \\mu ≤ 2300\\)\nWie in der Formulierung sichtbar, testen wir gerichtet bzw. einseitig, da wir nicht wissen wollen, ob das mittlere Einkommen größer oder kleiner als (also ungleich von) 2300 Euro ist, sondern, ob das mittlere Einkommen von Männern größer als 2300 Euro ist.\nFür einen zweiseitigen Test formulieren wir die Alternativhypothese wie folgt um: \\(H_A: \\mu \\neq 2300\\)\nWir möchten bei einem zweiseitigen Test prüfen, ob das mittlere Einkommen von Männern ungleich 2.300 Euro ist. Daraus ergibt sich wieder die Nullhypothese: \\(H_0: \\mu = 2300\\)\nDie Nullhypothese wird also falsifiziert, wenn das mittlere Einkommen kleiner als 2300 Euro ist oder wenn das mittlere Einkommen größer als 2300 Euro ist.\nDie formale Formulierung zweiseitiger (ungerichteter) Hypothesen ist immer gleich und wie folgt: \\[H_0: \\mu = angenommener \\thinspace Wert\\]\n\\[H_A: \\mu \\neq angenommener \\thinspace Wert\\]\nDie formale Formulierung einseitiger (gerichteter) Hypothesen ist wie folgt:\nBei einem linksseitigen Test: \\[H_0: \\mu \\geq angenommener \\thinspace Wert\\]\n\\[H_A: \\mu \u0026lt; angenommener \\thinspace Wert\\]\nUnd bei einem rechtsseitigen Test: \\[H_0: \\mu \\leq angenommener \\thinspace Wert\\]\n\\[H_A: \\mu \u0026gt; angenommener \\thinspace Wert\\]\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/chapter1/page-1-c/",
	"title": "Irrtumswahrscheinlichkeit",
	"tags": ["irrtumswahrscheinlichkeit", "signifikanz", "hypothesentest"],
	"description": "",
	"content": " Um einen Hypothesentest bewerten zu können, wird eine Irrtumswahrscheinlichkeit (\\(\\alpha\\)) angegeben, auf die der Test signifikant ist oder nicht. Um dies zu verstehen, müssen wir uns nochmals kurz verdeutlichen, welche Testsituationen auftreten können. Anhand der Vierfelder-Tabelle sind die Testsituationen dargestellt:\nFehler 1. Art und Fehler 2. Art Es gibt zwei Situationen, in denen wir korrekte Entscheidungen treffen. Wenn wir uns für die Nullhypothese entscheiden und diese auch tatsächlich korrekt ist und wenn wir uns für die Alternativhypothese entscheiden und diese auch tatsächlich korrekt ist.\nNeben diesen zwei korrekten Feldern, gibt es aber auch zwei Fehler, die in der Entscheidung auftreten können. So können wir uns für die Alternativhypothese entscheiden, aber tatsächlich ist die Nullhypothese korrekt. Dies bezeichnet man als Fehler 1. Art oder auch \\(\\alpha\\)-Fehler. Entscheiden wir uns für die Nullhypothese und die Alternativhypothese ist korrekt, so begehen wir einen Fehler 2. Art oder auch \\(\\beta\\)-Fehler.\nDie Höhe des \\(\\alpha\\)-Fehler (Fehler 1. Art) gibt an, in wie vielen Fällen wir uns für die Alternativhypothese entscheiden, in Wahrheit aber die Nullhypothese korrekt ist. In den Sozialwissenschaften ist eine Irrtumswahrscheinlichkeit von \\(5\\%\\) oder \\(1\\%\\), je nach theoretischem Bezug, die Konvention.\nDer \\(\\beta\\)-Fehler (Fehler 2. Art) ist in den Sozialwissenschaften von geringerer Bedeutung, da die Signifikanztests so konstruiert werden, dass die Nullhypothese falsifiziert werden muss. Somit ist die Alternativhypothese die prüfende Größe und gegen einen Fehler in dieser Konstruktion sichert die Irrtumswahrscheinlichkeit vor einem Fehler 1. Art ab.\nIn den Sozialwissenschaften sichern wir unsere Ergebnisse i.d.R. auf einer Irrtumswahrscheinlichkeit von \\(5\\%\\) ab. Wir testen, ob das gewünschte Ergebnis mit \\(95\\%\\) Sicherheit nicht durch Zufall oder Artefakte entstanden ist. Bei einer Irrtumswahrscheinlichkeit von \\(5\\%\\) wird getestet, ob der beobachtete Wert nicht innerhalb eines \\(95\\%\\)-Intervalls um den erwarteten Wert liegt. Liegt er in diesem Intervall, ist die Nullhypothese beizubehalten. Sollte der Wert aber außerhalb dieses Intervalls liegen, ist die Nullhypothese abzulehnen und die Alternativhypothese anzunehmen. Das errechnete Ergebnis ist damit signifikant, also zu \\(95\\%\\) abgesichert.\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/chapter1/page-1-d/",
	"title": "Vertrauenswahrscheinlichkeit",
	"tags": ["vertrauenswahrscheinlichkeit", "signifikanz", "hypothesentest"],
	"description": "",
	"content": " Die Vertrauenswahrscheinlichkeit (\\(p = 1 − \\alpha\\)) gibt die Wahrscheinlichkeit an, dass das Intervall den wahren Wert enthält. Sollte der Wert also innerhalb dieses Intervalls liegen, muss die Nullhypothese beibehalten werden und die Alternativhypothese muss verworfen werden.\nZum besseren Verständnis hier eine grafische Darstellung eines ungerichteten (zweiseitigen) Hypothesentests:\nHypothesentest zweiseitig Zur Erinnerung: Die Fläche unterhalb des Grafens stellt die Wahrscheinlichkeit dar. Der blaue Bereich ist der Vertrauensbereich. Sollte die Prüfgröße innerhalb dieses Bereichs liegen, wird die Nullhypothese beibehalten. Erst wenn die Prüfgröße außerhalb dieser Grenzen (roter Bereich; auch kritischer Bereich genannt) liegt, ist die Nullhypothese zu verwerfen und die Alternativhypothese anzunehmen.\nBei einem zweiseitigen Test ist die Irrtumswahrscheinlichkeit von \\(5\\%\\) auf beide Enden aufgeteilt. Die Grenzen ergeben sich ganz einfach aus: \\(Grenze = \\frac{\\alpha}{2} = \\frac{0,05}{2} = 0,025\\). Wenn die Nullhypothese abgelehnt wird, liegt die Prüfgröße im Wahrscheinlichkeitsbereich von \\(p \\le 0.025\\) oder \\(p \\ge 0.975\\).\nBei einem gerichteten (einseitigen) Test ist entweder nur die rechte Seite (\\(p \\ge 0,95\\)) oder die linke Seite (\\(p \\le 0,05\\)) als kritischer Bereich anzusehen.\nSignifikanztest rechtsseitig Signifikanztest linksseitig "
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/chapter1/page-1-e/",
	"title": "Anwendung Korrelation",
	"tags": ["praxis", "signifikanz", "hypothesentest"],
	"description": "",
	"content": " Auch bei Korrelationen testet man die Signifikanz. Es wird also geprüft, ob der Zusammenhang in der Grundgesamtheit auch wahrscheinlich ist. In unserem vorherigen Teil haben wir die Ergebnisse noch nicht auf Signifikanz geprüft. Daher werden wir dies nun im Beispiel berechnen und durchführen.\nZur Erinnerung unser Beispiel aus der Berechnung der Pearson-Korrelation: Wir haben eine Stichprobe mit \\(5\\) Fällen, die jeweils das Alter und die Präferenz zur Partei CDU angegeben haben. Dies ist ein fiktives Beispiel, dass auf keiner Zufallsstichprobe basiert. Für die Einfachheit des Rechenbeispiels wird aber hier angenommen, dass es sich um eine ausreichend große Zufallsstichprobe für die Berechnung der Signifikanz handelt.\nFall(i) Alter (\\(x_i\\)) Wahlpräferenz CDU (\\(y_i\\)) 1 19 24 2 24 41 3 27 20 4 56 74 5 69 81 Für diese zwei Variablen haben wir die Pearson-Korrelation (hier) berechnet, die folgenden Wert ergab: \\(r \\approx 0.97\\)\nFür den Signifikanztest stellen wir nun eine Null- und Alternativhypothese auf.\nDie Nullhypothese des Signifikanztests lautet bei einer Korrelation immer: \\(H_0: r = 0\\)\nDie Alternativhypothese ist in der Regel ungerichtet. Es wird also keine konkrete Hypothese über die Höhe der Korrelation geprüft, sondern es wird nur die Signifikanz des Zusammenhangs geprüft.\nDie Alternativhypothese lautet daher: \\(H_A: r \\neq 0\\)\nAls Irrtumswahrscheinlichkeit nehmen wir den Standardwert von \\(\\alpha = 0.05 (\\Rightarrow p = 1 - \\frac{\\alpha}{2} = 0.975)\\) (zweiseitiger Test!).\nIm Fall der Korrelation wird die Signifikanz mithilfe einer bestimmten Verteilung getestet. Dafür wird eine Prüfgröße errechnet, die dann mit dem kritischen Wert einer Verteilung verglichen wird. Wenn die Prüfgröße größer als der kritische Wert ist, gilt das Ergebnis als signifikant. Für große Stichproben (\\(n\u0026gt;100\\)) kann eine Normalverteilung der Prüfgröße angenommen werden, ansonsten muss eine t-Verteilung angenommen werden.\nBerechnung Um bewerten zu können, ob unser Ergebnis signifikant ist, müssen wir folgendes berücksichtigen: Stichprobengröße: \\(n=5\\) Korrelationskoeffizient: \\(r=0.97\\)\nDa die Stichprobe (\\(n=5\\)) klein ist, wird eine \\(t\\)-Verteilung für die Verteilung der Prüfgröße angenommen. In einigen Fällen wird die Prüfgröße dann auch \\(t\\)−Wert genannt. Mathematisch wird dies meist mit \\(t_{emp}\\) oder \\(t_{krit}\\) angegeben.\nDie Prüfgröße des Korrelationsergebnisses berechnet sich nach folgender Formel:\n\\(t_{emp} = \\frac {|r|}{\\sqrt\\frac{1-r^2}{N-2}} = |r| \\ast \\sqrt \\frac{N-2}{1-r^2}\\)\nZur Erinnerung hier nochmal unsere Daten:\n\\(r \\approx 0.97\\)\n\\(N=5\\)\nSetzen wir unseren Daten nun in die Formel ein:\n\\(t_{emp} = \\frac {0.97} {\\sqrt \\frac{1 - 0.97^2}{5-2}} \\approx \\frac {0.97} {0.1404} \\approx 6,909\\)\nUnsere errechnete Prüfgröße ist also \\(t_{emp} \\approx 6.909\\).\nNun muss diese errechnete Prüfgröße mit dem kritischen Wert abgeglichen werden. Ist die Prüfgröße größer als der kritische Wert, gilt das Ergebnis als signifikant.\nDen kritischen Wert liest man anhand von Verteilungstabellen aus. Dazu benötigt man die angegebene Irrtumswahrscheinlichkeit (hier: \\(1-\\alpha=0.95\\)) und die Freiheitsgrade (degree of freedom).\nDie Freiheitsgerade (\\(\\nu\\)) errechnen sich wie folgt:\n\\(\\nu=n−1\\)\nIm Beispiel also:\n\\(\\nu=5−1=4\\)\nIn einer t-Verteilungs-Tabelle liest man den entsprechenden kritischen Wert aus. Hier ein Ausschnitt der Tabelle:\nt-Verteilungs-Tabelle Der kritische Wert beträgt an der genannten Stelle: \\(t_{krit_{(\\nu=4;p=0.975)}} = 2.776\\).\nDamit das errechnete Ergebnis der Signifikanz gilt, muss: \\(t_{emp} \u0026gt; t_{krit}\\)\nIm Beispiel ist dies erfüllt: \\(6.909 \u0026gt; 2.776\\)\nUnsere Prüfgröße ist größer als der kritische Wert und damit können wir die Nullhypothese ablehnen. Somit läge in diesem Beispiel ein gesicherter statistischer Zusammenhang zwischen den zwei Merkmalen vor. Wir könnten also das Ergebnis der Stichprobe auf die Grundgesamtheit übertragen. Voraussetzung dafür wäre aber, dass die Stichprobe zufallsbasiert erfolgt wäre.\nInterpretationsfehler Bei der Interpretation der Signifikanz sollten folgende Fehlinterpretationen allerdings vermieden werden (Schnell et al. 2013, S. 442):\nEin signifikantes Ergebnis bedeutet nicht, dass …\nes sich um einen „wichtigen“ Effekt handelt,\ndie „Existenz“ eines Effekts bewiesen ist,\nes sich um einen „starken“ Effekt handelt.\nOb ein Effekt theoretisch wichtig oder nicht ist, kann nicht durch ein mathematisches Verfahren bestimmt werden. Des Weiteren prüfen wir die Signifikanz immer unter einer Irrtumswahrscheinlichkeit, die in aller Regel dem \\(5\\%\\)-Signifikanzniveau entspricht. Dies bedeutet, dass wir in \\(5\\%\\) der Fälle einen nicht vorhandenen Effekt als signifikant einstufen. Die Existenz kann daher nicht bewiesen werden, da der Signifikanztest nicht zwischen tatsächlichen Effekt oder zufälligen Fehler bestimmen kann.\nWir können nur die Aussage treffen, dass wenn ein Wert signifikant ist, dieser bei wiederholten unabhängigen Untersuchungen sehr wahrscheinlich ist. Die Stärke des Effekts wird ebenfalls nicht durch die Signifikanz angegeben, sondern durch die Höhe des auf Signifikanz geprüften Wertes. Ein signifikanter Wert kann sehr klein, mittel oder sehr stark sein. Die Signifikanz allein lässt keine Aussage über die Stärke eines Effekts zu.\nSiehe auch: Bortz (2005, Kapitel 4.4); Schnell et al. (2013, Kapitel 9.4).\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/chapter1/subchapter1/",
	"title": "Konfidenzintervall",
	"tags": ["konfidenzintervall", "signifikanz", "hypothesentest"],
	"description": "",
	"content": "Konfidenzintervall In diesem Teil des Web-Based-Trainings werden Signifikanz- bzw. Hypothesentests vorgestellt. Diese sind für die quantitative empirische Sozialforschung von besonderer Bedeutung, da nicht nur Aussagen über die Stichprobe geschlossen werden sollen, sondern auch über die Grundgesamtheit.\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/chapter1/subchapter1/page-1-1-a/",
	"title": "Konfidenzintervall",
	"tags": ["konfidenzintervall", "signifikanz", "hypothesentest"],
	"description": "",
	"content": " Ein Konfidenzintervall bezeichnet in den Sozialwissenschaften ein Intervall, in dem ein erwarteter Wert \\(\\mu\\) sehr wahrscheinlich liegt. Wie wir aus der Normalverteilung wissen, bildet die Fläche unterhalb des Graphen die Wahrscheinlichkeit ab. Innerhalb dieser Flächen testen Hypothesentests, ob der angegebene Wert in diesem Intervall liegt. Für die Sozialwissenschaften hat das Intervall von [\\(\\mu−1.96\\sigma,\\mu + 1.96\\sigma\\)] hohe Relevanz. Dies ist das sogenannte 95%-Konfidenzintervall, da genau \\(95\\%\\) der Fälle in diesem Intervall liegen.\nZur Wiederholung: Wir wissen, dass der Mittelwert normalverteilt ist. D.h. in \\(95\\%\\) aller Stichproben ist das arithmetische Mittel (\\(\\bar{x}\\)) einer Stichprobe nicht mehr als \\(1.96\\) Standardabweichungen von \\(\\mu\\) entfernt. Ein Intervall von \\(1.96\\) Standardabweichungen um \\(\\mu\\) schließt mit einer Sicherheit von \\(95\\%\\) den \\(x\\)-Wert ein. Denn nur \\(2.5\\%\\) der Fläche liegen oberhalb bzw. unterhalb von \\(1.96\\) bzw. \\(−1.96\\) Standardabweichungen. Anders ausgedrückt, mit \\(5\\%\\)-Fehlerwahrscheinlichkeit kann angenommen werden, dass der \\(x\\)-Wert tatsächlich in diesem Intervall liegt. Als weiteres relevantes Intervall in den Sozialwissenschaften ist das Intervall \\([\\mu - 2.58\\sigma, \\mu + 2.58\\sigma]\\) zu nennen, da in diesem genau \\(99\\%\\) aller Fälle liegen (\\(99\\%\\)-Konfidenzintervall).\n\\(95\\%\\)-Konfidenzintervall Die verbleibenden \\(5\\%\\) werden als Irrtumswahrscheinlichkeit in den Sozialwissenschaften angegeben. Dies ist die Wahrscheinlichkeit, dass das Intervall \\(\\mu\\) nicht einschließt. Diese Irrtumswahrscheinlichkeit wird mit \\(\\alpha\\) bezeichnet. Für ein \\(95\\%\\)-Konfidenzintervall liegt die Irrtumswahrscheinlichkeit bei \\(\\alpha=0.05\\) (bzw. auf jeder Seite bei \\(\\frac{\\alpha}{2} = 0.025\\)), für ein \\(99\\%\\)-Konfidenzintervall liegt die Irrtumswahrscheinlichkeit bei \\(\\alpha=0.01\\) (bzw. auf jeder Seite bei \\(\\frac{\\alpha}{2} = 0.005\\)).\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/chapter1/subchapter1/page-1-1-b/",
	"title": "z-Verteilung",
	"tags": ["z-verteilung", "konfidenzintervall", "signifikanz", "hypothesentest"],
	"description": "",
	"content": " Nutzt man die \\(z\\)-Verteilung muss die Standardabweichung der Grundgesamtheit (\\(\\sigma_x\\)) bekannt sein, oder durch die Stichprobe geschätzt werden. Das Intervall berechnet sich wie folgt:\n\\(\\bar{x} \\pm z_{1-\\frac{\\alpha}{2}} \\ast {\\sigma_{\\bar{x}}}\\)\nObergrenze Intervall:\n\\(x_U = \\bar{x} - z_{1-\\frac{\\alpha}{2}} \\ast {\\sigma_{\\bar{x}}}\\)\nUntergrenze Intervall:\n\\(x_O = \\bar{x} + z_{1-\\frac{\\alpha}{2}} \\ast {\\sigma_{\\bar{x}}}\\)\nZur Erinnerung: Der Standardfehler des Mittelwertes (Stichprobenfehler) wird wie folgt aus der Varianz der Grundgesamtheit berechnet.\n\\(\\sigma_{\\bar{x}} = \\sqrt{ \\frac{\\sigma^2_{x} } {n} } = \\frac{ \\sigma_{x}} { \\sqrt{n}}\\)\nFür ein zweiseitiges \\(99\\%\\)-Konfidenzintervall beträgt \\(z = 2.58\\), für ein zweiseitiges \\(95\\%\\)-Konfidenzintervall beträgt \\(z= 1.96\\) und für ein zweiseitiges \\(90\\%\\)-Konfidenzintervall beträgt \\(z = 1.645\\).\nWenn die Varianz der Grundgesamtheit (\\(\\sigma_x\\)) nicht bekannt ist, muss auch diese geschätzt werden (\\(\\hat{\\sigma}_x\\)). Somit ergibt sich eine doppelte Schätzung. Für Stichproben mit großem \\(n\\) ist dies unproblematisch, bei kleinem \\(n\\) sollte statt der \\(z\\)-Verteilung die \\(t\\)-Verteilung genutzt werden. Die Berechnung des Konfidenzintervalls mit der \\(t\\)-Verteilung wird nachfolgend dargestellt.\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/chapter1/subchapter1/page-1-1-c/",
	"title": "t-Verteilung",
	"tags": ["t-verteilung", "konfidenzintervall", "signifikanz", "hypothesentest"],
	"description": "",
	"content": " Bei Stichproben mit kleinem \\(n\\) und unbekannter Varianz in der Grundgesamtheit (\\(\\sigma_x\\)) empfiehlt es sich statt der \\(z\\)-Verteilung die \\(t\\)-Verteilung für die Berechnung eines Konfidenzintervalls zu nutzen.\nBei der t-Verteilung ist abhängig von den Freiheitsgraden (\\(\\nu\\)). Freiheitsgrade geben an, wie viele Werte in einem statistischen Ausdruck frei variieren können. In der \\(t\\)-Verteilung berechnen sich die Freiheitsgrade wie folgt: \\(\\nu = n-1\\).\nNeben den Freiheitsgraden muss zum Bestimmen des \\(t\\)-Wertes auch noch die Irrtumswahrscheinlichkeit (\\(\\alpha\\)) festgelegt werden.\nDas Konfidenzintervall berechnet sich analog wie bei der \\(z\\)-Verteilung:\n\\(\\bar{x} \\pm t_{(1-\\frac{\\alpha}{2}; n-1)} \\ast {\\sigma_{\\bar{x}}}\\)\nUntergrenze Intervall: \\(x_U = \\bar{x} - t_{(1-\\frac{\\alpha}{2}; n-1)} \\ast {\\sigma_{\\bar{x}}}\\)\nObergrenze Intervall: \\(x_O = \\bar{x} + t_{(1-\\frac{\\alpha}{2}; n-1)} \\ast {\\sigma_{\\bar{x}}}\\)\nwobei \\(\\sigma_{\\bar{x}} = \\frac{\\sigma_x}{\\sqrt{n}}\\)\nAuch hier wird \\(\\sigma_x\\) über die Stichprobe wie folgt geschätzt: \\(\\hat{\\sigma}_x =\\sqrt{\\hat{\\sigma}_x^2} = \\sqrt{\\frac{ \\sum\\limits_{i=1}^n (x_{i} - \\bar{x})^2} {n-1}}\\)\nSiehe auch: Gehring \u0026amp; Weins (2009, Kapitel 11); Bortz \u0026amp; Schuster (2010, Kapitel 6.5).\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/chapter1/subchapter1/page-1-1-d/",
	"title": "Beispiele",
	"tags": ["konfidenzintervall", "signifikanz", "hypothesentest", "lernvideo"],
	"description": "",
	"content": " Im Folgenden werden zwei Beispiele berechnet, die auch in dem Lernvideo nochmals erklärt werden.\nbekanntes \\(\\sigma_x\\) Wir haben eine Umfrage mit \\(2064\\) Befragten (\\(n=2064\\)) mit einem Durchschnittsalter von \\(42.7\\) Jahren (\\(\\bar{x}_{Alter} = 42.7\\)) und einer Standardabweichung in der Grundgesamtheit von \\(11.2\\) Jahren (\\(\\sigma_{Alter} = 11.2\\)). Wir wollen das \\(90\\%\\)-Konfidenzintervall berechnen.\n\\(n = 2064\\)\n\\(\\bar{x}_{Alter} = 42.7\\)\n\\(\\sigma_{Alter} = 11.2\\)\n\\(z_{0.95} = 1.645\\)\nGeschätzter Standardfehler der Grundgesamtheit ist daher:\n\\(\\hat{\\sigma}_\\bar{x} = \\frac{\\sigma_x}{\\sqrt{n}} = \\frac{11.2}{\\sqrt{2064}} \\approx 0.25\\)\nFormel zur Berechnung des Intervalls:\n\\(\\bar{x} \\pm z_{1-\\frac{\\alpha}{2}} \\ast {\\sigma_{\\bar{x}}}\\)\nBerechnung der Grenzen:\n\\(x_U = \\bar{x} - z_{1-\\frac{0.1}{2}} \\ast {\\sigma_{\\bar{x}}} = 42.7 - 1.645 * \\frac{11.2}{\\sqrt{2064}} \\approx 42.7 - 0.406 \\approx 42.294\\)\n\\(x_O = \\bar{x} - z_{1-\\frac{0.1}{2}} \\ast {\\sigma_{\\bar{x}}} = 42.7 + 1.645 * \\frac{11.2}{\\sqrt{2064}} \\approx 42.7 + 0.406 \\approx 43.106\\)\nDas \\(90\\%\\)-Konfidenzintervall um den Mittelwert \\(\\bar{x} = 42.7\\) geht von \\([42.294, 43.106]\\).\nHier ein weiteres Beispiel im Video:\nUnbekanntes \\(\\sigma_x\\) (\\(z\\)-Verteilung) Wir haben eine Umfrage mit \\(1500\\) Befragten (\\(n=1500\\)) mit einem Durchschnittsalter von \\(34.5\\) Jahren (\\(\\bar{x}_{Alter} = 34.5\\)) und einer Standardabweichung in der Stichprobe von \\(4\\) Jahren (\\(s_{Alter} = 4\\)). Wir wollen das \\(99\\%\\)-Konfidenzintervall berechnen.\n\\(n = 1500\\)\n\\(\\bar{x}_{Alter} = 34.5\\)\n\\(s_{Alter} = 4\\)\n\\(z_{0.995} = 2.58\\)\nZuerst muss nun die Standardabweichung in der Grundgesamtheit geschätzt werden (\\(\\hat{\\sigma}_x\\)):\n\\(\\hat{\\sigma}_x = \\sqrt{\\hat{\\sigma}_x^2} = \\sqrt{\\frac{ \\sum\\limits_{i=1}^n (x_{i} - \\bar{x})^2} {n-1}} = \\sqrt{s^2} = s = 4\\)\nDanach muss der Standardfehler geschätzt werden:\n\\(\\hat{\\sigma}_{\\bar{x}} = \\frac {\\hat{\\sigma}_{x}}{\\sqrt{n}} = \\frac{4}{1500} \\approx 0.003\\)\nNun kann das Intervall berechnet werden:\n\\(x_U = \\bar{x} - z_{1-\\frac{\\alpha}{2}} \\ast {\\hat{\\sigma}_{\\bar{x}}} = 34.5 - 2.58 * 0.003 \\approx 34.492\\)\n\\(x_O = \\bar{x} + z_{1-\\frac{\\alpha}{2}} \\ast {\\hat{\\sigma}_{\\bar{x}}}= 34.5 + 2.58 * 0.003 \\approx 34.508\\)\nDas \\(99\\%\\)-Konfidenzintervall um den Mittelwert \\(\\bar{x} = 34.5\\) geht von \\([34.492, 34.508]\\).\nHier ein weiteres Beispiel im Video:\nUnbekanntes \\(\\sigma_x\\) (\\(t\\)-Verteilung) Wir haben eine Umfrage mit \\(101\\) Befragten (\\(n=101\\)) mit einem Durchschnittsalter von \\(51.4\\) Jahren (\\(\\bar{x}_{Alter} = 51.4\\)) und einer Standardabweichung in der Stichprobe von \\(4.5\\) Jahren (\\(s_{Alter} = 4.5\\)). Wir wollen das \\(95\\%\\)-Konfidenzintervall berechnen.\n\\(n = 101\\)\n\\(\\bar{x}_{Alter} = 51.4\\)\n\\(s_{Alter} = 4.5\\)\n\\(t_{(0.975, 100)} = 1.984\\)\nZuerst muss hier nun wieder die Standardabweichung der Grundgesamtheit geschätzt werden:\n\\(\\hat{\\sigma}_x = \\sqrt{\\hat{\\sigma}_x^2} = \\sqrt{\\frac{ \\sum\\limits_{i=1}^n (x_{i} - \\bar{x})^2} {n-1}} = \\sqrt{s^2} = s = 4.5\\)\nDanach muss der Standardfehler geschätzt werden:\n\\(\\hat{\\sigma}_{\\bar{x}} = \\frac {\\hat{\\sigma}_{x}}{\\sqrt{n}} = \\frac{4.5}{101} \\approx 0.045\\)\nNun kann das Intervall berechnet werden:\n\\(x_U = \\bar{x} - t_{(0.975, 100)} \\ast {\\hat{\\sigma}_{\\bar{x}}} = 51.4 - 1.984 * 0.045 \\approx 51.311\\)\n\\(x_O = \\bar{x} + t_{(0.975, 100)} \\ast {\\hat{\\sigma}_{\\bar{x}}}= 51.4 + 1.984 * 0.045 \\approx 51.579\\)\nDas \\(95\\%\\)-Konfidenzintervall um den Mittelwert \\(\\bar{x} = 51.4\\) geht von \\([51.311, 51.579]\\).\nHier ein weiteres Beispiel im Video:\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/chapter2/",
	"title": "Mittelwertvergleiche",
	"tags": [],
	"description": "",
	"content": "Kapitel 2 Mittelwertvergleiche Nun werden Mittelwertvergleiche dargestellt. Das Verfahren der Mittelwertvergleiche ist ein Standardinstrument quantitativer empirischer Sozialforschung.\nMithilfe von Mittelwertvergleichen wird getestet, inwieweit sich zwei oder mehrere Gruppen auf einer metrischen Variable unterscheiden. Damit wird nachgewiesen, ob eine bestimmte Populationsgruppe über systematisch andere Werte verfügt als eine andere Gruppe. Ein Beispiel ist, ob Frauen im Mittel weniger Einkommen haben als Männer.\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/chapter2/page-2-a/",
	"title": "Einführung",
	"tags": ["mittelwertvergleich", "gruppenvergleich"],
	"description": "",
	"content": " Mit Mittelwertvergleichen werden also Zusammenhänge zwischen nominalen (Gruppenvariable, im Beispiel Geschlecht) und metrischen Variablen (im Beispiel Einkommen) getestet.\nFür den Mittelwertvergleich gibt es mehrere Verfahren, die in Abhängigkeit der Gruppenanzahl der nominalen Variable und in Abhängigkeit der Stichprobe angewandt werden. Für einen Mittelwertvergleich zwischen zwei Gruppen wird der sogenannte t-test (Mittelwertvergleich) genutzt und für eine Untersuchung von mehr als zwei Gruppen der sogenannte F-Test (Varianzanalyse).\nBei beiden genannten Verfahren wird nochmals unterschieden, ob die Stichproben abhängig oder unabhängig voneinander sind.\nUnabhängige Stichproben liegen vor, wenn keine Zuordnung (zwischen den Befragten) möglich ist. Zum Beispiel, wenn wir untersuchen, ob männliche Befragte länger Fernsehen schauen als weibliche Befragte. Die Zusammensetzung der Männer in der einen Stichprobe ist unabhängig von der Zusammensetzung der Frauen in der anderen Stichprobe.\nEine abhängige Stichprobe (auch gepaarte/verbundene Stichprobe) liegt dann vor, wenn jedem Wert der einen Stichprobe eindeutig und sinnvoll ein Wert der anderen Stichprobe zugeordnet werden kann. So können wir zum Beispiel Partnerschaften (Zweierbeziehungen) befragen und eine Stichprobe zwischen den Partner:innen vergleichen. Hier können wir jeder:jedem Partner:in A eindeutig eine:einen Partner:in B zuordnen – also einen Fall aus der einen Stichprobe (alle Partner A) einem Fall aus der anderen (alle Partner B) zuordnen. Dies liegt auch vor, wenn wir von einem Individuum eine Messung zu mehreren Zeitpunkten untersuchen.\nEs ergeben sich also für Mittelwertvergleiche vier verschiedene Testsituationen:\nVergleiche von zwei unabhängigen Stichproben statistischer Test: t-Test für unabhängige Stichproben Vergleich von zwei abhängigen Stichproben statistischer Test: t-Test für abhängige Stichproben Vergleich von mehr als zwei unabhängigen Stichproben einfaktorielle Varianzanalyse Vergleich von mehr als abhängigen Stichproben einfaktorielle Varianzanalyse mit Messwiederholungen Die häufigste Anwendung ist ein Gruppenvergleich zwischen zwei unabhängigen Stichproben (t-Test). Dieses Verfahren wird neben dem Verfahren für abhängige Stichproben in diesem Lernmodul vorgestellt. Die Interpretation der anderen Verfahren ist ähnlich und kann mithilfe der empfohlenen Literatur selbstständig aufgearbeitet werden.\nSiehe auch: Behnke \u0026amp; Behnke (2006, Kapitel 24); Bortz \u0026amp; Schuster (2010, Kapitel 8.1 \u0026amp; 8.2).\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/chapter2/subchapter1/",
	"title": "t-Test ungepaart",
	"tags": ["t-test", "ungepaart", "mittelwertvergleich"],
	"description": "",
	"content": "Kapitel 2.1 t-Test ungepaart Der t-test wird für Mittelwertvergleiche genutzt, in denen zwei Gruppen verglichen werden. Dabei gibt es einen t-test für unabhängige Stichproben und einen t-test für abhängige Stichproben. Hier wird nun der t-test für unabhängige Stichproben vorgestellt. Wie zuvor auch beschränken wir uns hier auf die Interpretation des Verfahrens.\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/chapter2/subchapter1/page-2-1-a/",
	"title": "Formulierung t-Test",
	"tags": ["t-test", "mittelwertvergleich"],
	"description": "",
	"content": " Als Beispiel für die Berechnung eines ungepaarten \\(t\\)-tests nehmen wir folgende Frage auf: Weisen Frauen und Männer einen unterschiedlichen Mittelwert bei der wöchentlichen Arbeitszeit auf? Falls ja, sind diese Unterschiede statistisch signifikant?\nDer \\(t\\)-test ist ein Hypothesentest. In der Nullhypothese wird geprüft, dass es keine Unterschiede zwischen den Mittelwerten gibt und in der Alternativhypothese, dass es einen Unterschied gibt. Diese Alternativhypothese kann gerichtet (einseitig) wie ungerichtet (zweiseitig) geprüft werden.\nIm Beispiel lautet die Nullhypothese \\(H_0: \\mu_{Männer} = \\mu_{Frauen}\\)\nEs gibt keinen Unterschied zwischen Männer und Frauen in der mittleren wöchentlichen Arbeitszeit.\nDie ungerichtete Alternativhypothese lautet: \\(H_A: \\mu_{Männer} \\neq \\mu_{Frauen}\\)\nEs gibt einen Unterschied zwischen Männer und Frauen in der mittleren wöchentlichen Arbeitszeit.\nAlternativ könnte eine gerichtete Alternativhypothese lauten: \\(H_{A_{1}}: \\mu_{Männer} \u0026gt; \\mu_{Frauen}\\)\nMänner haben eine höhere mittlere wöchentliche Arbeitszeit als Frauen.\noder\n\\(H_{A_{2}}: \\mu_{Männer} \u0026lt; \\mu_{Frauen}\\)\nFrauen haben eine höhere mittlere wöchentliche Arbeitszeit als Männer.\nIn unserem Beispiel verwenden wir aber eine ungerichtete Hypothese: Wir nehmen also in der Nullhypothese an, dass der Mittelwert der Männer nicht unterschieden wird von dem der Frauen. Bei der ungerichteten Alternativhypothese nehmen wir genau das Gegenteil an und führen einen zweiseitigen Test durch. Bei den gerichteten Alternativhypothesen wird ein einseitiger Test (je nach Formulierung links- oder rechtsseitig) ausgeführt.\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/chapter2/subchapter1/page-2-1-b/",
	"title": "Beispiel gleiche Varianzen",
	"tags": ["t-test", "gleiche varianzen", "mittelwertvergleich", "lernvideo"],
	"description": "",
	"content": " Kommen wir nun zu einem Rechenbeispiel des zweiseitigen t-Tests für unabhängige Stichproben mit gleichen Varianzen. Als Beispiel übernehmen wir die Fragestellung, die wir eingangs als Beispiel benannt haben: Weisen Frauen und Männer einen unterschiedlichen Mittelwert bei der wöchentlichen Arbeitszeit auf? Falls ja, sind diese Unterschiede statistisch signifikant?\nAus einem Survey haben wir folgende Daten:\nFrauen: \\(\\bar{x}_1 = 34.14084, s_1 = 10.89673, n_1 = 884\\)\nMänner: \\(\\bar{x}_2 = 43.64809, s_2 = 9.541511, n_2 = 1023\\)\nDie Hypothesen zum Hypothesentest lauten: \\(H_0: \\mu_1 = \\mu_2\\), Die Mittelwerte der Stichproben unterscheidet sich nicht. \\(H_1: \\mu_1 \\neq \\mu_2\\), Die Mittelwerte der Stichproben unterscheiden sich, sind nicht gleich.\nAus dem Hypothesen folgt, dass wir annehmen: \\(\\mu_1 - \\mu_2 = 0\\)\nDie Formel zur Berechnung des unabhängigen t-Tests mit gleichen Varianzen ist:\n\\(t_{emp} = \\frac{\\bar{x}_1 - \\bar{x}_2}{\\hat{\\sigma}_{(\\bar{x}_1 - \\bar{x}_2)}}\\)\nVereinfacht ausgedrückt berechnet die Prüfgröße:\n\\(t_{emp} = \\frac{Mittelwertdifferenz \\, der \\, Stichproben}{Standardfehler \\, der \\, Mittelwertdifferenz}\\)\nDer Standardfehler der Mittelwertdifferenz berechnet sich wie folgt:\n\\(\\hat{\\sigma}_{(\\bar{x}_1 - \\bar{x}_2)} = \\sqrt{\\frac{(n_1 - 1)*s^2_1 + (n_2 - 1)*s^2_2}{n_1 + n_2 -2}} * \\sqrt{ \\frac{1}{n_1} + \\frac{1}{n_2}}\\)\nNun muss nur noch die Anzahl der Freiheitsgrade errechnet werden, um den kritischen \\(t\\)-Wert bestimmen zu können:\n\\(\\nu = n_1 + n_2 -2\\)\nNun setzen wir die Werte in die Formel ein:\n\\(\\hat{\\sigma}_{(\\bar{x}_1 - \\bar{x}_2)}= \\sqrt{\\frac{(884 - 1)*10.89673^2 + (1023 - 1)*9.541511^2}{884 + 1023 - 2}} * \\sqrt{\\frac{1}{884} + \\frac{1}{1023}} \\approx 0.468\\)\n\\(t_{emp} = \\frac {34.14084 - 43.64809}{0.468} \\approx -20.313\\)\n\\(\\nu = n_1 + n_2 - 2 = 884 + 1023 - 2 = 1905\\)\nNun müssen wir den Betrag der Prüfgröße (\\(|t_{emp}|\\)) gegenüber dem kritischen Wert prüfen. Wir prüfen auf das konventionellen \\(95\\%\\)-Niveau und haben insgesamt \\(1905\\) Freiheitsgrade (\\(\\nu\\)). Wir führen einen zweiseitigen Test durch. Da die meisten \\(t\\)-Tabellen nur bis \\(1000\\) Freiheitsgrade verzeichnet sind, nehmen wir approximativ den kritischen Wert an der Stelle \\(t_{krit(\\nu=1000; p = 0.975)}\\).\nDer kritische Wert beträgt an der genannten Stelle:\n\\(t_{krit_{(\\nu=1000; p=0.975)}} = 1.962\\).\nDamit das errechnete Ergebnis der Signifikanz gilt, muss:\n\\(|t_{emp} | \u0026gt; t_{krit}\\)\nIm Beispiel ist dies erfüllt: \\(|-20.313| \u0026gt; 1.962\\)\nWir können somit schlussfolgern, dass Frauen im Mittel \\(6.909\\) Stunden weniger als Männer in Erwerbsarbeit tätig sind.\nHier ein weiteres Beispiel im Video:\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/chapter2/subchapter1/page-2-1-c/",
	"title": "Beispiel ungleiche Varianzen",
	"tags": ["t-test", "ungleiche varianzen", "mittelwertvergleich", "lernvideo"],
	"description": "",
	"content": " Nun wollen wir ein Beispiel für einen \\(t\\)-Test mit unabhängigen Stichproben, aber ungleichen Varianzen berechnen. Dieser Test wird auch Welch-Test oder \\(t\\)-Test mit Welch-Korrektur genannt. Aus einer weiteren Stichprobe haben wir folgende Werte, wobei Varianzgleichheit zwischen den Gruppen nicht gegeben ist.\nFrauen: \\(\\bar{x}_1 = 34.14084, s_1 = 5.89673, n_1 = 884\\)\nMänner: \\(\\bar{x}_2 = 43.64809, s_2 = 11.541511, n_2 = 1023\\)\nDer empirische \\(t\\)-Wert berechnet sich wie folgt:\n\\(t_{emp} = \\frac{\\bar{x}_1 - \\bar{x}_2}{\\sqrt{\\frac{s^2_1}{n_1} + \\frac{s^2_2}{n_2}}}\\)\nDie Anzahl der Freiheitsgrade berechnet sich bei ungleichen Varianzen wie folgt:\n\\(\\nu = \\frac{(\\frac{s^2_1}{n_1}+ \\frac{s^2_2}{n_2})^2}{\\frac{(\\frac{s^2_1}{n_1})^2}{n_1-1} + \\frac{(\\frac{s^2_2}{n_2})^2}{n_2-1}}\\)\nSetzen wir die Werte in die Formel ein:\n\\(t_{emp} = \\frac{34.14084 - 43.64809}{\\sqrt{\\frac{5.89673}{884} + \\frac{11.541511}{1023}}} \\approx -23.089\\)\n\\(\\nu = \\frac{(\\frac{5.89673}{884}+ \\frac{11.541511}{1023})^2}{\\frac{(\\frac{5.89673}{884})^2}{884-1} + \\frac{(\\frac{11.541511}{1023})^2}{1023-1}} \\approx 1567\\)\nNun müssen wir den Betrag der Prüfgröße (\\(|t_{emp}|\\)) gegenüber dem kritischen Wert prüfen. Wir prüfen auf das \\(99\\%\\)-Niveau und haben insgesamt \\(1567\\) Freiheitsgrade (\\(\\nu\\)). Wir führen einen einseitigen Test durch. Da die meisten \\(t\\)-Tabellen nur bis \\(1000\\) Freiheitsgrade verzeichnet sind nehmen wir approximativ den kritischen Wert an der Stelle \\(t_{krit(\\nu=1000; p =0.99)}\\).\nDer kritische Wert beträgt an der genannten Stelle:\n\\(t_{krit_{(\\nu=1000; p=0.99)}} = 2.330\\).\nDamit das errechnete Ergebnis der Signifikanz gilt, muss:\n\\(|t_{emp} | \u0026gt; t_{krit}\\)\nIm Beispiel ist dies erfüllt: \\(|-20.313| \u0026gt; 2.330\\)\nWir können somit schlussfolgern, dass Frauen im Mittel \\(6.909\\) Stunden weniger als Männer in Erwerbsarbeit tätig sind.\nHier ein weiteres Beispiel im Video:\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/chapter2/subchapter1/page-2-1-d/",
	"title": "Beispiel Veröffentlichung",
	"tags": [],
	"description": "",
	"content": " In Veröffentlichungen sind die Ergebnisse oft verkürzt dargestellt. Hier kommen wir auf ein ähnliches Beispiel wie auf der vorherigen Seite zurück, allerdings mit Daten aus dem ALLBUS 2008. Bei einem Mittelwertvergleich werden immer die Mittelwerte (\\(\\bar{x}\\)) beider Gruppen sowie die Differenz angegeben.\nMittelwertvergleich (Wochenarbeitsstunden nach Geschlecht) Anhand dieser Ausgabe erkennt man, dass sich die mittlere wöchentliche Arbeitszeit zwischen den beiden Gruppen (Männer und Frauen) um ca. \\(9\\) Stunden unterscheidet. Dies gilt in dem Maße nur für die Stichprobe.\nUm nun den Rückschluss auf die Grundgesamtheit ziehen zu können, wird ein t-test durchgeführt. Diese Teststatistik wird angenommen, da wir so testen können, ob die für die aufgeteilte Stichprobe (es ergeben sich so formal zwei unabhängige Stichproben) ergebene Differenz der Mittelwerte auch in der Gesamtpopulation wahrscheinlich ist.\nGrundlage dafür ist immer eine durch zufalls-gesteuerte Stichprobe. Für die Stichproben wird eine Normalverteilung angenommen und somit ist die Differenz beider Stichprobenmittelwerte ebenfalls normalverteilt (genaueres siehe Behnke und Behnke 2006, Kapitel 24).\nFür die Differenz wird entsprechend der t-Verteilung eine Prüfgröße (auch \\(t−Wert\\)) berechnet, die entsprechend ihrer Freiheitsgerade (degrees of freedom) und ihrer Irrtumswahrscheinlichkeit einen bestimmten kritischen Wert annimmt. Wenn die Prüfgröße größer als der kritische Wert ist, gilt die Differenz als signifikant.\nKommen wir zurück auf das Beispiel: Neben den einzelnen Mittelwerten der Gruppen und der Differenz wird immer die Signifikanz der Differenz angegeben. Zur Wiederholung: Die Signifikanz drückt aus, wie wahrscheinlich es ist, dass diese Differenz auch in der Grundgesamtheit messbar ist.\nMittelwertvergleich (Wochenarbeitsstunden nach Geschlecht) Die Signifikanz wird in Veröffentlichungen und in Statistikprogrammen meist mit einem Sternchen angegeben. In der Legende einer Tabelle kann man i.d.R. den \\(p−Wert\\) ablesen. Dieser von Statistikprogrammen ausgegebene \\(p−Wert\\) ersetzt die Angabe der Prüfgröße und erspart den Nutzer*innen das Nachschlagen von Verteilungstabellen.\nDer \\(p−Wert\\) ist die Wahrscheinlichkeit, dass die Prüfgröße bei Gültigkeit der Nullhypothese mindestens den in der Stichprobe berechneten Wert annimmt. Es ist somit die Irrtumswahrscheinlichkeit, mit der die Nullhypothese gerade noch widerlegt werden kann. Der \\(p−Wert\\) trifft somit eine Entscheidung über den Ablehnungsbereich der Nullhypothese.\nBei der Verwendung dieser \\(p−Werte\\) (statt des kritischen Wertes und Prüfgröße) wird ein Test wie folgt entschieden: \\(p \u0026lt; \\alpha : Nullhypothese \\thinspace wird \\thinspace abgelehnt\\) \\(p \u0026gt; \\alpha : Nullhypothese \\thinspace wird \\thinspace bestätigt\\)\nDer Tabelle können wir entnehmen, dass das Ergebnis signifikant ist, da der \\(p−Wert\\) kleiner als die Irrtumswahrscheinlichkeit \\(\\alpha\\) ist (\\(p \u0026lt; \\alpha \\Rightarrow Ablehnung \\thinspace Nullhypothese\\)).\nDie angegebene Differenz der mittleren wöchentlichen Arbeitszeit zwischen Männern und Frauen ist also nicht nur in der Stichprobe gegeben, sondern ist bei einer 5%-Fehlerwahrscheinlichkeit auch für die Grundgesamtheit sehr wahrscheinlich (Grundgesamtheit ist hierbei die über 18-Jährige Wohnbevölkerung in Deutschland).\nSiehe auch: Behnke \u0026amp; Behnke (2006, Kapitel 24); Bortz \u0026amp; Schuster (2010, Kapitel 8.2).\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/chapter2/subchapter2/",
	"title": "t-Test gepaart",
	"tags": ["t-test", "gepaart", "mittelwertvergleich"],
	"description": "",
	"content": "Kapitel 2.2 t-Test gepaart Nun wird ein Beispiel für den t-Test mit abhängigen Stichproben vorgestellt. Dazu nehmen wir folgendes Beispiel an: In einem Landtag wurde ein neues Bildungsprogramm für Schulen initiiert, dass u.a. das politische Wissen der teilnehmenden Schüler:innen steigern soll. Dazu wurde vor der Teilnahme und nach der Teilnahme jeweils das politische Wissen gemessen. Von jeder Schüler:in liegen also zwei Messwerte vor: einmal das politische Wissen vor der Teilnahme und einmal das politische Wissen nach der Teilnahme.\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/chapter2/subchapter2/page-2-2-a/",
	"title": "Berechnung mit Rohwerten",
	"tags": ["t-test", "gepaart", "mittelwertvergleich", "lernvideo"],
	"description": "",
	"content": " Dazu nehmen wir folgendes Beispiel an: In einem Landtag wurde ein neues Bildungsprogramm für Schulen initiiert, dass u.a. das politische Wissen der teilnehmenden Schüler:innen steigern soll. Dazu wurde vor der Teilnahme und nach der Teilnahme jeweils das politische Wissen gemessen. Von jeder Schüler:in liegen also zwei Messwerte vor: einmal das politische Wissen vor der Teilnahme (\\(x_1\\)) und einmal das politische Wissen nach der Teilnahme (\\(x_2\\)).\nWir haben folgende Messwerte für 5 fiktive Schüler:innen: \\(x_1\\) ist der Messwert vor der Teilnahme, \\(x_2\\) der Messwert nach der Teilnahme und \\(d\\) die Differenz zwischen \\(x_1\\) und \\(x_2\\).\nSchüler-ID \\(x_1\\) \\(x_2\\) \\(d\\) \\(1\\) \\(4\\) \\(7\\) \\(-3\\) \\(2\\) \\(8\\) \\(8\\) \\(0\\) \\(3\\) \\(3\\) \\(9\\) \\(-6\\) \\(4\\) \\(2\\) \\(6\\) \\(-4\\) \\(5\\) \\(7\\) \\(10\\) \\(-3\\) Die Formel zur Berechnung des \\(t\\)-Tests für abhängige Stichproben lautet wie folgt:\n\\(t_{emp} = \\frac{\\bar{x}_d - \\mu_d}{\\hat{\\sigma}_{\\bar{x}_d}} = \\frac{\\bar{x}_d - 0}{\\hat{\\sigma}_{\\bar{x}_d}} = \\frac{\\bar{x}_d}{\\hat{\\sigma}_{\\bar{x}_d}}\\)\nmit \\(\\hat{\\sigma}_{\\bar{x}_d} = \\frac{\\hat{\\sigma}_d}{\\sqrt{n}}\\)\nund \\(\\hat{\\sigma}_d = \\frac{\\sum_{i=1}^n (d_i - \\bar{x}_d)^2}{n-1}\\)\nund \\(d_i = x_{1_i} - x_{2_i}\\)\nund \\(\\bar{x}_d = \\frac{1}{n}\\sum_{i=1}^n {x_{1_i} - x_{2_i}}\\)\nDie Freiheitsgrade (\\(\\nu\\)) ergeben sich aus \\(\\nu = 2*n-2\\), wobei \\(n\\) die Größe einer Gruppe ist. Bei abhängigen Stichproben muss die Gruppengröße gleich sein.\nBerechnen wir nun den \\(t\\)-Test für abhängige Stichproben:\n\\(\\bar{x}_d\\) \\(\\bar{x}_d = \\frac{1}{n}\\sum_{i=1}^n {x_{1_i} - x_{2_i}} = \\frac{1}{5}*((-3) + 0 + (-6) + (-4) + (-3)) = -3.2\\)\n\\(\\hat{\\sigma}_d\\) \\(\\hat{\\sigma}_d = \\frac{\\sum_{i=1}^n (d_i - \\bar{x}_d)^2}{n-1} = \\frac{(-3-(-3.2))^2+(0-(-3.2))^2 +(-6-(-3.2))^2+(-4-(-3.2))^2+(-3-(-3.2))^2}{5-1} = 4.7\\)\n\\(\\hat{\\sigma}_{\\bar{x}_d}\\) \\(\\hat{\\sigma}_{\\bar{x}_d} = \\frac{\\hat{\\sigma}_d}{\\sqrt{n}} = \\frac{4.7}{\\sqrt{5}} \\approx 2.102\\)\n\\(t_{emp}\\) \\(t_{emp} = \\frac{\\bar{x}_d}{\\hat{\\sigma}_{\\bar{x}_d}} = \\frac{-3.2}{2.102} \\approx -1.552\\)\n\\(\\nu\\) \\(\\nu = 2 * n - 2 = 2 * 5 -2 = 8\\)\nAuch hier würde man nun den empirischen \\(t\\)-Wert am kritischen \\(t\\)-Wert überprüfen. Dies werden wir hier nur zu Illustrationszwecken machen. Wir prüfen auf das \\(90\\%\\)-Niveau und haben insgesamt \\(8\\) Freiheitsgrade (\\(\\nu\\)). Wir führen einen zweiseitigen Test durch. Der kritische \\(t\\)-Wert an dieser Stelle ist \\(t_{krit(\\nu=8; p = 0.95)} = 1.860\\).\nDer kritische Wert beträgt an der genannten Stelle:\n\\(t_{krit_{(\\nu=8; p=0.95)}} = 1.860\\).\nDamit das errechnete Ergebnis der Signifikanz gilt, muss:\n\\(|t_{emp} | \u0026gt; t_{krit}\\)\nIm Beispiel ist dies nicht erfüllt: \\(|-1.552| \u0026gt; 1.860\\)\nWir haben also kein signifikantes Ergebnis in diesem Beispiel.\nHier ein weiteres Beispiel im Video:\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/chapter2/subchapter2/page-2-2-b/",
	"title": "Angegebene Differenzwerte",
	"tags": ["t-test", "gepaart", "mittelwertvergleich", "lernvideo"],
	"description": "",
	"content": " Man kann den \\(t\\)-Test für abhängige Stichproben auch berechnen, wenn die Werte des Mittelwerts der Differenzen der gepaarten Mittelwerte (\\(\\bar{x}_d\\)) sowie die Standardabweichung der Differenzen der gepaarten Mittelwerte (\\(\\hat{\\sigma}_d\\)) angegeben sind. Wir nehmen das Beispiel von oben, gehen aber nun von folgenden Werten für eine Stichprobe mit \\(1.000\\) Schüler:innen aus.\n\\(\\bar{x}_d = -3.7\\)\n\\(\\hat{\\sigma}_d = 1.3\\)\n\\(n = 1000\\)\nHier müssen nun nur die Rechenschritte aus dem vorherigen Beispiel ab Punkt 3 berechnet werden:\n\\(\\hat{\\sigma}_{\\bar{x}_d} = \\frac{\\hat{\\sigma}_d}{\\sqrt{n}} = \\frac{1.3}{\\sqrt{1000}} \\approx 0.041\\)\n\\(t_{emp} = \\frac{\\bar{x}_d}{\\hat{\\sigma}_{\\bar{x}_d}} = \\frac{-3.7}{0.041} \\approx -90.003\\)\n\\(\\nu = 2 * n -2 = 2* 1000 - 2 = 1998\\)\nNun müssen wir den Betrag der Prüfgröße (\\(|t_{emp}|\\)) gegenüber dem kritischen Wert prüfen. Wir prüfen auf das konventionellen \\(90\\%\\)-Niveau und haben insgesamt \\(1998\\) Freiheitsgrade (\\(\\nu\\)). Wir führen einen zweiseitigen Test durch. Da die meisten \\(t\\)-Tabellen nur bis \\(1000\\) Freiheitsgrade verzeichnet sind nehmen wir approximativ den kritischen Wert an der Stelle \\(t_{krit(\\nu=1000; p = 0.90)}\\).\nDer kritische Wert beträgt an der genannten Stelle:\n\\(t_{krit_{(\\nu=1000; p=0.90)}} = 1.282\\).\nDamit das errechnete Ergebnis der Signifikanz gilt, muss:\n\\(|t_{emp} | \u0026gt; t_{krit}\\)\nIm Beispiel ist dies erfüllt: \\(|-90.003| \u0026gt; 1.282\\)\nWir können somit schlussfolgern, dass das politische Wissen nach der Teilnahme signifikant im Mittel um \\(3.7\\) Punkte steigt.\nHier ein weiteres Beispiel im Video:\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/chapter3/",
	"title": "Lineare Regression",
	"tags": [],
	"description": "",
	"content": "Kapitel 3 Lineare Regression In diesem letzten Kapitel des Lernmoduls wird nun die lineare Regression in Grundzügen dargestellt. Auch hierbei werden die mathematischen Grundlagen dargestellt und dann auf die Interpretation der wichtigsten Kennzahlen einer Regressionsrechnung eingegangen. Für eine tiefere Auseinandersetzung, insbesondere mit den mathematischen Annahmen eines Regressionsmodells, werden die Literaturverweise empfohlen.\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/chapter3/page-3-a/",
	"title": "Einführung",
	"tags": ["regression", "bivariat"],
	"description": "",
	"content": " Im ersten Schritt werden nun die mathematischen Grundlagen des Verfahrens der linearen Regression am Beispiel einer bivariaten linearen Regression vorgestellt. Aufbauend darauf werden die Grundlagen auf eine multivariate lineare Regression übertragen und die Interpretation der wichtigsten Kennzahlen vorgestellt. Abschließend wird zum besseren Verständnis ein Praxisbeispiel aktueller Forschung der empirischen Demokratieforschung dargestellt.\nDie Regressionsanalyse ist eine statistische Methode, um die Beziehungsstruktur zwischen mehreren Variablen zu untersuchen. Dabei findet eine Informationsreduktion statt, so dass die Untersuchung auf wenige Kennzahlen beschränkt werden kann. Mit der Regressionsanalyse kann die Wirkung einer oder mehrerer unabhängiger Variable(n) auf eine abhängige Variable in Bezug auf die Richtung und die Stärke des Einflusses überprüft werden.\nIm Unterschied zur Korrelation wird bei der Regressionsanalyse die Kausalität überprüft. Die Richtung des Einflusses wird theoriegeleitet festgelegt, von der unabhängigen auf die abhängige Variable. Inferenzstatistisch wird zusätzlich überprüft, ob dieser Einfluss signifikant ist, also ob er in der Grundgesamtheit der Stichprobe als wahrscheinlich gilt.\nAllgemein ausgedrückt können mit der Regressionsanalyse zwei verwandte Fragen beantwortet werden:\nWie gut erklären bestimmte Faktoren (unabhängige Variablen) die Varianz einer abhängigen Variable?\nWelchen Einfluss üben die einzelnen Faktoren auf diese abhängige Variable unter Konstanthalten (Kontrolle) des Einflusses der anderen unabhängigen Variablen aus?\nDie multivariate lineare Regression stellt dabei eine grundsätzliche Analysemethode dar. Mit der (multivariaten) linearen Regression wird versucht, eine metrische Variable über eine Linearkombination mehrerer anderer Variablen (metrisch) darzustellen und kausal zu erklären.\nMithilfe eines Regressionsmodells lassen sich aus der Theorie hergeleitete Hypothesen über eine Beeinflussungsstruktur bestimmter Variablen auf andere Variablen überprüfen. Für die Sozialwissenschaften ist dies ein geeignetes Verfahren, da in der Regel mehrere Variablen einen Einfluss auf eine abhängige Variable ausüben, die wir in eine Regressionsanalyse integrieren können. Es werden dabei verschiedene Arten der Regression in Abhängigkeit des Skalenniveaus der abhängigen Variable unterschieden:\nlineare Regression\nlogistische Regression\nProbit-Regression\nBei der linearen Regression wird ein linearer Zusammenhang zwischen abhängiger Variable und unabhängigen Variablen angenommen. Es stellt somit eine Weiterentwicklung der bivariaten Korrelationsanalyse dar. Die abhängige Variable muss ein metrisches Skalenniveau aufweisen.\nDie logistische Regression wird für dichotome abhängige Variablen genutzt (nominale Skala), da der Zusammenhang sich nur logistisch und nicht linear darstellen lässt. Die Probit-Regression erweitert die logistische Regression auf kategoriale Variablen mit mehr als zwei Ausprägungen. Sowohl die logistische und Probit-Regression sind fortgeschrittene Verfahren quantitativer Analysetechniken. Sie werden daher in diesem Lernmodul nicht näher behandelt.\nSiehe auch: Urban \u0026amp; Mayerl (2011); Fromm (2010, Kapitel 3); Janssen \u0026amp; Laatz (2013, Kapitel 17).\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/chapter3/page-3-b/",
	"title": "Lineare Gleichung",
	"tags": ["linear", "bivariat"],
	"description": "",
	"content": " Die Grundannahme einer linearen Regression ist die lineare Darstellung, in der die abhängige Variable \\(Y\\) über eine Linearkombination mit der unabhängigen Variable \\(X\\) dargestellt wird:\n\\(y = \\beta_0 + \\beta_1 \\ast x\\)\n\\(\\beta_0\\) stellt die Konstante (engl. intercept) dar, also den Schnittpunkt mit der \\(y\\)-Achse. In manchen Veröffentlichungen wird hierfür auch ein kleines \\(a\\) oder \\(\\alpha\\) verwendet. \\(\\beta_1\\) ist die Steigung der Geraden, in diesem Fall die Steigung der abhängigen Variable \\(y\\) durch die unabhängige Variable \\(x\\) (englt. slope). Beide Variablen stellen in diesem Modell beobachtete, nicht zufällige Messwerte dar. Mithilfe einer linearen Funktion errechnen wir diese Gerade, die aus der Konstanten (\\(\\beta_0\\)) und der Steigung (\\(\\beta_1\\)) besteht.\nGrafisch lässt sich dies leichter nachvollziehen. Nehmen wir folgende Lineardarstellung als Beispiel:\n\\(y = 0 + 1 \\ast x\\)\nBeispiel Regression Die blauen Punkte stellen einzelne Fälle dar. Die Linie spiegelt die lineare Gleichung wieder, also die Darstellung mit der sich \\(y\\) aus der Steigung von \\(x\\) und einer Konstante darstellen lässt.\nNochmal zum besseren Verständnis: \\(\\beta_0\\) stellt den Schnittpunkt mit der \\(y\\)-Achse dar, also gibt den Wert wieder, wenn \\(x=0\\). \\(\\beta_1\\) dagegen gibt die Steigung bei der Erhöhung von \\(x\\) dar. Diese ist für jede gleiche Steigerung von \\(X\\) (im Beispiel 1) gleich.\nPerfekte lineare Darstellung "
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/chapter3/page-3-c/",
	"title": "Bivariate Regression",
	"tags": ["regression", "bivariat"],
	"description": "",
	"content": " Bei der Berechnung einer linearen Regression in den Sozialwissenschaften lässt sich das Regressionsmodell nur mit einer Anpassung als lineare Darstellung durchführen. Dies liegt daran, dass keine abhängige Variable perfekt durch eine oder mehrere unabhängige Variable(n) darstellbar ist. In der Modellannahme wird deshalb davon ausgegangen, dass es einen nicht-erklärbaren Anteil gibt, der mit der Störvariable \\(\\varepsilon\\) (manchmal auch \\(U\\)) angegeben wird. In den Sozialwissenschaften ergibt sich diese Störvariable aus der Forschungslogik heraus: Wir können in den Sozialwissenschaften niemals alle Variablen, die auf etwas wirken, berücksichtigen, da ein soziales Ereignis stets umfassend ist.\nAus dieser Überlegung heraus ergibt sich die Anpassung einer linearen Gleichung zur bivariaten linearen Regression für die Grundgesamtheit: \\((1) \\quad y_i = \\beta_0 + \\beta_1 \\ast x_i + \\varepsilon_i\\)\nMit \\(Y = (y_1, y_2, ..., y_n), \\thinspace X = (x_1, x_2, ..., x_n) \\thinspace und \\thinspace \\varepsilon = (\\varepsilon_1, \\varepsilon_2, ..., \\varepsilon_n)\\)\nGeschätzt werden im Modell die Regressionskoeffizienten \\(\\beta_0\\) und \\(\\beta_1\\) (bzw. weitere Regressionskoeffizienten im multivariaten Modell, dazu später mehr). Diese stellen mit den beobachteten Werten der unabhängigen Variablen die systematische Komponente dar. Die Störvariable \\(\\varepsilon\\) stellt die stochastische Komponente dar. Um die empirische Beobachtung der Wertepaare der zwei Variablen \\(Y\\) und \\(X\\) darzustellen, wird diese stochastische Komponente benötigt. Denn, wie oben bereits erwähnt, lassen sich niemals eine perfekt lineare Beziehungen in den Sozialwissenschaften darstellen. Die Störvariable \\(\\varepsilon\\) ist nicht beobachtbar und nicht messbar. \\(\\varepsilon\\) kann man sich inhaltlich als die Gesamtheit der nicht berücksichtigten Variablen vorstellen.\nIn der Stichprobe, mit dessen Variablen und Beobachtungen wir die lineare Regression berechnen, können wir die Störvariable(n) nicht messen. Es können nie perfekte Beobachtungen entstehen: Die beobachteten Werte von \\(y\\) (aus der Stichprobe) weichen von den durch die Regressionsgerade geschätzten Werten (\\(\\hat{y}\\)) ab. Diese Differenzen bezeichnet man im Modell (bzw. der Stichprobe) als Residuen und sie werden mathematisch im Term \\(e\\) berücksichtigt. Dies ist als die Differenz von beobachteten \\(y\\)-Werten und geschätzten \\(\\hat{y}\\)-Werten zu verstehen und ist ein Nebenprodukt der Schätzung der Regressionskoeffizienten \\(\\hat{\\beta}_0\\) und \\(\\hat{\\beta}_1\\).\nDie Residuen im geschätzten Modell ergeben sich also wie folgt:\n\\(e_i = y_i - \\hat{y}_i = y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 * x_1\\)\nFür die geschätzte Regressionsrechnung einer Stichprobe müssen wir theoretisch also zwischen zwei Gleichungen unterscheiden: Einmal die Schätzung der Regressionsgleichung (später mehr zum Verfahren der Schätzung) und einmal die Modellgleichung.\nModellgleichungen der linearen Regressionen sehen in aller Regel wie folgt aus:\n\\((2) \\quad y_i = \\beta_0 + \\beta_1 \\ast x_i + e_i\\)\nMit \\(Y = (y_1, y_2, ..., y_n), \\thinspace X = (x_1, x_2, ..., x_n) \\thinspace und \\thinspace e= (e_1, e_2, ..., e_n) \\thinspace bzw. \\thinspace e_i=\\hat{y}_i - y_i\\)\nEs wird der beobachtete Wert von \\(Y\\) über die Konstante und die Steigerung der beobachteten unabhängigen Variablen (\\(X\\)) errechnet. Da sich diese Gleichung in aller Regel nicht perfekt linear darstellt, wird über das Residuum (\\(e\\)) ein Anpassungsterm errechnet.\nFür die geschätzte Regressionsgleichung, bei der für \\(Y\\) Werte geschätzt werden, sieht die Formel wie folgt aus: \\((3) \\quad \\hat{y_i} = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\ast x_i\\)\nDieses Modell gilt mathematisch nur unter der Annahme, dass \\(\\hat{Y}\\) allein durch \\(X\\) beeinflusst wird.\nDie Schätzung der Regressionskoeffizienten \\(\\hat{\\beta}_0\\) und \\(\\hat{\\beta}_1\\) erfolgt über die beobachteten Werte von \\(X\\) und \\(Y\\):\n\\(\\hat{\\beta}_1\\) wird über die Differenz zum Mittelwert der beobachten Variablen berechnet: \\(\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n {(x_i - \\bar{x})(y_i - \\bar{y})}}{\\sum_{i=1}^n {(x_i-\\bar{x})^2}}\\)\n\\(\\hat{\\beta}_0\\) wird dann unter Zuhilfenahme der Berechnung von \\(\\hat{\\beta}_1\\) berechnet: \\(\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1\\bar{x} = \\bar{y} - \\frac{\\sum_{i=1}^n {(x_i - \\bar{x})(y_i - \\bar{y})}}{\\sum_{i=1}^n {(x_i-\\bar{x})^2}}\\)\nAus diesem Schätzmodell können dann geschätzte Werte von \\(Y\\) berechnet werden. Ebenso können so die einzelnen Residuen der jeweiligen beobachteten Wertepaare von \\(X\\) und \\(Y\\) berechnet werden. Zur Erinnerung: Die Residuen in der Modellgleichung geben die Differenz zwischen beobachtetem \\(y_i\\)-Wert und dem geschätztem \\(\\hat{y}_i\\)-Wert an (\\(e_i=\\hat{y}_i - y_i\\)).\nMithilfe dieser Gleichung (3) errechnen Statistikprogramme die beste Gerade unter Berücksichtigung des Residuums aus der vorherigen Gleichung (2). Das Ziel der Regressionsrechnung ist es, die Gerade zu finden, bei der das Residuum e minimal ist.\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/chapter3/page-3-d/",
	"title": "Beispiel",
	"tags": ["bivariat", "regression"],
	"description": "",
	"content": " Kommen wir nun zu einem eingängigeren Beispiel: Die folgenden Daten sind fiktiv generiert, damit diese das Verfahren gut simulieren können. Wir möchten eine bivariate lineare Regression berechnen, in der die Zufriedenheit mit der Demokratie (satdem) durch die Zufriedenheit mit der ökonomischen Leistung (sateco) berechnet wird. Wir prüfen also, ob und inwieweit die Zufriedenheit mit der ökonomischen Leistung die Zufriedenheit mit der Demokratie beeinflusst.\nDie Formel unseres Modells lautet daher: \\(satdem_i= \\beta_0 + \\beta_1 \\ast sateco_i + e_i\\), wobei \\(i=1,…,n\\).\nDie Formel unserer geschätzten Werte ist in diesem Beispiel wie folgt: \\(\\hat{satdem}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\ast sateco_i\\), wobei \\(i=1,...,n\\)\nDas Residuum ergibt sich aus der Differenz zwischen beobachteten und geschätzten Werten: \\(e_i = {satdem}_i - \\hat{satdem}_i\\)\nHier erschließt sich auch nochmals der Sinn der Residuen inhaltlich: Denn die Varianz der Zufriedenheit mit der Demokratie wird sich nicht vollkommen allein über die Zufriedenheit mit der ökonomischen Leistung erklären lassen. Weitere Faktoren können die Höhe der Zufriedenheit mit der Demokratie beeinflussen. Die Residuen werden im Schätzmodell zusammengefasst und als Residuen mit \\(e\\) dargestellt.\nGrafisch lässt sich dies leichter darstellen: Im Plot sehen wir die beobachteten Werte (blau) und die geschätzten Werte (rot).\nResiduen in der linearen Regression In der Abbildung sehen wir einen Scatterplot zwischen Zufriedenheit mit der Demokratie und Zufriedenheit mit der ökon. Leistung. Eingezeichnet ist ebenfalls die geschätzte Regressionsgerade (rote Linie). Auf der \\(x\\)-Achse ist die unabhängige Variable (Zufriedenheit mit der ökon. Leistung) abgetragen, auf der \\(y\\)-Achse die abhängige Variable (Zufriedenheit mit der Demokratie). Die blauen Punkte geben die beobachteten und gemessenen \\(y\\)-Werte wieder (aus der Stichprobe) und die rote Linie die geschätzten Werte (\\(\\hat{y}\\)) aus der Regressionsrechnung. Der Abstand zwischen einem beobachteten blauen Punkt der Daten und dem jeweiligen geschätzten roten Punkt auf der Regressionsgerade ist der Wert des Residuums (\\(e\\)). Das Residuum umfasst also die Abweichung des beobachteten Wertes vom geschätzten Wert. Die Summe dieser Abweichungen beinhaltet den Anteil der nicht erklärten Varianz.\nIn der nächsten Grafik ist dies nochmals eingezeichnet und deutlicher dargestellt am Beispiel eines Datenpunktes.\nResiduen in der linearen Regression Für jede einzelne Berechnung wird das Residuum (grün) errechnet. Dieses ist der Abstand zwischen beobachtetem (blau) und geschätztem (rot) Wert.\nResiduen werden daher für jede einzelne Beobachtung wie folgt berechnet: \\(e_i = y_i - \\hat{y}_i\\)\nWichtig für die Unterscheidung ist: Residuen treten in den Modellen auf und variieren pro Fall (in diesem Beispiel Individuum). Residuen treten nicht in der Grundgesamtheit auf. In der Grundgesamtheit finden sich Störvariablen, die den nicht erklärten Teil des Modells beinhalten. Die Schätzung der Regression ist also immer eine Anpassung, die einen nicht erklärten Teil beinhaltet.\nSiehe auch: Fromm (2010, Kapitel 3.2); Urban \u0026amp; Mayerl (2011, Kapitel 2.1 \u0026amp; 2.2).\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/chapter3/page-3-e/",
	"title": "Schätzverfahren",
	"tags": ["schaetzung", "regression", "lernvideo"],
	"description": "",
	"content": " In diesem Lernvideo werden die relevanten Teile der Regressionsberechnung an einem weiteren Beispiel vereinfacht dargestellt und das mathematische Vorgehen der Berechnung der besten Regressionsgeraden kurz erläutert. Weitergehende Ausführungen sind unterhalb des Lernvideos im Text zu finden.\nKommen wir zurück zur Modellschätzung. Die Gleichung einer Modellschätzung lautet: \\(Y = \\beta_0 + \\beta_1 \\ast X + e\\) Mit \\(Y = (y_1, y_2, ..., y_n), X = (x_1, x_2, ..., x_n)\\) und \\(e=(e_1, e_2, ..., e_n)\\).\nDabei wird die Regressionsgerade über folgende Gleichung berechnet:\n\\(\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\ast X\\), mit \\(\\hat{Y} = (\\hat{y}_1, \\hat{y}_2, ..., \\hat{y}_n), X = (x_1, x_2, ..., x_n)\\)\nUm nun die beste Regressionsgerade zu finden, gibt es für die Schätzung einer Regressionsgerade verschiedene Verfahren. Für die lineare Regression wird das Ordinary-Least-Squares(OLS)-Verfahren angewendet, dass nach dem Gauss-Markov-Theorem die best linear unbiased estimation (BLUE) ist. Für eine genauere Auseinandersetzung mit diesem Verfahren empfehlen wir folgende Literatur: Urban \u0026amp; Mayerl (2011, Kapitel 3.1.2 BLUE-Schätzer).\nMit Rückbezug zur Grafik können wir im Beispiel der Regression von Zufriedenheit mit der Demokratie leicht verstehen, wie das Schätzverfahren OLS die beste Regressionsgerade berechnet.\nResiduen in der linearen Regression Wie vorhin festgestellt, verursacht die Rechnung der Regressionsgerade Abweichungen der beobachteten Werte von den geschätzten Werten. In der Grafik stellen die blauen Punkte den gemessen \\(y\\)-Wert dar und die roten Punkte den geschätzten \\(y\\)-Wert (\\(\\hat{y}\\)). Der Abstand zwischen dem beobachteten blauen Punkt (\\(y_i\\)) der Daten und den geschätzten roten Punkt (\\(\\hat{y}_i\\)) auf der Regressionsgerade ist der Wert des Residuums (\\(e_i\\)) des jeweiligen Falls \\(i\\). Die Summe der Residuen beinhaltet sozusagen den Anteil, der nicht über die Steigung von \\(x\\) erklärt werden kann (Varianz). Das OLS-Verfahren berechnet die beste Regressionsgerade über die kleinsten quadrierten Abstände von \\(y\\) und \\(\\hat{y}\\) und minimiert diese. Das Ziel ist es, die Gerade zu finden, die die quadratischen Abstände zwischen den geschätzten (rote Punkte) und beobachteten (blaue Punkte) Werten minimiert. Also die Gerade, die durchschnittlich am wenigstens von den beobachteten Werten abweicht.\nFür die einfache lineare Regression lautet der mathematische Ausdruck wie folgt: OLS: \\(\\sum\\limits_{i=1}^n e^2_i = \\sum\\limits_{i=1}^n (y_i - \\hat{y}_i)^2 = \\sum\\limits_{i=1}^n (y_i - \\beta_0 - \\beta_i \\ast x_i)^2 \\rightarrow min.!\\)\nSteigung (slope): \\(\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n {(x_i - \\bar{x})(y_i - \\bar{y})}}{\\sum_{i=1}^n {(x_i-\\bar{x})^2}}\\)\nKonstante (intercept): \\(\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta_1} \\ast \\bar{x}\\)\nDas OLS-Verfahren hat verschiedene Bedingungen, die hier nur kurz genannt werden und in der angegebenen Literatur vertieft werden können. Bei der Durchführung einer linearen Regression müssen diese kontrolliert werden. Damit das OLS-Verfahren wirklich ein blue-Schätzer ist, müssen drei Bedingungen für die Residuen erfüllt sein:\nHomoskedastizität \\((VAR(\\varepsilon_i)=\\delta^2)\\)\n\\(E(\\varepsilon_i )=0\\)\nAutokorrelation \\(cov(\\varepsilon_i; \\varepsilon_j )=0\\) mit \\(i \\neq j\\)\nFür die lineare Regressionsrechnung gilt, dass es sich um eine einmalige Schätzung handelt und bei Erfüllung dieser Bedingungen das OLS-Verfahren als blue-Schätzer gilt. Diese Aussagen nach dem Gauss-Markov-Prinzip beziehen sich auf die Störvariable(n) \\(\\varepsilon\\), die in der Grundgesamtheit vorliegt/vorliegen. In der einzelnen Stichprobe wird deshalb das Residuum (\\(e\\)) überprüft. Zum einen (1) müssen die Varianzen der Residuen homoskedastisch sein, also dürfen nicht sehr stark zu- oder abnehmen. Ebenfalls (2) muss der Erwartungswert des Fehlers einer Beobachtung (\\(i\\)) 0 sein. Das bedeutet, dass im Mittel der Fehler für die Regressionsrechnung quasi wegfällt. Drittens (3) dürfen die Fehler einer Beobachtung (\\(i\\)) nicht mit den Fehlern einer weiteren Beobachtung (\\(j\\)) korrelieren. Die Überprüfung der Modellannahmen erfolgt im multivariaten Modell über die Residualanalyse (siehe dazu Urban \u0026amp; Mayerl (2011, Kapitel 4)).\nAls zusätzliche Voraussetzung kommt in der linearen Regression hinzu, dass wir mit dem Rückgriff auf die Inferenzstatistik von einer Normalverteilung der Störterme ausgehen müssen.\nNormalverteilung \\(\\varepsilon \\sim \\mathcal{N}(0, \\, \\sigma^2)\\)\nAndernfalls können die statistischen Tests keine korrekten Ergebnisse liefern. In der Analyse bedeutet dies, dass man auch die Normalverteilung der Residuen der spezifischen Regressionsrechnung überprüfen muss.\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/chapter3/page-3-f/",
	"title": "Multivariate lineare Regression",
	"tags": ["multivariat", "regression", "lernvideo"],
	"description": "",
	"content": " Nun erweitern wir die Annahmen auf mehrere unabhängige Variablen. Mit der Regression als multivariate Analyse können Einflüsse möglicher Drittvariablen kontrolliert werden und es wird Aufschluss über die Abhängigkeitsstrukturen zwischen mehreren Variablen gegeben. Die Regression ist daher ein strukturprüfendes Verfahren.\nBei der multivariaten Regression betrachten wir nicht mehr nur eine unabhängige Variable, sondern beziehen mehrere unabhängige Variablen in das Modell hinein. Der Einbezug von unabhängigen Variablen findet stets theoriegeleitet statt. Die Modellformulierung der multivariaten linearen Regression ändert sich daher auf folgende:\n\\(y_i = \\beta_0 + \\beta_1 \\ast x_{1_i} + \\beta_2 \\ast x_{2_i} + ... + \\beta_k \\ast x_{k_i} + \\varepsilon_i\\)\nMit \\(y=(y_1, y_2, ..., y_n), \\thinspace x_{k_1} =(x_{k_1}, x_{k_2}, ..., x_{k_n}), \\thinspace und \\thinspace \\varepsilon_i=(\\varepsilon_1, \\varepsilon_2, ..., \\varepsilon_n)\\)\nDie Schätzgleichung des multivariaten Modells lautet wie folgt:\n\\(\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\ast x_{1_i} + \\hat{\\beta}_2 \\ast x_{2_i} + ... + \\hat{\\beta}_{k_i} \\ast x_{k_i}\\)\nDie Schätzgleichung für die beobachteten Werte des multivariaten Modells lautet in Anpassung an die Modellgleichung wie folgt:\n\\(y_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\ast x_{1_i} + \\hat{\\beta}_2 \\ast x_{2_i} + ... + \\hat{\\beta}_{k_i} \\ast x_{k_i} + \\hat{\\epsilon}_i\\)\nEs gilt, dass die abhängige und unabhängige Variablen metrisch skaliert sein müssen. Ordinale Variablen, sofern die Annahme gleicher Abstände zwischen den Ausprägungen stimmt, können in das Modell übernommen werden (sie werden dann als pseudo-metrisch behandelt). Auch bei der multivariaten linearen Regression wird das OLS-Verfahren zur Berechnung der Regression genutzt.\nKommen wir wieder auf unser Beispiel der Zufriedenheit mit der Demokratie zurück: im bivariaten Fall wollten wir die Zufriedenheit mit der Demokratie über die Zufriedenheit mit der ökonomischen Leistung einer Person erklären. Als weiteren erklärenden Faktor könnten wir annehmen, dass das Einkommen einer Person ebenfalls Einfluss auf die Höhe der Zufriedenheit mit der Demokratie hat. Damit würde sich folgende Gleichung für die Schätzung des Zufriedenheit mit der Demokratie ergeben:\n\\(satdem_i= \\beta_0 + \\beta_1 \\ast sateco_i + \\beta_2 \\ast Einkommen_i + e_i\\)\nWir nehmen also an, dass sich die Zufriedenheit mit der Demokratie einer Person über die unabhängigen Variablen Zufriedenheit mit der ökonomischen Leistung und Einkommen dieser Person erklären lässt.\nIm Nachfolgenden wird nun die Interpretation der wichtigsten Kennzahlen einer Regression am genannte Beispiel erklärt.\nSiehe auch: Urban \u0026amp; Mayerl (2011, Kapitel 2.3); Fromm (2010, Kapitel 3.2.2).\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/chapter3/subchapter1/",
	"title": "Kennzahlen",
	"tags": ["kennzahlen", "regression"],
	"description": "",
	"content": "Kapitel 3.1 Kennzahlen Ein Vorteil der linearen Regression ist, dass die Interpretation dieser Berechnung auf wenige Kennzahlen beschränkt ist. In der Anwendung ist dieses Verfahren durchaus anspruchsvoll, in der Interpretation genügt dagegen schon die Kenntnis weniger Werte. Für die Interpretation einer Regression sind vier Kennzahlen wichtig: Das Bestimmtsheitsmaß R2, der F-Test des Bestimmtheitsmaßes als Signifikanz des Modells, die Regressionskoeffizienten und die Signifikanz der einzelnen Regressionskoeffizienten.\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/chapter3/subchapter1/page-3-1-a/",
	"title": "Bestimmtheitsmaß",
	"tags": ["bestimmtheitsmaß", "$r^2$", "regression"],
	"description": "",
	"content": " Um zu messen, wie aussagekräftig die Regression ist, wird über eine Varianzzerlegung das Bestimmtheitsmaß \\(R^2\\) bestimmt. Vereinfacht dargestellt beschreibt \\(R^2\\) den Anteil der erklärten Varianz am Gesamtanteil.\nUmgangssprachlich könnte man sagen, dass \\(R^2\\) angibt, wie viel die unabhängigen Variablen (an der Varianz) der abhängigen Variable erklären. Es ist somit ein Maß der Güte des Modells. Das Ziel ist es, ein Modell zu finden, dass möglichst einen großen Anteil der Varianz einer abhängigen Variable erklären kann (also ein hohes \\(R^2\\) aufweist).\n\\(R^2\\) kann Werte zwischen \\(0\\) und \\(1\\) annehmen. Ein \\(R^2=1\\) gibt an, dass das Modell die komplette Varianz der abhängigen Variable erklärt, ein \\(R^2=0\\) gibt an, dass das Modell keinen Anteil an der Erklärung der Varianz der abhängigen Variable hat. In diesem Fall ist das Modell unbrauchbar.\nDie Varianzzerlegung zerlegt den Anteil der Varianz des Gesamtmodells auf die Schätzung (Regressionsgerade) und die Residuen.\nDie Formel ist folgende: Varianzzerlegung: \\(\\sum\\limits_{i=1}^n (y_i - \\bar{y})^2 = \\sum\\limits_{i=1}^n (\\hat{y}_i - \\bar{y})^2 + \\sum\\limits_{i=1}^n (y_i - \\hat{y}_i)^2\\) \\(SS_{total} = SS_{model} + SS_{residual}\\)\n\\(SS_Y = SS_X + SS_e\\)\n\\(\\hat{y}_i: \\thinspace geschätzte \\thinspace Werte, \\thinspace y_i: \\thinspace beobachtete \\thinspace Werte, \\thinspace \\bar{y}: \\thinspace Mittelwert \\thinspace der \\thinspace beobachteten \\thinspace Werte\\)\nDas Bestimmtheitsmaß \\(R^2\\) errechnet sich dann wie folgt:\n\\(R^2 = \\frac {SS_{model}} {SS_{total}}\\), wobei \\(R^2 \\in [0;1]\\)\nBei multivariaten Regressionsmodellen wird \\(R^2\\) um die Anzahl der Beobachtungen angepasst und als korrigiertes \\(\\thinspace R^2\\) ausgegeben. Die Formel des \\(korrigierten \\thinspace bzw. \\thinspace angepassten \\thinspace R^2\\) lautet:\n\\(R_{korr}^2 =1- \\frac {(1-R^2) \\ast (n-1)} {(n-p-1)}\\).\nDies ist notwendig, da \\(R^2\\) allein aufgrund der Hinzunahme weiterer Variablen steigt.\nSehen wir uns nun an unserem Beispiel den Auszug des multivariaten linearen Regressionsmodells für Zufriedenheit mit Demokratie aus dem Statistikprogramm SPSS und R an.\nModellblock lineare Regression (SPSS) Modell lineare Regression (R) In der Modellübersicht in SPSS sehen wir die Angabe von \\(R^2\\) und dem \\(angepassten\\thinspace R^2\\). Da es sich um ein multivariates Modell handelt, müssen wir letzteres interpretieren. Der Wert des \\(angepassten\\thinspace R^2\\) beträgt \\(0.976\\). Das heißt, dass unser Modell mit den unabhängigen Variablen politisches Interesse und Alter \\(97.6 \\%\\) der Varianz der abhängigen Variable politisches Wissen erklärt. Bitte beachten Sie, dass dieser Wert untypisch hoch ist. Wir haben hier einen Trainings-Datensatz, um das Beispiel zu veranschaulichen. Mit real erhobenen Datensätzen liegen diese Werte in der Regel zwischen \\(0.15\\) und \\(0.6\\).\nIm Modell in R wird \\(R^2\\) in der vorletzten Zeile ausgegeben. Auch hier nehmen wir den Wert des Adjusted \\(R^2\\), der hier mit \\(0.9761\\) angegeben ist (also um eine Nachkommastelle genauer).\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/chapter3/subchapter1/page-3-1-b/",
	"title": "F-Test",
	"tags": ["f-test", "regression"],
	"description": "",
	"content": " Neben \\(R^2\\) muss ein weiteres Modellmaß interpretiert werden: der \\(F\\)-Test des Bestimmtheitsmaßes: Der \\(F\\)-Test ist ein Hypothesentest und prüft in der Nullhypothese, dass das Bestimmtheitsmaß \\(R^2\\) \\(=0\\) ist, also dass das zugrunde gelegte Modell keine Varianz der abhängigen Variable erklärt. Trifft diese Nullhypothese zu, muss das Modell verworfen werden. In Veröffentlichungen findet man daher nur Modelle, die diesen \\(F\\)-Test „bestanden haben“, also in denen mindestens ein Regressionskoeffizient einer unabhängigen Variable von \\(0\\) abweicht.\nWie interpretieren wir den \\(F\\)-Test nun: In der Abbildung sehen wir einen weiteren Auszug aus der Ausgabe unserer Beispielregression in SPSS.\nF-Test (lineare Regression) (SPSS) F-Test (lineare Regression) (R) Im Output von SPSS sehen wir In der Spalte \\(F\\) den \\(F\\)-Wert, entsprechend der Freiheitsgerade und des Konfidenzintervalls können wir entsprechend einer Tabelle der \\(F\\)-Verteilung ablesen, ob der \\(F\\)-Wert über dem Grenzwert liegt. Dieser Schritt ist hinfällig, da das Computerprogramm SPSS dies in der Spalte Sig. direkt ausgibt. SPSS gibt hier den Signifikanzwert des \\(F\\)-Tests direkt an. Dieser beträgt im Beispiel \\(0.000\\). Dieser \\(p\\)-Wert ist unterhalb der konventionellen \\(5\\%\\)-Hürde (p-Wert \\(0.05\\)) und damit kann die Nullhypothese (dass das Modell keine Varianz der abhängigen Variable erklärt) verworfen werden und das Modell weiter interpretiert werden. Im Output der Software R müssen wir in die letzte Zeile sehen. Dort ist zuerst der gerundete F-Wert angegeben und die Parameter der Verteilung und anschließend der p-Wert, hier mit \\(\u0026lt; 2.2e^{-16}\\). Dieser Wert ist also auch kleiner als \\(0.05\\), denn \\(2.2e^{-16} = 0.00000000000000022\\).\nFür eine weitergehende Auseinandersetzung, vor allem zur Berechnung der Teststatistik, ist folgende Literatur zu empfehlen: Urban \u0026amp; Mayerl (2011, Kapitel 3.4.2, Kapitel 3.3.3 \u0026amp; Kapitel 3.4.3).\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/chapter3/subchapter1/page-3-1-c/",
	"title": "Regressionskoeffizienten",
	"tags": ["regressionskoeffizienten", "regression"],
	"description": "",
	"content": " In Veröffentlichungen von Regressionsmodellen finden Sie neben der Angabe des Bestimmtheitsmaßes \\(R^2\\) auch immer die Angabe der einzelnen Regressionskoeffizienten und deren Signifikanz. Denn in den meisten Regressionsmodellen geht es nicht nur um die Erklärungskraft eines Modells, sondern auch darum, wie einzelne Faktoren auf die abhängige Variable wirken. Daher ist die Angabe und die Interpretation der Regressionskoeffizienten wichtig.\nIn der Abbildung sehen wir wieder einen Auszug der Beispielregression: Im Computerprogramm SPSS wird dieser als Koeffizientenblock bezeichnet, in R finden wir diese auch wie zuvor in der normalen Modell-Ausgabe.\nKoeffizientenblock Regression (SPSS) Modell lineare Regression unstandardisierte Regressionskoeffizienten (R) Modell lineare Regression standardisierte Regressionskoeffizienten (R) Generell unterscheiden wir zwischen standardisierten und nicht-standardisierten Regressionskoeffizienten. In SPSS werden beide direkt ausgegeben, in R die unstandardisierten (sofern man nicht vorher die Variablen standardisiert). In dem man die Variablen standardisiert und die Regression erneut berechnet (untere R-Tabelle) oder spezielle Funktionen anwendet, erhält man auch in R die standardisierten Regressionskoeffizienten. In Veröffentlichungen werden meist entweder die standardisierten oder die nicht-standardisierten Regressionskoeffizienten angegeben.\nNicht-standardisiert bedeutet, dass die Regressionskoeffizienten in der Maßeinheit der Variable ausgegeben werden. In unserem Beispiel werden daher die nicht-standardisierten Regressionskoeffizienten wie folgt interpretiert: Mit einer Steigerung um eine Einheit auf der Variable Zufriedenheit mit der ökonomischen Leistung (Einheit ist ein Jahr), steigt die Zufriedenheit mit der Demokratie um \\(0.544\\) Punkte. Mit einer Steigerung um eine Einheit auf der Variable Einkommen (\\(1\\) Euro) steigt die Zufriedenheit mit der Demokratie um \\(0.001\\) Punkte.\nDoch welche Variable ist nun einflussreicher? Mithilfe der nicht-standardisierten Regressionskoeffizienten kann diese Frage nicht beantwortet werden, da die Wirkung auf die abhängige Variable in unterschiedlichen Einheiten angegeben ist. Zum Vergleich der Stärke der Koeffizienten innerhalb des Modells eignen sich nicht-standardisierte Regressionskoeffizienten also nicht.\nStandardisierte Regressionskoeffizienten sind über die Standardabweichung (z-Transformation: \\(\\beta_k = b_k\\ast \\frac {s_{x_k}} {s_y}\\)) von der Maßeinheit der Variablen losgelöst. Die Maßeinheit der standardisierten Regressionskoeffizienten ist die Standardabweichung. Damit kann die unterschiedliche Stärke des Einflusses der Koeffizienten innerhalb eines Modells verglichen werden.\nInterpretativ sagt dieser Wert dann aus, um wie viel Standardabweichungen sich die abhängige Variable bei Erhöhen um eine Standardabweichung der unabhängigen Variable ändert. Der Wertebereich reicht in aller Regel von \\([-1;1]\\). Ein Wert von \\(-1\\) ist dabei als perfekt inverse Beziehung zu deuten (\\(y\\) sinkt um eine Standardabweichung, wenn x um eine Standardabweichung steigt) und bei \\(+1\\) als perfekt positiver Zusammenhang. Werte von größer \\(1\\) oder kleiner \\(−1\\) bei standardisierten Regressionskoeffizienten weisen auf Multikollinearität (Variablen korrelieren untereinander zu stark) zwischen Variablen hin.\nIm Beispiel von oben steigt die Zufriedenheit mit der Demokratie um \\(0.607\\) Standardabweichungen, wenn die Zufriedenheit mit der ökonomischen Leistung um eine Standardabweichung erhöht wird. Die Zufriedenheit mit der Demokratie steigt dagegen um \\(0.385\\) Standardabweichungen, wenn das Einkommen um eine Standardabweichung steigt. Der Effekt der Zufriedenheit mit der ökonomischen Leistung ist damit größer als der Effekt des Einkommens.\nVorsicht ist geboten bei modellübergreifender Interpretation. Die standardisierten Regressionskoeffizienten lassen sich nämlich nur innerhalb eines Modells vergleichen, da diese über die Varianz berechnet werden und in zwei unterschiedlichen Stichproben können die Varianzen differieren. Es würde sich daher für dieselbe Rechnung unterschiedliche standardisierte Regressionskoeffizienten ergeben.\nRegressionskoeffizient Maßeinheit Interpretation unstandardisiert Einheit der jeweiligen Variablen kein Vergleich der unabhängigen Variablen im selben Modell möglich; Interpretation leichter, da in Maßeinheit der Variablen standardisiert Standardabweichung relative Stärke zwischen unabhängigen Variablen im selben Modell vergleichbar Die Regressionskoeffizienten werden jeweils auch auf Signifikanz über einen t-Test geprüft. Zur Wiederholung: Es wird also überprüft, ob der angegebene Wert in der Stichprobe auf den Wert der Grundgesamtheit übertragen werden kann. Hieraus ergibt sich forschungspraktisch, welche Regressionskoeffizienten interpretierbar sind, da die signifikanten Regressionskoeffizienten Bedeutung über die Stichprobe hinaus haben. In den meisten Veröffentlichungen wird dies mit Sternen (\\(\\star\\), \\(\\star \\star\\), \\(\\star \\star \\star\\)) markiert. Welche Sterne für welches Signifikanzniveau gelten, ist unterschiedlich und ist immer unterhalb einer Regressionstabelle angegeben. Üblich ist folgende Konvention:\nZeichen Signifikanzniveau \\(\\dag\\) 0.10 * 0.05 ** 0.01 *** 0.001 Im Beispiel von oben befinden sich der t-Wert jedes einzelnen Regressionskoeffizienten in der Spalte t-Wert und die Signifikanz entsprechend in der Spalte Sig (SPSS-Ausgabe). Sowohl für die Zufriedenheit mit der ökonomischen Leistung als auch das Einkommen ist die Signifikanz gegeben (\\(p\\)-Wert unter kritischer Grenze von \\(0,05\\)).\nIn der Ausgabe von R sehen wir im Block Coefficients: in der vorletzten Spalte den \\(t\\)-Wert und in der letzten Wahrscheinlichkeit die Signifikanz. Auch hier zeigt sich, dass beide Variablen einen signifikanten Einfluss haben.\nSiehe auch: Urban \u0026amp; Mayerl (2011, Kapitel 3.4.1).\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/chapter3/subchapter1/page-3-1-d/",
	"title": "Lernvideo",
	"tags": ["Kennzahlen", "regression", "lernvideo"],
	"description": "",
	"content": " In diesem Lernvideo werden die drei Schritte nochmals dargestellt:\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/chapter3/subchapter2/",
	"title": "Kategoriale Variablen",
	"tags": ["kategorial", "regression"],
	"description": "",
	"content": "Kapitel 3.2 Kategoriale Variablen Bisher wurden nur metrische Variablen in das Regressionsmodell aufgenommen. Als zusätzliche Variable könnte zum Beispiel das Geschlecht die Höhe des Einkommens erklären (ein weiteres Beispiel finden Sie im Lernvideo am Ende der Seite). Das Geschlecht ist eine Variable, die kein metrisches Skalenniveau aufweist, sondern ein nominales. Da viele Variablen in den Sozialwissenschaften ein nominales oder ordinales Skalenniveau aufweisen, gibt es eine Möglichkeit, diese in ein Regressionsmodell einzufügen. Dieses Verfahren wird Dummy-Kodierung genannt. Dazu sind Umkodierungen der Variable und Grundlagen der Datenkodierung zum Verständnis notwendig.\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/chapter3/subchapter2/page-3-2-a/",
	"title": "Beispiel 1",
	"tags": ["kategorial", "regression"],
	"description": "",
	"content": " Nun möchten wir das Bildungslevel der Befragten in die Regressionsrechnung einfügen. Insgesamt hat diese Variable drei Ausprägungen: \\((1)\\) Haupt-/Realschule, \\((2)\\) Abitur und \\((3)\\) mind. BA-Abschluss. Der Bildungslevel beschreibt die höchste formale Bildungsqualifikation.\nEin Statistikprogramm kann ausschließlich mit Zahlen operieren. Warum wir die Variable nicht wie eine metrische Variable einfügen können, erklärt folgendes Problem: Bei metrischen Variablen können wir die Steigerung um eine Einheit interpretieren, da die Abstände eindeutig sind. Bei einer Variable wie dem Bildungslevel können wir dies nicht. Wir können den Abstand zwischen Abitur und mind. BA-Abschluss nicht mit dem Zahlenwert \\(1\\) gleichsetzen (den wir aufgrund der Kodierung als Abstand identifizieren würden). Daher können kategoriale Variablen nicht einfach in ein Regressionsmodell als unabhängige Variable eingefügt werden.\nUm nominale und ordinale Variablen als unabhängige Variablen in einem Regressionsmodell interpretieren zu können, ist es notwendig, sich einen Schritt der Codierung zu verdeutlichen.\nDazu wird nun der Bildungslevel in das Regressionsmodell übernommen. Da diese Variable polytom ist, müssen wir Dummy-Variablen bilden. Wir bilden soviele Dummy-Variablen, wie die Variable Ausprägungen hat - also 3! Jede Dummy-Variable ist eine Indikatorvariable, die anzeigt, ob das Merkmal vorhanden ist (\\(0\\)) oder ob das Merkmal nicht vorhanden ist (\\(1\\)). Der Dummy für die Ausprägung Haupt-/Realschule hat den Wert \\(1\\) für Personen, die diesen als höchsten Bildungsabschluss haben und der Dummy nimmt den Wert \\(0\\) an, wenn ein höherer Abschluss vorliegt (in diesem Fall für alle Personen mit Abitur oder mind. BA-Abschluss). Der Dummy für die Ausprägung Abitur nimmt den Wert \\(1\\) an für Personen mit höchstem Bildungsabschluss Abitur und den Wert \\(0\\) für Personen, die entweder nur einen niedrigeren Bildungsabschluss haben (Personen mit Haupt-/Realschule) oder einen höheren Abschluss haben (Personen mit mind BA-Abschluss). Wir rekodieren also die Ursprungsvariable auf das Format \\(0/1\\).\nIn die Regressionsgleichung können nicht alle Dummy-Variablen übernommen werden. Es muss immer eine Ausprägung außen vor bleiben, damit die Gleichung mathematisch lösbar ist (mehr dazu auf der nächsten Seite). Die ausgelassene Kategorie bildet die Referenzkategorie. Im Beispiel lassen wir die niedrigste Ausprägung aus und fügen nur Abitur und mind. BA-Abschluss der Gleichung hinzu. Die Effekte dieser beiden Dummies interpretieren wir dann in Referenz zu Ausprägung Haupt-/Realschule.\nUnsere Regressionsgleichung sieht daher für die Schätzung jedes \\(y\\)-Wertes (Zufriedenheit mit der Demokratie eines beobachteten Individuums) wie folgt aus:\n\\(satdem_i= \\beta_0 + \\beta_1 \\ast sateco_i + \\beta_2 \\ast Einkommen_i + \\beta_3 \\ast Abitur_i + \\beta_4 \\ast BA-Abschluss_i\\)\nDer Effekt für Personen mit Haupt-/Realschule als höchstem formalen Bildungsabschluss ist in der Konstanten (\\(\\beta_0\\)) gefasst. Dazu müssen wir uns nur kurz verinnerlichen, was mit der Modellgleichung geschieht, wenn alle Variablen den Wert \\(0\\) annehmen:\n\\(Wissen_i = \\beta_0 + \\beta_1 \\ast 0+ \\beta_2 \\ast 0+ \\beta_3 \\ast 0 + \\beta_4 \\ast 0\\)\nDie errechnete Zufriedenheit mit der Demokratie würde im Beispiel also für Personen gelten, die auf der Zufriedenheit mit der ökonomischen Leistung den Wert \\(0\\) angegeben haben, gar kein Einkommen haben (Wert \\(0\\)), und weder Abitur noch BA-Abschluss als höchsten BIldungabschluss besitzen (Wert \\(0\\)). Dieser Schritt ist nur zur Verdeutlichung der Referenzkategorie. In aller Regel interpretieren wir in den Sozialwissenschaften die Konstante nicht.\nEs wird also nur für Abitur und BA-Abschluss (und nicht für Haupt-/Realschule) ein Regressionskoeffizient in Referenz zu Haupt-/Realschule berechnet. Das heißt der Regressionskoeffizient \\(\\beta_3\\), dem Regressionskoeffizienten für Abitur, gibt die Veränderung der Zufriedenheit mit der Demokratie von Befragten an, die als höchsten Bildungsabschluss das Abitur besitzen. Analog gibt der Regressionskoeffizient \\(\\beta_4\\) den zusätzlichen Effekt für Personen an, die als höchsten Bildungsabschluss mind. BA-Abschluss haben.\nHier die Interpretation wieder im Beispiel erklärt:\nKoeffizientenblock Regression mit Dummy-Variablen (SPSS) Modell lineare Regression mit Dummy-Variablen (R) Wir sehen, dass Personen mit höchstem Bildungsabschluss Abitur eine um \\(0.77\\) Punkte höhere Zufriedenheit mit der Demokratie im Vergleich zu Personen mit höchstem Bildungsabschluss Haupt-/Realschule haben. Personen mit höchstem Bildungsabschluss mind. BA-Abschluss haben eine um \\(0.82\\) Punkte höhere Zufriedenheit mit der Demokratie im Vergleich zu Personen mit höchstem Bildungsabschluss Haupt-/Realschule haben.\nDies lässt sich auch gut grafisch darstellen, um die dummykodierten Variable zu verstehen. Sie sehen den Effekt von Zufriedenheit mit der Demokratie in Abhängigkeit des Bildungslevels auf die abhängige Variable Zufriedenheit mit der Demokratie. Die obere grüne Linie ist die Regressionslinie für Personen, die mind. BA-Abschluss besitzen, die mittlere rote für Personen, die das Abitur haben und die untere blaue Linie ist für Personen, die als höchsten Bildungsabschluss Haupt-/Realschule besitzen. Das Einkommen wurde in dieser Darstellung konstant gehalten.\nBeispiel Regression mit Dummy-Variable Wie sich sehen lässt, hat der Regressionskoeffizient der Dummy-Variable den Effekt, dass bei Vorhandensein der Merkmalsausprägung die Linie parallel verschoben wird. In diesem Fall im Vergleich zur Referenzkategorie auf der y-Achse parallel nach unten, da der Regressionskoeffizient negativ ist (nach oben, wenn er positiv ist).\nÄhnlich verhält es sich bei ordinalen oder nominalen Variablen mit nur zwei Ausprägungen, zum Beispiel dem Geschlecht. Es gilt immer, dass die Anzahl der dummykodierten Variablen in der Regressionsgleichung wie folgt bestimmt ist:\n\\(Anzahl \\thinspace Dummys = Anzahl \\thinspace Merkmalsausprägungen - 1\\)\nDies ist leicht zu verstehen, da eine Merkmalsausprägung immer die Referenzkategorie bilden muss (also wenn alle eingefügten Dummys \\(0\\) sind). Bei einer dichotomen Variable muss daher nur ein Dummy der Regression hinzugefügt werden. Wenn wir kein Dummy auslassen, könnten wir die Regressionskoeffizienten nicht in Referenz zu einer Ausprägung interpretieren. Auf der nächsten Seite zeigen wir in zwei Lernvideos die Codierung von Dummy-Variablen und die grafische Interpretation der Dummy-codierten Variablen an einem weiteren Beispiel. Ebenso wird erklärt, wie die Regressionsgleichung interpretiert werden kann.\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/chapter3/subchapter2/page-3-2-b/",
	"title": "Beispiel 2",
	"tags": ["kategorial", "regression"],
	"description": "",
	"content": " In den zwei Lernvideos wird zum einen gezeigt wie Dummy-Variablen grafisch zu interpretieren sind und wie Dummy-Variablen gebildet werden.\nNehmen wir ein weiteres Beispiel für dummykodierte Variablen mit ordinalen Skalenniveau: Wir möchten testen, welchen Einfluss - neben den bereits genutzten Variable (Alter, Arbeitszeit, Geschlecht) - der Schulabschluss auf das Einkommen einer Person hat. Der Schulabschluss ist in unserem Beispiel mit drei Merkmalsausprägungen kodiert:\n\\(1:Hauptschule\\)\n\\(2:Realschule\\)\n\\(3:(Fach−)Abitur\\)\nWelche Ausprägung die Referenzkategorie ist, liegt in der Entscheidung der Forscher:in. Oftmals wird entlang der gebildeten Hypothesen entschieden, welche Ausprägung die Referenzkategorie bildet, da so die vermutete Differenz zwischen den Merkmalsausprägungen getestet werden kann. Sollte eine Variable sehr viele Ausprägungen haben, ist es sinnvoll, Ausprägungen vorher zusammenzufassen.\nIn diesem Beispiel des Schulabschlusses könnte man den höchsten Abschluss (Fach-)Abitur als Referenzkategorie wählen und zwei Dummy-Variablen mit Hauptschule (Dummy-Variable 1) und Realschule (Dummy-Variable 2) in die Regressionsrechnung einfügen.\nDummy-Bildung aus Variable Schulabschluss Im Beispiel ist (Fach-)Abitur die Referenzkategorie und die erste Dummy-Variable würde also die Steigerung/Minderung des Einkommens von Personen mit Hauptschulabschluss gegenüber Personen mit (Fach-)Abitur messen, die zweite Dummy-Variable von Personen mit Realschulabschluss in Referenz zu Personen mit (Fach-)Abitur. Beide Variablen sind \\(0/1\\)-kodiert, also mit Merkmal vorhanden (\\(1\\)) und Merkmal nicht vorhanden (\\(0\\)).\nDie Schätzung einer einzelnen Beobachtung würde daher wie folgt aussehen:\n\\(Einkommen_i = \\beta_0 + \\beta_1 \\ast Alter_i + \\beta_2 \\ast Arbeitszeit_i + \\beta_3 \\ast Geschlecht_i + \\beta_4 * dummy_{HS_i} + \\beta_5 * dummy_{RS_i}\\)\nJetzt vergleichen wir die Gleichungen von verschiedenen Personen hinsichtlich ihrer Ausprägungen auf den Dummy-Variablen und nehmen an, dass alle Personen im Beispiel \\(40\\) Jahre alt sind und \\(40\\) Stunden arbeiten.\nBeispiel 1: Person 1 ist weiblich und hat Abitur \\(Einkommen_1 = \\beta_0 + \\beta_1 \\ast 40 + \\beta_2 \\ast 40 + \\beta_3 \\ast 0 + \\beta_4 * 0 + \\beta_5 * 0 = \\beta_0 + \\beta_1 * 40 + \\beta_2 * 40\\)\nBeispiel 2: Person 2 ist männlich und hat Realschulabschluss \\(Einkommen_1 = \\beta_0 + \\beta_1 \\ast 40 + \\beta_2 \\ast 40 + \\beta_3 \\ast 1+ \\beta_4 * 0 + \\beta_5 * 1 = \\beta_0 + \\beta_1 * 40 + \\beta_2 * 40 + \\beta_3 * 1 + \\beta_5 * 1\\)\nBeispiel 3: Person 3 ist weiblich und hat Hauptschulabschluss \\(Einkommen_1 = \\beta_0 + \\beta_1 \\ast 40 + \\beta_2 \\ast 40 + \\beta_3 \\ast 0 + \\beta_4 * 1 + \\beta_5 * 0 = \\beta_0 + \\beta_1 * 40 + \\beta_2 * 40 + \\beta_4 * 1\\)\nWir erkennen an den drei Beispielen nun folgendes: Die Referenzkategorien weiblich für Geschlecht und (Fach-)Abitur für Schulabschluss sind nicht mit eigenen Regressionskoeffizienten angegeben. Beide Effekte sind Referenzen zu den jeweiligen Dummys (männlich, Hauptschule und Realschule). Die Konstante (\\(\\beta_0\\)) fasst die Referenzkategorien mathematisch und daher können diese Effekte nicht einzeln betrachtet werden. Daher ist es von Nöten, bevor ordinale Variablen (oder polytome nominale Variablen) in ein Regressionsmodell aufgenommen werden, sich klar zu machen, über welche Ausprägungen man eine Aussage treffen möchte und die Referenzkategorien dementsprechend zu wählen.\nNun wollen wir zurück auf unser Beispiel kommen und die Interpretation errechneter Werte:\nSPSS Koeffizientenblock R-Output Für unsere drei Beispiel-Personen ergeben sich folgende geschätzte Einkommenswerte in der Stichprobe:\nBeispiel 1: Person 1 ist weiblich und hat Abitur \\(Einkommen_1 = \\beta_0 + \\beta_1 \\ast 40 + \\beta_2 \\ast 40 + \\beta_3 \\ast 0 + \\beta_4 * 0 + \\beta_5 * 0 = 2092.624 + (-3.009) * 40 + 4.071 * 40 = 2135.104\\)\nBeispiel 2: Person 2 ist männlich und hat Realschulabschluss \\(Einkommen_2 = \\beta_0 + \\beta_1 \\ast 40 + \\beta_2 \\ast 40 + \\beta_3 \\ast 1+ \\beta_4 * 0 + \\beta_5 * 1 = 2092.624 + (-3.009) * 40 + 4.071 * 40 + 381.848 * 1\\) \\(+ (-437.378) * 1=2079.574\\)\nBeispiel 3: Person 3 ist weiblich und hat Hauptschulabschluss \\(Einkommen_3 = \\beta_0 + \\beta_1 \\ast 40 + \\beta_2 \\ast 40 + \\beta_3 \\ast 0 + \\beta_4 * 1 + \\beta_5 * 0 = 2092.624 + (-3.009) * 40 + 4.071 * 40 + (-1131.309)* 1 = 1003.795\\)\nPersonen, die männlich sind, verdienen knapp \\(382\\) Euro mehr als weibliche Personen (\\(\\beta_3\\)). Personen mit Realschulabschluss verdienen knapp \\(437\\) Euro weniger als Personen mit (Fach-)Abitur (\\(\\beta_4\\)) und Personen mit Hauptschulabschluss verdienen sogar \\(1131\\) Euro weniger als Personen mit (Fach-)Abitur (\\(\\beta_5\\)).\nSiehe auch: Urban \u0026amp; Mayerl (2011, Kapitel 5.1.1).\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/glossar/",
	"title": "Glossar",
	"tags": [],
	"description": "",
	"content": "Glossar Hier finden sich alle Begriffe aufgelistet, die im Lernmodul genutzt werden.\nabhängige Stichprobe abhängige Variable Alternativhypothese Bestimmtheitsmaß bewusste Auswahl bivariat dichotom dummykodierte Variable Einseitiger Test Erwartungswert F-Verteilung Grundgesamtheit Homoskedastizität Inferenzstatistik Irrtumswahrscheinlichkeit kategoriales Skalenniveau Kausalität Konfidenzintervall Korrelation Kritischer Wert lineare Regression Merkmalsausprägung metrisches Skalenniveau Mittelwert Multivariat Nominal-Skala Normalverteilung Nullhypothese Ordinal-Skala Ordinary-Least-Squares-Verfahren Pearsons r polytom pseudometrisch quantitative Sozialforschung Referenzkategorie Regressionskoeffizienten Residuum Scatterplot Signifikanzniveau Signifikanztest Skalenniveau Standardabweichung Stichprobe Stichprobenumfang Störvariable t-Test (Regression) t-Verteilung unabhängige Stichprobe unabhängige Variable Variable Varianz Vertrauenswahrscheinlichkeit Willkürliche Auswahl z-Verteilung Zufallsstichprobe zweiseitiger Signifikanztest "
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/bibliography/",
	"title": "Literatur",
	"tags": [],
	"description": "",
	"content": " Literaturnachweise Behnke, J., \u0026amp; Behnke, N. (2006). Grundlagen der statistischen datenanalyse. Eine einführung für politikwissenschaftler. VS Verlag für Sozialwissenschaften. Bortz, J. (2005). Statistik für human- und sozialwissenschaftler (6th ed.). Springer Medizin Verlag. Bortz, J., \u0026amp; Schuster, C. (2010). Statistik für human- und sozialwissenschaftler (7th ed.). Springer-Lehrbuch. Cleff, T. (2011). Deskriptive statistik und moderne datenanalyse: Eine copmutergestützte einführung mit excel, PASW (SPSS) und STATA (2nd ed.). Springer Fachmedien. Diekmann, A. (2008). Empirische sozialforschung. Grundlagen, methoden, anwendungen (19th ed.). Rowohlt Taschenbuch Verlag. Fromm, S. (2010). Datenanalyse mit SPSS für fortgeschrittene 2: Multivariate verfahren für querschnittsdaten. VS Verlag für Sozialwissenschaften. Gehring, U. W., \u0026amp; Weins, C. (2009). Grundkurs statistik für politologen und soziologen (5th ed.). VS Verlag für Sozialwissenschaften. Janssen, J., \u0026amp; Laatz, W. (2013). Statistische datenanalyse mit SPSS. Eine anwendungsorientierte einführung in das basissystem und das modul exakte tests (8th ed.). Springer Gabler. Kromrey, H., Roose, J., \u0026amp; Strübing, J. (2016). Empirische sozialforschung (13th ed.). UVK Verlagsgesellschaft mbH. Kühnel, S., \u0026amp; Krebs, D. (2010). Statistik für die sozialwissenschaften. Grundlagen, methoden, anwendungen (5th ed.). Rowohlt Taschenbuch Verlag. Schnell, R., Hill, P. B., \u0026amp; Esser, E. (2013). Methoden der empirischen sozialforschung (10th ed.). Oldenbourg Verlag. Urban, D., \u0026amp; Mayerl, J. (2011). Regressionsanalyse: Theorie, technik und anwendung (4th ed.). VS Verlag für Sozialwissenschaften. "
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/tags/r2/",
	"title": "$r^2$",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/glossar/abhaengigestichprobe/",
	"title": "abhängige Stichprobe",
	"tags": [],
	"description": "",
	"content": " liegt vor, wenn jedem Wert der einen Stichprobe eindeutig und sinnvoll ein Wert der anderen Stichprobe zugeordnet werden kann (z.B. Mutter und Kind; Messung eines Individuums zu zwei Zeitpunkten)\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/glossar/abhaengigevariable/",
	"title": "abhängige Variable",
	"tags": [],
	"description": "",
	"content": " Die abhängige Variable (Kurzform: aV) ist die Variable, die beeinflusst wird (Wirkung). In Gleichungen wird diese mit einem \\(Y\\) gekennzeichnet. Sie wird auch als Zielvariable oder zu erklärende Variable bezeichnet.\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/glossar/alternativhypothese/",
	"title": "Alternativhypothese",
	"tags": [],
	"description": "",
	"content": " Ungetestete Hypothese in einem Hypothesentest; wird angenommen, wenn Nullhypothese verworfen wird; die Alternativhypothese nimmt in den Sozialwissenschaften inhaltlich die theoretische Vermutung der Forschenden ein.\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/tags/bestimmtheitsma%C3%9F/",
	"title": "bestimmtheitsmaß",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/glossar/bestimmtheitsmass/",
	"title": "Bestimmtheitsmaß",
	"tags": [],
	"description": "",
	"content": " \\(R^2\\); Gütemaß einer Modellregression; nimmt Werte zwischen 0 und 1 an; je höher \\(R^2\\), desto mehr erklären die unabhängigen Variablen die Varianz der abhängigen; umgangssprachlich gibt \\(R^2\\) die Prozente an, die das Regressionsmodell erklärt; Beispiel \\(R^2= 0,54\\) \\(\\Rightarrow\\) erklärt 54 % der Varianz der abhängigen Variable.\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/glossar/bewussteauswahl/",
	"title": "bewusste Auswahl",
	"tags": [],
	"description": "",
	"content": " Verfahren der nicht-wahrscheinlichkeitsbasierten Auswahl einer Stichprobe; gezielte Auswahl („typischer Fall“; „ausschlaggebender Fall“) ; oft genutzt in explorativen Studien; Auswahl ist kriteriengeleitet.\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/tags/bivariat/",
	"title": "bivariat",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/glossar/bivariat/",
	"title": "bivariat",
	"tags": [],
	"description": "",
	"content": " betrachtet man die gemeinsame Verteilung zweier Merkmale bzw. Variablen, spricht man von einer bivariaten Analyse.\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/credits/",
	"title": "Credits",
	"tags": [],
	"description": "",
	"content": "Contributors Web-Based-Training (since v1.0) @B. Philipp Kleer 19 commits Contributors to hugo-learner-theme Thanks to them for making Open Source Software a better place!\n@matcornic 158 commits @matalo33 48 commits @coliff 19 commits @lierdakil 16 commits @mdavids 10 commits @ozobi 5 commits @Xipas 5 commits @Alan-Cha 4 commits @pdelaby 4 commits @helfper 4 commits @Chris-Greaves 3 commits @mreithub 3 commits @massimeddu 3 commits @LinuxSuRen 3 commits @dptelecom 3 commits @willwade 3 commits @diemol 2 commits @denisvm 2 commits @hucste 2 commits @ImgBotApp 2 commits @jamesbooker 2 commits @jice-lavocat 2 commits @wikijm 2 commits @lfalin 2 commits @JianLoong 2 commits @armsnyder 1 commits @afilini 1 commits @MrAkaki 1 commits @AmirLavasani 1 commits @afs2015 1 commits @arifpedia 1 commits @berryp 1 commits @MrMoio 1 commits @ChrisLasar 1 commits @DCsunset 1 commits @IEvangelist 1 commits @fritzmg 1 commits @bogaertg 1 commits @geoffreybauduin 1 commits @giuliov 1 commits @haitch 1 commits @zeegin 1 commits @RealOrangeOne 1 commits @jared-stehler 1 commits @JohnBlood 1 commits @JohnAllen2tgt 1 commits @kamilchm 1 commits @gwleclerc 1 commits @lloydbenson 1 commits @massimocireddu 1 commits @sykesm 1 commits @nvasudevan 1 commits @nnja 1 commits @owulveryck 1 commits @654wak654 1 commits @PierreAdam 1 commits @qiwenmin 1 commits @ripienaar 1 commits @stou 1 commits @razonyang 1 commits @HontoNoRoger 1 commits @pocc 1 commits @EnigmaCurry 1 commits @taiidani 1 commits @exKAZUu 1 commits @Oddly 1 commits @sandrogauci 1 commits @shelane 1 commits @mbbx6spp 1 commits @swenzel 1 commits @tedyoung 1 commits @Thiht 1 commits @editicalu 1 commits @fossabot 1 commits @kamar535 1 commits @mtbt03 1 commits @ngocbichdao 1 commits @nonumeros 1 commits @pgorod 1 commits @proelbtn 1 commits And a special thanks to vjeantet for his work on docdock, a fork of hugo-theme-learn. v2.0.0 of this theme is inspired by his work.\nPackages and libraries mermaid - generation of diagram and flowchart from text in a similar manner as markdown font awesome - the iconic font and CSS framework jQuery - The Write Less, Do More, JavaScript Library lunr - Lunr enables you to provide a great search experience without the need for external, server-side, search services\u0026hellip; horsey - Progressive and customizable autocomplete component clipboard.js - copy text to clipboard highlight.js - Javascript syntax highlighter modernizr - A JavaScript toolkit that allows web developers to use new CSS3 and HTML5 features while maintaining a fine level of control over browsers that don\u0026rsquo;t support KaTeX - rendering LaTeX math chunks Tooling Netlify - Continuous deployement and hosting of this documentation Hugo "
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/glossar/dichotom/",
	"title": "dichotom",
	"tags": [],
	"description": "",
	"content": " Eine Variable ist dichotom, wenn sie nur zwei Merkmalsausprägungen hat (Bsp. Wahlteilnahme: ja/nein).\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/glossar/dummykodiertevariable/",
	"title": "dummykodierte Variable",
	"tags": [],
	"description": "",
	"content": " Möglichkeit der Inklusion von ordinalen und nominalen Variablen als unabhängige Variablen im linearen Regressionsmodell; bei Interpretation muss Referenzkategorie festgelegt werden;\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/glossar/einseitigertest/",
	"title": "Einseitiger Test",
	"tags": [],
	"description": "",
	"content": " auch gerichteter T. genannt; es wird eine Richtung über die Abweichung eines angenommenen Wertes festgelegt; Ablehnungsbereich einer Hypothese ist auf eine Seite festgelegt\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/glossar/erwartungswert/",
	"title": "Erwartungswert",
	"tags": [],
	"description": "",
	"content": " Grundbegriff der Wahrscheinlichkeitsrechnung; griechischer Buchstabe \\(\\mu\\); Erwartungswert ist der Wert, die die untersuchte Variable im Mittelwert in der Population annimmt (in Abhängigkeit der Verteilungsform der Variablen)\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/tags/f-test/",
	"title": "f-test",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/glossar/fverteilung/",
	"title": "F-Verteilung",
	"tags": [],
	"description": "",
	"content": " Fisher-Verteilung; Verteilungsform der Quotienten zweier \\(\\chi^2\\)-verteilter Zufallsvariablen dividiert durch die Anzahl der Freiheitsgerade; wichtige Verteilung für die Varianzanalyse (inferenzstatistische Verfahren)\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/tags/gepaart/",
	"title": "gepaart",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/tags/gleiche-varianzen/",
	"title": "gleiche varianzen",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/glossar/grundgesamtheit/",
	"title": "Grundgesamtheit",
	"tags": [],
	"description": "",
	"content": " auch Zielpopulation oder angestrebte Grundgesamtheit; Menge aller Einheiten, über die wir eine Aussage treffen wollen.\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/tags/gruppenvergleich/",
	"title": "gruppenvergleich",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/glossar/homoskedastizitaet/",
	"title": "Homoskedastizität",
	"tags": [],
	"description": "",
	"content": " Homoskedastizität ist eine Annahme der linearen Regression. Dies setzt somit voraus, dass die Varianzen der Residuen an allen Prädiktorwerte für die abhängige Variable gleich sein müssen. Die Streuung dieser darf mit zunehmenden oder abnehmenden Werten nicht größer/kleiner werden. Sollte keine Homoskedastizität vorhanden sind, sind die Schätzer nicht zuverlässig. Das heißt, dass die Standardabweichung, das Konfidenzintervall und dadurch die Teststatistiken auf Signifikanz nicht interpretierbar sind.\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/tags/hypothesentest/",
	"title": "hypothesentest",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/glossar/inferenzstatistik/",
	"title": "Inferenzstatistik",
	"tags": [],
	"description": "",
	"content": " Mithilfe der Inferenzstatistik können Rückschlüsse aus einer zufallsbasierten Stichprobe auf die Grundgesamtheit übertragen werden. Hierbei werden i.d.R. Hypothesen- bzw. Signifikanztests durchgeführt.\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/tags/irrtumswahrscheinlichkeit/",
	"title": "irrtumswahrscheinlichkeit",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/glossar/irrtumswahrscheinlichkeit/",
	"title": "Irrtumswahrscheinlichkeit",
	"tags": [],
	"description": "",
	"content": " Auch Fehler 1. Art oder \\(\\alpha\\)-Fehler; gibt an, in wie vielen Fällen die Entscheidung auf die Alternativhypothese fällt, obwohl die Nullhypothese richtig ist.\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/tags/kategorial/",
	"title": "kategorial",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/glossar/kategorialesskalenniveau/",
	"title": "kategoriales Skalenniveau",
	"tags": [],
	"description": "",
	"content": " umfasst nominale wie ordinale Skalenniveaus.\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/glossar/kausalitaet/",
	"title": "Kausalität",
	"tags": [],
	"description": "",
	"content": " Das Ziel insbesondere hypothesen- und theorie-prüfender Untersuchungen ist die Bestimmung von Ursache-Wirkungs-Beziehungen. Diese Ursache-Wirkungs-Beziehung wird auch als Kausalität bezeichnet.\nEine Kausalaussage ist „eine Aussage, in der behauptet wird, dass bestimmte Phänomene Ursachen für andere Phänomene sind“ (Opp 2010). Die Ursache (X) muss zeitlich vor der Wirkung (Y) liegen.\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/tags/kennzahlen/",
	"title": "kennzahlen",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/tags/konfidenzintervall/",
	"title": "konfidenzintervall",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/glossar/konfidenzintervall/",
	"title": "Konfidenzintervall",
	"tags": [],
	"description": "",
	"content": " bezeichnet in den Sozialwissenschaften ein Intervall, in dem ein erwarteter Wert μ sehr wahrscheinlich liegt; in Sozialwissenschaften gängige Intervalle: 95%-Konfidenzintervall (Wert liegt zu 95% in dem Intervall) und 99%-Konfidenzintervall (Wert liegt zu 99% in dem Intervall)\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/glossar/korrelation/",
	"title": "Korrelation",
	"tags": [],
	"description": "",
	"content": " beschreibt in allgemeiner Weise den Zusammenhang zwischen zwei Variablen; mithilfe des Korrelationskoeffizienten wird eine Kennzahl berechnet, die einfach interpretierbar ist.\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/glossar/kritischerwert/",
	"title": "Kritischer Wert",
	"tags": [],
	"description": "",
	"content": " Teil des Signifikanztests; Prüfgröße muss größer als der kritische Wert sein, damit der Wert als signifikant wird; wird aus Verteilungstabellen in Abhängigkeit der Irrtumswahrscheinlichkeit und der Freiheitsgrade (degree of freedoms, \\(\\nu\\)) abgelesen.\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/",
	"title": "Lernmodul 4",
	"tags": [],
	"description": "",
	"content": "Lernmodul 4: Signifikanztests, Mittelwertvergleiche \u0026amp; lineare Regression In diesem Lernmodul werden Ihnen weitere quantitative Analysetechniken dargestellt. Das Web-Based-Training führt an dieser Stelle in die Hypothesentests bzw. Signifikanztests ein. Hier werden zum einen Konfidenzintervalle berechnet und die Signifikanz der Korrelation aus dem vorherigen Lernmodul. Anschließend werden Mittelwertvergleiche vorgestellt, bevor abschließend in die lineare Regression eingeführt wird.\nHier sehen Sie den Aufbau des Lernmoduls:\nÜber diesen Button kannst du Fehler auf Seiten direkt an uns melden. Dazu musst du dich einmalig mit der JLU-Kennung anmelden. Du findest den Button auf jeder Seite.\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/tags/lernvideo/",
	"title": "lernvideo",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/tags/linear/",
	"title": "linear",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/glossar/lineareregression/",
	"title": "lineare Regression",
	"tags": [],
	"description": "",
	"content": " multivariates Analyseverfahren; eine metrische abhängige Variable wird durch mehrere unabhängige Variablen erklärt; Die lineare Regression ist ein strukturprüfendes Modell, mit dem eine Ursachenanalyse und Wirkungsprognose möglich ist. Es lassen sich theoretisch entwickelte Hypothesen über die Beeinflussungsstruktur bestimmter Variablen auf andere Variablen prüfen.\nUrsachenanalyse: Wie stark ist der Einfluss der unabhängigen Variablen auf abhängige Variable?\nWirkungsprognose: Wie verändert sich abhängige Variable bei der Veränderung der unabhängigen Variable?\nAllgemeines Modell der linearen Regression: \\(y_i=\\beta_0 + \\beta_1 ∗ x_{1,i} + ... + \\beta_{k} ∗ x_{k,i} + \\epsilon_i\\), wobei \\(y_i\\) = berechneter Wert für abhängige Variable bei i-ten Fall, \\(\\beta_0\\) = Konstante (Intercept), \\(β_k\\) = Regressionskoeffizienten für k-te Variable, \\(x_{k,i}\\) = Wert der k-ten unabhängigen Variable für i-ten Fall, und \\(\\epsilon_i\\) = Residuum des Falls i.\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/glossar/merkmalsauspraegung/",
	"title": "Merkmalsausprägung",
	"tags": [],
	"description": "",
	"content": " Merkmale können verschiedene Werte annehmen, die Merkmalsausprägungen genannt werden (Bsp. Augenfarbe; Merkmalsausprägungen= grün, blau, braun).\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/glossar/metrischesskalenniveau/",
	"title": "metrisches Skalenniveau",
	"tags": [],
	"description": "",
	"content": " umfasst intervall- und ratio-skalierte Merkmale.\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/glossar/mittelwert/",
	"title": "Mittelwert",
	"tags": [],
	"description": "",
	"content": " auch arithmetisches Mittel; Maß der zentralen Tendenz; stellt den Durchschnittswert der Verteilung dar und daher muss das Merkmal mind. intervallskaliert sein. Es werden alle erhobenen Werte aufsummiert und dann durch die Anzahl der vorhandenen Werte geteilt. Der Mittelwert wird durch Ausreißer und Extremwerte stark beeinflusst, da alle Werte gleichermaßen und ungewichtet in die Berechnung einbezogen werden.\nFormel zur Berechnung: \\(\\bar{x}=\\frac {1} {n} * \\sum_{i=1} ^n x_i\\)\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/tags/mittelwertvergleich/",
	"title": "mittelwertvergleich",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/tags/multivariat/",
	"title": "multivariat",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/glossar/multivariat/",
	"title": "Multivariat",
	"tags": [],
	"description": "",
	"content": " Betrachtet man die gemeinsame Verteilung von mehr als zwei Merkmalen bzw. Variablen, spricht man von einer multivariaten Analyse.\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/glossar/nominalskala/",
	"title": "Nominal-Skala",
	"tags": [],
	"description": "",
	"content": " Niedrigste Skala der vier Skalenniveaus. Zu untersuchende Objekte können lediglich hinsichtlich ihres Vorhandenseins/Nichtvorhandenseins bzw. der Gleichheit/Ungleichheit untersucht werden.\nMathematische Operatoren: \\(=\\) \\(/\\) \\(≠\\)\nMessbare Eigenschaften: Häufigkeit\nBeispiel: Wohnort, Universität, Augenfarbe.\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/glossar/normalverteilung/",
	"title": "Normalverteilung",
	"tags": [],
	"description": "",
	"content": " auch Gauß’sche Normalverteilung; Mittelwert und Streuung beschreiben die Lage; symmetrisch, nähert sich asymptotisch der \\(Y\\)-Achse an; größte Häufigkeiten liegen in der Mitte, geringere Häufigkeiten rechts bzw. links von der Mitte; Median, Modus und Mittelwert nehmen in einer Normalverteilung denselben Wert an; wichtige Verteilungsform für eine Reihe quantitativer Analyseverfahren und für die Inferenzstatistik.\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/glossar/nullhypothese/",
	"title": "Nullhypothese",
	"tags": [],
	"description": "",
	"content": " Getestete Hypothese in einem Hypothesentest; die Nullhypothese umfasst i.d.R. die Negierung der aufgestellten Alternativhypothese (der Forschungshypothese)\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/glossar/ordinalskala/",
	"title": "Ordinal-Skala",
	"tags": [],
	"description": "",
	"content": " Zweitniedrigste Skala der vier Skalenniveaus. Die Ordinal-Skala besitzt die Eigenschaften der Nominalskala (Verlinkung) und zusätzlich können die Ausprägungen in eine bestimmte Rangfolge bringen (größer/kleiner, schlechter/besser). Mathematische Operatoren: \\(= / ≠ ; \u0026lt; / \u0026gt;\\) Messbare Eigenschaften: Häufigkeit, Rangordnung Beispiel: Schulabschluss\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/glossar/ols/",
	"title": "Ordinary-Least-Squares-Verfahren",
	"tags": [],
	"description": "",
	"content": " auch OLS-Verfahren; auch kleinste-Quadrate-Verfahren genannt; Berechnungsmethode der linearen Regression; Regressionsgerade wird über Minimierung der Summe der Abweichungen der Residuen geschätzt.\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/glossar/pearsonsr/",
	"title": "Pearsons r",
	"tags": [],
	"description": "",
	"content": " Korrelationskoeffizient für metrische Variablen; Voraussetzung zur Berechnung ist Linearität und Monotonie des Zusammenhangs.\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/glossar/polytom/",
	"title": "polytom",
	"tags": [],
	"description": "",
	"content": " Als polytom werden Variablen beschrieben, die mehr als zwei Ausprägungen besitzen. (z.B. die Augenzahl beim Wurf eines Würfels).\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/tags/praxis/",
	"title": "praxis",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/glossar/pseudometrisch/",
	"title": "pseudometrisch",
	"tags": [],
	"description": "",
	"content": " ordinales Skalenniveau, wobei gleichwertige Abstände zwischen Ausprägungen angenommen werden; betrifft oftmals Skalen und wird angewendet bei Skalen ab 11 Ausprägungen (z.B. 0-10er Skala der Zustimmung).\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/glossar/quantitativesozialforschung/",
	"title": "quantitative Sozialforschung",
	"tags": [],
	"description": "",
	"content": " Die quantitative Sozialforschung zeichnet sich durch „ein streng zielorientiertes Vorgehen aus, das die Objektivität (…) der Resultate durch möglichst weitgehende Standardisierung aller Teilschritte anstrebt und das zur Qualitätssicherung die intersubjektive Nachprüfbarkeit des gesamten Prozesses als zentrale Norm postuliert“ (Kromrey 2009: 25).\nIst das Ziel eine Generalisierung von Aussagen werden oftmals Verfahren quantitativer Sozialforschung angewendet.\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/glossar/referenzkategorie/",
	"title": "Referenzkategorie",
	"tags": [],
	"description": "",
	"content": " Festlegen der Referenzkategorie bei Hinzufügen ordinaler/nominaler Variablen in der linearen Regression; Regressionskoeffizienten geben die Veränderung auf der abhängigen Variable im Vergleich von angegebener Kategorie zur Referenzkategorie aus\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/tags/regression/",
	"title": "regression",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/tags/regressionskoeffizienten/",
	"title": "regressionskoeffizienten",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/glossar/regressionskoeffizienten/",
	"title": "Regressionskoeffizienten",
	"tags": [],
	"description": "",
	"content": " Einflussfaktoren einzelner unabhängiger Variable auf abhängige Variable (unter Konstanthalten der anderen unabhängigen Variablen); es wird unterschieden zwischen standardisierten und nicht-standardisierten Regressionskoeffizienten\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/glossar/residuum/",
	"title": "Residuum",
	"tags": [],
	"description": "",
	"content": " Umfasst die Differenz von geschätzten \\(y\\)-Wert und beobachteten \\(y\\)-Wert \\((\\hat{y})\\); Formel: \\(e_i= y_i - \\hat{y}_i\\) ; mathematische Rechenwert in der Regression\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/glossar/scatterplot/",
	"title": "Scatterplot",
	"tags": [],
	"description": "",
	"content": " auch Streudiagramm genannt; S. zwischen zwei Variablen; wird insbesondere bei Zusammenhang zwischen metrischen Variablen genutzt; wird zur Kontrolle der Linearität und Monotonie bei Pearson-Korrelation genutzt.\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/tags/schaetzung/",
	"title": "schaetzung",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/tags/signifikanz/",
	"title": "signifikanz",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/glossar/signifikanzniveau/",
	"title": "Signifikanzniveau",
	"tags": [],
	"description": "",
	"content": " gibt an, wie hoch das Risiko ist einen Fehler 1. Art (\\(\\alpha\\)-Fehler) zu begehen; auch Irrtumswahrscheinlichkeit\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/glossar/signifikanztest/",
	"title": "Signifikanztest",
	"tags": [],
	"description": "",
	"content": " auch Hypothesentest genannt; Test der Inferenzstatistik; es wird getestet, inwieweit ein Zusammenhang in der Stichprobe auch in der Grundgesamtheit wahrscheinlich ist (-\u0026gt;Inferenz); getestet wird die Nullhypothese (also das Gegenteil der Annahme muss widerlegt werden); Alternativhypothese beinhaltet theoretische Vermutung der Forscher*in; Entscheidung wird gegen Nullhypothese unter Irrtumswahrscheinlichkeit gefällt; Testgröße in Abhängigkeit der Verteilung (z.B. t-Test, F-Test oder z-Test); es wird zwischen einem einseitigen (Richtungstest) und zweiseitigen Signifikanztest (ungleich/gleich-Test) unterschieden.\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/glossar/skalenniveau/",
	"title": "Skalenniveau",
	"tags": [],
	"description": "",
	"content": " Skalenniveaus werden zur Bestimmung der Relationen zwischen Variablen definiert. Man unterscheidet zwischen Nominal-, Ordinal-, Intervall- und Ratio-Skala.\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/glossar/standardabweichung/",
	"title": "Standardabweichung",
	"tags": [],
	"description": "",
	"content": " Streuungsmaß; Angabe mit \\(\\sigma\\); wird berechnet als Wurzel aus der Varianz (\\(\\sigma=\\sqrt{\\sigma^2}\\); gibt die durchschnittliche Abweichung vom Mittelwert der Verteilung an (in der Maßeinheit der Variablen). Für die Berechnung einer Standardabweichung aus einer Stichprobe wird die Bessel-Korrektur angewendet (\\(\\frac{1}{n-1}\\)): \\(s = \\sqrt{\\frac{1}{n-1}*s^2}\\)\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/glossar/stichprobe/",
	"title": "Stichprobe",
	"tags": [],
	"description": "",
	"content": " ist ein Teil der Grundgesamtheit; da oftmals keine Vollerhebung stattfinden kann, werden Stichproben aus der Grundgesamtheit gezogen; um inferenzstatistische Verfahren anwenden zu können, müssen Stichproben zufallsbasiert gezogen werden.\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/glossar/stichprobenumfang/",
	"title": "Stichprobenumfang",
	"tags": [],
	"description": "",
	"content": " Anzahl der Beobachtungseinheiten in einer Stichprobe.\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/glossar/stoervariable/",
	"title": "Störvariable",
	"tags": [],
	"description": "",
	"content": " Theoretische Größe des theoretischen Regressionsmodell; in Sozialwissenschaften können nie alle notwendigen Variablen erfasst werden, daher kann nie ein perfekter Zusammenhang dargestellt werden; Störvariablen bilden den Term ϵ, der als Fehlerterm der Lineargleichung der Regression hinzugefügt wird.\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/tags/t-test/",
	"title": "t-test",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/glossar/ttestregression/",
	"title": "t-Test (Regression)",
	"tags": [],
	"description": "",
	"content": " testet die Signifikanz der einzelnen Regressionskoeffizienten gegen die Nullhypothese, dass diese je 0 sind \\((H_0: \\beta_i=0)\\); beruht auf t-Verteilung;\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/tags/t-verteilung/",
	"title": "t-verteilung",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/glossar/tverteilung/",
	"title": "t-Verteilung",
	"tags": [],
	"description": "",
	"content": " glockenförmige, flache Verteilung; wird für kleine Stichproben genutzt (n\u0026lt;30); kann auch bei Unwissenheit über die Standardabweichung der Grundgesamtheit genutzt werden; geht bei großen Stichproben in Normalverteilung über;\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/tags/testsituation/",
	"title": "testsituation",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/glossar/unabhaengigestichprobe/",
	"title": "unabhängige Stichprobe",
	"tags": [],
	"description": "",
	"content": " liegt vor, wenn keine Zuordnung zwischen den Fällen möglich ist (z.B. Aufteilung eines Datensatzes nach Geschlecht, eine Stichprobe weiblich, die zweite Stichprobe männlich)\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/glossar/unabhaengigevariable/",
	"title": "unabhängige Variable",
	"tags": [],
	"description": "",
	"content": " Die unabhängige Variable (Kurzform: uV) ist die Variable, die die Wirkung auslöst (Ursache). Sie wird zumeist mit X bezeichnet. Sie kann auch als Prädiktor-Variable oder erklärende Variable bezeichnet.\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/tags/ungepaart/",
	"title": "ungepaart",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/tags/ungleiche-varianzen/",
	"title": "ungleiche varianzen",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/glossar/variable/",
	"title": "Variable",
	"tags": [],
	"description": "",
	"content": " Variablen sind die gemessenen Merkmale bzw. Eigenschaften von Beobachtungseinheiten. Jede Variable bildet ein Merkmal und dessen Merkmalsausprägungen ab.\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/glossar/varianz/",
	"title": "Varianz",
	"tags": [],
	"description": "",
	"content": " Streuungsmaß; Zeichen der Varianz \\(\\sigma^2\\); Summe der quadrierten Abweichungen der Werte vom Mittelwert; Einheit der Varianz ist nicht interpretierbar; Berechnung: \\(s^2=\\sigma^2=\\frac{1}{n} \\ast \\sum_{i=1}^n(x_i - \\bar{x})^2\\) Für die Berechnung einer Standardabweichung aus einer Stichprobe wird die Bessel-Korrektur angewendet (\\(\\frac{1}{n-1}\\)):\n\\[s^2 = \\frac{1}{n-1}* \\sum_{i=1}^n (x_i - \\bar{x})^2\\]\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/tags/vertrauenswahrscheinlichkeit/",
	"title": "vertrauenswahrscheinlichkeit",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/glossar/vertrauenswahrscheinlichkeit/",
	"title": "Vertrauenswahrscheinlichkeit",
	"tags": [],
	"description": "",
	"content": " gibt die Wahrscheinlichkeit an, dass ein Intervall den wahren Wert enthält (\\(p=1- \\alpha\\))\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/glossar/willkuerlicheauswahl/",
	"title": "Willkürliche Auswahl",
	"tags": [],
	"description": "",
	"content": " Verfahren der nicht-wahrscheinlichkeitsbasierten Auswahl einer Stichprobe; Auswahl auf das Geratewohl (willkürliche Entscheidung); Grundgesamtheit ist nicht identifizierbar; Beispiel Schneeball-Auswahl.\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/tags/z-verteilung/",
	"title": "z-verteilung",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/glossar/zverteilung/",
	"title": "z-Verteilung",
	"tags": [],
	"description": "",
	"content": " auch Standardnormalverteilung genannt; Transformierte Form der Normalverteilung; für eine Standardnormalverteilung gilt: \\(\\bar{x}=0, s^2=s =1\\) ; z-Transformation: \\(z_i = \\frac{x_i - \\bar{x}}{s}\\) ; z-Wert geben relative Position des Rohwertes zum Mittelwert in Standardabweichungen an.\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/glossar/zufallsstichprobe/",
	"title": "Zufallsstichprobe",
	"tags": [],
	"description": "",
	"content": " Verfahren der Stichprobenziehung, dass auf der Stochastik beruht; mithilfe von Zufallsstichproben können Verfahren der Inferenzstatistik angewendet werden.\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM4/glossar/zweiseitigertest/",
	"title": "zweiseitiger Signifikanztest",
	"tags": [],
	"description": "",
	"content": " uch Hypothesentest, auch ungerichteter S.; es wird keine Richtung festgelegt; der Ablehnungsbereich einer Hypothese ist je hälftig auf beiden Seiten anzulegen \\((\\frac {\\alpha} {2} \\thinspace und \\thinspace (1-\\frac {\\alpha} {2}))\\)\n"
}]
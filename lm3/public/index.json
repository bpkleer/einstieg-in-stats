[
{
	"uri": "https://lehre.bpkleer.de/stats101/LM3/chapter1/",
	"title": "Grundlagen",
	"tags": [],
	"description": "",
	"content": "Kapitel 1 Grundlagen In diesem Kapitel werden die Grundlagen für die Berechnung der Zusammenhangsmaße vorgestellt: Zuerst wird dazu die Berechnung von Chi-Quadrat vorgestellt. Anschließend wird die Logik von PRE-Maßen vorgestellt und abschließend die Korrelation dargestellt.\nIn den nachfolgenden Kapiteln wird dann die Berechnung der einzelnen Maße vorgestellt. Anhand der folgenden Abbildung können Sie die verschiedenen Zusammenhangsmaße einordnen.\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM3/chapter1/page-1-a/",
	"title": "Beobachtete und erwartete Häufigkeiten",
	"tags": ["häufigkeiten", "beobachtete häufigkeiten", "erwartete häufigkeiten", "kreuztabellen", "lernvideo"],
	"description": "",
	"content": " Mit Kreuztabellen wird versucht, die Unabhängigkeit von Variablen zu prüfen. Bei Unabhängigkeit muss die prozentuale Verteilung der abhängigen Variablen in jeder Ausprägung der unabhängigen Variable gelten. Im Beispiel hieße es, dass sowohl auf dem Land als auch in der Stadt die Aufteilung zwischen Wahl der CDU und Nicht-Wahl der CDU gleich der Gesamtaufteilung ist.\nAbweichungen von diesen Verteilungen lassen darauf schließen, dass die Variablen nicht unabhängig voneinander sind. Wenn die Variablen nicht unabhängig voneinander sind, wird dies als Zusammenhang gewertet.\nAus diesem Grund werden immer auch die relativen beobachteten Häufigkeiten der Kombination angegeben. In der Konvention werden die Prozente in Abhängigkeit der Spalte dargestellt. Für jede Spalte wird die prozentuale Häufigkeit berechnet.\nKontingenztabelle Von den in der Stadt lebenden Personen haben also 20 % CDU gewählt und von Personen, die auf dem Land leben 70 %. Anhand der beobachteten Häufigkeiten lassen sich deutlich unterschiedliche Präferenzen für Personen erkennen, die auf dem Land leben und Personen, die in der Stadt leben.\nUm diesen Befund nun statistisch zu prüfen, muss zuerst eine Indifferenztabelle berechnet werden. Diese Indifferenztabelle nimmt die erwarteten Werte an, wenn kein Zusammenhang zwischen den Variablen besteht. Das heißt, in beiden Spalten der unabhängigen Variable müssten die gleichen Häufigkeiten wie in der Gesamtverteilung stehen.\nWenn kein Zusammenhang bestehen würde, ergibt sich somit die gleiche prozentuale Verteilung. Das Rechenbeispiel finden Sie auch in einem Lernvideo auf der nächsten Seite. Die erwartete Häufigkeit bei Unabhängigkeit, die sogenannte Indifferenztabelle, ergibt sich aus der beobachteten Randverteilung: Wir wissen aus den Daten, dass insgesamt 45 % CDU gewählt haben und 55 % nicht die CDU gewählt haben. Dies ist in der letzten Spalte oben zu sehen. Alternativ errechnet sich die erwartete Häufigkeit wie folgt:\n\\(f_{e_{ij}}=\\frac {f_{e_{i.}} \\ast f_{e_{.j}}}{n}\\), wobei gilt das \\(f_{e_{i.}}=Zeilensumme\\) und \\(f_{e_{.j}}=Spaltensumme\\) ist.\nIm Beispiel ergibt sich die Ausprägung Wahl der CDU folgende erwartete Häufigkeit:\n\\[\\begin{align*}f_{e_{ij}}\u0026amp;=\\frac {f_{e_{i.}} \\ast f_{e_{.j}}}{n} \\\\ \u0026amp;=\\frac{450 \\ast 500}{1000} \\\\ \u0026amp;= \\frac{22500}{1000} \\\\ \u0026amp;= 225 \\end{align*}\\]\nLiegt Unabhängigkeit vor, muss dieses Verteilungsmuster auch in den einzelnen Spalten der abhängigen Variable vorliegen. Daraus ergibt sich dann die Indifferenztabelle.\nDie Werte der folgenden Tabelle würden wir bei Unabhängigkeit erwarten, deshalb bezeichnet man diese auch als erwartete Werte im Gegensatz zu den beobachteten Werten zuvor. Diese Tabelle stellt eine Indifferenztabelle dar.\nIndifferenztabelle Aus der Differenz zwischen der Kontingenztabelle mit den beobachteten Werten und der Indifferenztabelle mit den erwarteten Werten wird der Zusammenhang dann überprüft.\nSiehe auch: Gehring \u0026amp; Weins (2009, Kapitel 7.1).\nIm folgenden Lernvideo wird Ihnen an zwei Beispielen die Berechnung der erwarteten Werte dargestellt.\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM3/chapter1/page-1-b/",
	"title": "Chi-Quadrat-Unabhängigkeitstest",
	"tags": ["chi-quadrat", "unabhängigkeitstest"],
	"description": "",
	"content": " Die Indifferenztabelle ist die Grundlage der Berechnung von \\(\\chi^2\\) (Chi-Quadrat), welches ein Maß zur Bewertung der Unabhängigkeit zwischen zwei Variablen ist. Das untenstehende Rechenbeispiel wird auf der nächsten Seite in einem Lernvideo nochmals erklärt.\nJe stärker sich erwartete (Indifferenztabelle) und beobachtete Häufigkeiten (Kontingenztabelle) voneinander unterscheiden, desto stärker ist der Zusammenhang zwischen beiden Variablen.\nDie beobachtete Häufigkeit wird hierbei mit \\(f_b\\) bezeichnet und die erwartete Häufigkeit mit \\(f_e\\). Vereinfacht ausgedrückt werden die tatsächlich beobachteten absoluten Häufigkeiten ins Verhältnis zu den erwarteten absoluten Häufigkeiten gesetzt.\nHier zum Verständnis die Tabelle mit den erwarteten und beobachteten Häufigkeiten:\nHäufigkeitstabelle mit erwarteten und beobachteten Häufigkeiten Als Residuum wird dabei die Differenz zwischen \\(f_b\\) und \\(f_e\\) bezeichnet, die für jedes Feld berechnet werden muss (\\(res=f_b – f_e\\)).\nDie Formel zur Berechnung von \\(\\chi^2\\) (Chi-Quadrat) lautet daher:\n\\[\\chi^2 = \\sum_{i=1}^k \\sum_{j=1}^m \\frac{(f_{b_{ij}} – f_{e_{ij}})^2}{f_{e_{ij}}}\\]\n\\(i\\) ist dabei der Laufindex für die Zeilen und \\(j\\) der Laufindex für die Spalten.\nIm Beispiel ergibt sich daher folgende Rechnung:\n\\[ \\begin{align*} \\chi^2 \u0026amp;= \\sum_{i=1}^k \\sum_{j=1}^m \\frac{(f_{b_{ij}} – f_{e_{ij}})^2}{f_{e_{ij}}} \\\\ \u0026amp;=\\frac {(100-225)^2}{225} + \\frac{(400-275)^2}{275} + \\frac {(350-225)^2}{225} + \\frac {(150-275)^2}{275} \\\\ \u0026amp;\\approx 69,44 + 56,82 + 69,44 + 56,82 \\\\ \u0026amp;\\approx 252,53 \\end{align*}\\]\n\\(\\chi^2\\) (Chi-Quadrat) weist einen Wertebereich von \\(0\\) bis \\(+\\infty\\) (\\(\\chi^2 \\in [0;+\\infty]\\)) auf. Ein \\(\\chi^2\\) (Chi-Quadrat)-Wert von \\(0\\) würde bedeuten, dass zwischen den beiden Variablen kein Zusammenhang besteht. Diese Maßzahl ist abhängig von der Fallzahl und der Tabellengröße. Außerdem können \\(\\chi^2\\) (Chi-Quadrat)-Werte unterschiedlicher Tabellengrößen nicht miteinander verglichen werden.\nIn unserem Beispiel sehen wir, dass der Wert deutlich von \\(0\\) entfernt ist und somit keine Unabhängigkeit vorliegt. Die drei nachfolgenden Zusammenhangsmaße beruhen auf \\(\\chi^2\\) und werden im weiteren Verlauf vorgestellt:\nPhi-Koeffizient (2x2 Kreuztabelle)\nCramers V (jede mögliche Kreuztabelle)\nKontingenzkoeffizient C (jede mögliche Kreuztabelle)\nSiehe auch: Gehring \u0026amp; Weins (2009, Kapitel 7.3.1); Bortz \u0026amp; Schuster (2010, Kapitel 9.1).\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM3/chapter1/page-1-c/",
	"title": "PRE-Maße",
	"tags": ["pre-maße", "lambd", "gamm", "eta-quadrat"],
	"description": "",
	"content": " Neben der Berechnung von Zusammenhangsmaßen, die auf \\(\\chi^2\\) beruhen, können auch noch Zusammenhangsmaße berechnet werden, die auf der proportionalen Fehlerreduktion beruhen (PRE-Maße). Ziel dieser Maße ist es auszudrücken, wie gut durch die Kenntnis einer Variablen die Ausprägungen einer weiteren Variablen vorhergesagt werden können. Das errechnete Maß drückt dann diese Verbesserung aus. Hierbei unterscheidet man zwei Fälle: Einmal den Fall, dass man keine Kenntnis über den Zusammenhang der zwei Variablen hat (\\(E_1\\)) und zum anderen, dass man eben über entsprechende Kenntnisse verfügt (\\(E_2\\)). Beides beschreibt das Ausmaß des Vorhersagefehlers (einmal unter Unkenntnis des Zusammenhangs und einmal unter Kenntnis des Zusammenhangs).\nGenerell wird ein PRE-Maß daher wie folgt berechnet:\n\\[PRE = \\frac{E_1 - E_2}{E_1}\\]\nDie folgenden PRE-Maße werden im weiteren Verlauf vorgestellt:\nGoodman \u0026amp; Kruskals \\(\\lambda\\),\nGoodman \u0026amp; Kruskals \\(\\gamma\\),\nGoodman \u0026amp; Kruskals \\(\\eta^2\\).\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM3/chapter1/page-1-d/",
	"title": "Korrelation",
	"tags": ["korrelation"],
	"description": "",
	"content": " Korrelationen werden berechnet, um den Zusammenhang, die Richtung und die Stärke des Zusammenhangs zweier Variablen festzustellen. Eine Korrelation misst allerdings keine Kausalität.\nIn einem Beispiel erklärt, bedeutet dies, wenn Arbeitszeit und Einkommen positiv miteinander korrelieren, das daraus nicht geschlussfolgert werden kann, dass das Einkommen aufgrund höherer Arbeitszeit steigt bzw. dass die Arbeitszeit aufgrund höherem Einkommen steigt. Die Kausalität kann mit einer Korrelationsanalyse nicht belegt werden.\nEntsprechend der Skalenniveaus einer Variable gibt es verschiedene Korrelationskoeffizienten:\nPearson’s r (wird für den Zusammenhang zweier metrischer Variablen genutzt)\nSpearman’s \\(\\rho\\) (für mindestens ordinale Variablen)\nGemeinsam haben diese Korrelationskoeffizienten, dass sie alle Werte zwischen \\([-1;+1]\\) einnehmen und gleich interpretiert werden. Positive Werte bedeuten, dass mit dem Steigen der Variable \\(A\\) auch Variable \\(B\\) steigt und negative Werte bedeuten, dass beim Steigen der Variable \\(A\\) die Variable \\(B\\) sinkt.\nBeispiel positiver und negativer Zusammenhang Im Beispiel der roten Linie sinkt die Variable \\(B\\), wenn Variable \\(A\\) steigt und im Beispiel der grünen Linie steigt die Variable \\(B\\), wenn die Variable \\(A\\) steigt. Für die grüne Linie würde sich ein positiver Korrelationskoeffizient ergeben, für die rote Linie ein negativer.\nIm Beispiel der roten Linie sinkt die Variable B um \\(1,5\\) je Steigen um ein \\(x\\) und im Beispiel grünen Linien steigt die Variable B um \\(1,5\\) je Steigen um ein \\(x\\).\nBeispiel positiver und negativer Zusammenhang Je näher der Korrelationskoeffizient an das Ende des Intervalls kommt (also an \\(+1\\) oder \\(−1\\)), desto stärker ist der Zusammenhang zwischen den zwei Variablen.\nIst der Korrelationskoeffizient nahe \\(0\\), liegt nur ein sehr schwacher Zusammenhang vor. Ein Korrelationskoeffizient von \\(0\\) bedeutet, dass es keinen Zusammenhang zwischen den zwei Variablen gibt.\nAls Grenzwerte für die Korrelationskoeffizienten werden folgende festgehalten:\nWert des Korrelationskoeffizienten Interpretation \\(\\leq |0.05|\\) zu vernachlässigen \\(|0.05| \\leq |0.2|\\) geringer Zusammenhang \\(|0.2| \\leq |0.5|\\) mittlerer Zusammenhang \\(|0.5| \\leq |0.7|\\) starker Zusammenhang \\(\\geq |0.7|\\) sehr starker Zusammenhang Quelle: Kühnel/Krebs 2010, S. 404/405 In den folgenden Teilen des Web-Based-Trainings werden zwei Korrelationsmaße vorgestellt:\nPearsons r\nSpearmans \\(\\rho\\)\nSiehe auch: Gehring \u0026amp; Weins (2009, Kapitel 7.6: Pearson’s r); Behnke \u0026amp; Behnke (2006, Kapitel 14.3.2: Spearman’s rho); Behnke \u0026amp; Behnke (2006, Kapitel 14.4: Kovarianz und Pearson-Korrelation); Bortz \u0026amp; Schuster (2010, Kapitel 10.3.2: Pearson’s r); Bortz \u0026amp; Schuster (2010, Kapitel 10.3.6: Spearman’s rho); Cleff (2011, Kapitel 4.3.2 Pearson’s r); Cleff (2011, Kapitel 4.4.1 Spearman’s rho).\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM3/chapter2/",
	"title": "Zusammenhangsmaße",
	"tags": ["zusammenhangsmaße"],
	"description": "",
	"content": "Kapitel 2 Zusammenhangsmaße In diesem Kapitel werden die verschiedenen Zusammenhangsmaße nach Skalenniveau vorgestellt.\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM3/chapter2/subchapter1/",
	"title": "Nominales Skalenniveau",
	"tags": ["nominal", "zusammenhangsmaß"],
	"description": "",
	"content": "Kapitel 2.1 Nominales Skalenniveau In diesem Kapitel werden Zusammenhangsmaße für Variablen mit nominalen Skalenniveau vorgestellt.\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM3/chapter2/subchapter1/page-2-1-a/",
	"title": "Chi-Quadrat, Phi-Koeffizient &amp; Cramers V",
	"tags": ["chi-quadrat", "phi-Koeffizient", "Cramers V", "lernvideo"],
	"description": "",
	"content": " \\(\\chi^2\\) (Chi-Quadrat) weist drei Probleme auf. Erstens ist \\(\\chi^2\\) (Chi-Quadrat) abhängig von den absoluten Häufigkeiten. Verdoppeln sich bspw. die Häufigkeiten, so verdoppelt sich auch \\(\\chi^2\\) (Chi-Quadrat). Die prozentuale Verteilung ändert sich jedoch nicht. Alternativ könnte der \\(\\chi^2\\) (Chi-Quadrat)-Wert normiert werden.\nFür die zwei unten erklärten berechneten Koeffizienten finden Sie auf der nächsten Seite Rechenbeispiele in Lernvideos.\nNur für 2x2-Kreuztabellen (wie im Beispiel) bietet sich \\(\\phi\\) (Phi) an. Das Ziel von \\(\\phi\\) (Phi) ist die Relativierung von \\(\\chi^2\\) (Chi-Quadrat) für die Anzahl von Beobachtungen. Je größer \\(\\phi\\) (Phi), desto stärker ist der Zusammenhang zwischen den beiden Variablen. \\(\\phi\\) (Phi) variiert zwischen \\(0\\) (min.) und \\(1\\) (max.) (\\(\\phi \\in [0;1]\\)).\nDie Formel zur Berechnung von \\(\\phi\\) (Phi) lautet:\n\\(\\phi = \\sqrt \\frac{\\chi^2}{n}\\), wobei \\(\\phi \\in [0;1]\\)\nIm vorherigen Beispiel ergibt sich daher:\n\\[\\begin{align*} \\phi \u0026amp;= \\sqrt \\frac{\\chi^2}{n} \\\\ \u0026amp;=\\sqrt \\frac{252.53}{1000} \\\\ \u0026amp;\\approx 0.503 \\end{align*}\\]\nZweiten ist \\(\\chi^2\\) (Chi-Quadrat) abhängig von der Kategorienanzahl pro Tabelle. Drittens lassen sich Kontingenzkoeffizienten aus Tabellen unterschiedlicher Größe schwierig miteinander vergleichen. Diese Fehler behebt die Berechnung von Cramer´s V. Formal wird der \\(\\chi^2\\) (Chi-Quadrat)-Wert durch den maximal erreichbaren \\(\\chi_{max}^2\\) (Chi-Quadrat)-Wert dividiert.\nFür Mehrfeldertabellen gilt:\n\\(\\chi_{max}^2 = n \\ast (R-1)\\), wobei \\(R\\) die minimale Spalten- bzw. Zeilenzahl ist.\nDie Formel für Cramer´s V ist daher:\n\\(V = \\sqrt \\frac {\\chi^2}{\\chi_{max}^2}= \\sqrt \\frac {\\chi^2}{n \\ast (min(k, m)-1)}\\), wobei \\(V \\in [0;1]\\), wobei \\(n\\): Anzahl Beobachtungen; \\(k\\): Anzahl Spalten; \\(m\\): Anzahl Zeilen\nDer Wertebereich von Cramer´s V beträgt \\(0\\) bis \\(1\\). Für die Interpretation der Werte gibt es Konventionen:\nInterpretation Cramer´s V Um die Probleme von \\(\\chi^2\\) (Chi-Quadrat) zu umgehen, wird am häufigsten Cramer’s V genutzt, da diese Maßzahl alle drei Probleme umgeht.\nIm Beispiel ergibt sich für uns folgender Wert von Cramer´s V:\nDie Anzahl der Beobachtungen ist: \\(n = 1000\\)\nDie Anzahl der Spalten ist: \\(k = 2\\)\nDie Anzahl der Zeilen ist: \\(m = 2\\)\nDa die Anzahl der Spalten und Zeilen gleich ist, können wir entweder \\(m\\) oder \\(k\\) benutzen. Ist die Anzahl nicht gleich, muss der kleinere Wert genutzt werden.\n\\[ \\begin{align*} V \u0026amp;= \\sqrt \\frac {\\chi^2}{\\chi_{max}^2} \\\\ \u0026amp;= \\sqrt \\frac {\\chi^2}{n \\ast (min(k, m)-1)} \\\\\u0026amp;= \\sqrt \\frac{252.53}{1000 \\ast (2-1)} \\\\ \u0026amp;\\approx 0.503 \\end{align*}\\]\nCramer´s V gibt einen Wert von \\(0,387\\) aus und damit laut obigen Konventionen einen mittleren Zusammenhang zwischen den zwei Variablen. Im Beispiel ist der Wert von Cramer´s V dem Wert von \\(\\phi\\) (Phi) gleich, da es sich um eine 2x2-Tabelle handelt.\nSiehe auch: Gehring \u0026amp; Weins (2009, Kapitel 7.3.1); Bortz \u0026amp; Schuster (2010, Kapitel 10.3.4).\nIn diesem Lernvideo wird Ihnen die Berechnung von \\(\\chi^2\\) sowie der Koeffizienten \\(\\phi\\) und Cramers V anhand zweier Beispiel gezeigt.\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM3/chapter2/subchapter1/page-2-1-b/",
	"title": "Kontingenzkoeffizient",
	"tags": ["kontingenzkoeffizient"],
	"description": "",
	"content": " Der Kontingenzkoeffizient \\(C\\) ist ein weiteres auf \\(\\chi^2\\)-basiertes Zusammenhangsmaß. Es kann angewendet werden, wenn beide Variablen mindestens nominalskaliert sind. Idealerweise wird er erst angewendet, wenn beide Variablen mehr als drei Ausprägungen haben (mind. \\(3x3\\) Tabelle). Bei \\(2x2\\) Tabellen sollte stattdessen \\(\\phi\\) genutzt werden.\nDer Kontingenzkoeffizient \\(C\\) berechnet sich nach folgender Formel: \\(C = \\sqrt {\\frac {\\chi^2}{\\chi^2 + N}}\\)\nWichtig bei der Interpretation von \\(C\\) ist, dass der Maximalwert von C von der Zahl der Ausprägungen der Variablen abhängt. \\(C\\) kann also nur interpretiert werden, wenn auch der Wert von \\(C_{max}\\) berechnet wurde. Auch wird so der Vergleich verschiedener Werte von \\(C\\) erschwert.\nDie Formel zur Berechnung des Maximums lautet: \\(C_{max} = \\sqrt{\\frac{R-1}{R}}\\), wobei \\(R = min(k,m)\\) ist.\nKommen wir zu einem Beispiel zwischen dem Universitätswahl und Wohnort. Wir haben folgende Kreuztabelle zwischen beiden Variablen:\nBeispiel Kontingenzkoeffizient C Der Wert von \\(\\chi^2\\) beträgt: \\(\\chi^2 \\approx 545.89\\).\nDamit können wir nun den Kontigenzkoeffizienten \\(C\\) berechnen: \\[\\begin{align*} C \u0026amp;= \\sqrt{\\frac{\\chi^2}{\\chi^2 + N}} \\\\ \u0026amp;= \\sqrt{\\frac{545.89}{545.89 + 1150}} \\\\ \u0026amp;\\approx 0.567 \\end{align*}\\]\nDer maximale Wert beträgt bei dieser \\(3x3\\) Tabelle: \\[ \\begin{align*}C_{max} \u0026amp;= \\sqrt{\\frac{R - 1}{R}} \\\\ \u0026amp;= \\sqrt{\\frac{3-1}{3}} \\\\ \u0026amp;\\approx 0.667 \\end{align*}\\]\nIn diesem Beispiel ist also der Zusammenhang zwischen den zwei nominalskalierten Variablen Wohnort und Universitätswahl stark. Der errechnete Wert des Kontingenzkoeffizienten \\(C\\) kommt recht nah an den Wert von \\(C_{max}\\).\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM3/chapter2/subchapter1/page-2-1-c/",
	"title": "Lambda",
	"tags": ["lambda", "pre maß"],
	"description": "",
	"content": " Mit \\(\\lambda\\) ist ein PRE-Maß und wird angewendet, um den Zusammenhang zweier Merkmale, deren Skalenniveau mindestens nominal ist, in einer Kreuztabelle zu überprüfen.\nHierzu nehmen wir wieder das Beispiel von vorher auf (Wahl der CDU in Abhängigkeit des Wohnorts). Wichtig: Die Wahl der CDU war unsere abhängige Variable und der Wohnort die unabhängige Varible.\nBeispiel Kontingenztabelle Um \\(\\lambda\\) zu berechnen, benötigen wir drei Schritte:\nVorhersagefehler der abhängigen Variable ohne Kenntnis der unabhängigen Variable\nVorhersagefehler der abhängigen Variable mit Kenntnis der unabhängigen Variable\nErmittlung des PRE-Maßes\n1. Prognose des Wertes der abhängigen Variable ohne Kenntnis der unabhängigen Variable Hierzu suchen wir die Modalkategorie der zwei Kategorien: CDU und Nicht-CDU. In diesem Beispiel liegen für den Fall CDU 450 Beobachtungen vor und für den Fall Nicht-CDU 550. Die Modalkategorie ist daher Nicht-CDU. Der Vorhersagefehler \\(E_1\\) wird wie folgt berechnet: \\(E_1 = 1 - \\frac {f_{x_M}}{n}\\), wobei gilt \\(M = Modalkategorie\\)\nim Beispiel also:\n\\[\\begin{align*} E_1 \u0026amp;= 1-\\frac{f_{x_{Nicht-CDU}}}{n} \\\\ \u0026amp;= 1- \\frac{550}{1000} \\\\ \u0026amp;= 1- 0.55 \\\\ \u0026amp;= 0.45 \\end{align*}\\]\nDer Vorhersagefehler ohne Kenntnis der unabhängigen Variable beträgt also \\(E_1 = 0.45\\) also 45 %.\n2. Prognose des Wertes der abhängigen Variable mit Kenntnis der unabhängigen Variable Die Vorhersage unter Kenntnis des Zusammenhangs ist die Modalkategorie der abhängigen Variable in Abhängigkeit von den kategorien der unabhängigen Variablen. Pro Spalte wird also die Modalkategorie festgelegt und davon der Vorhersagefehler berechnet. Diese werden dann über alle Spalte summiert und ergeben \\(E_2\\). Der Vorhersagefehler berechnet sich daher wie folgt: \\(E_2 = \\sum_j \\frac{h_{.j}}{n} (1- \\frac{h_{Mj}}{h_{.j}})\\)\nim Beispiel also \\[ \\begin{align*} E_2 \u0026amp;= \\sum_j \\frac{h_{.j}}{n} (1- \\frac{h_{Mj}}{h_{.j}}) \\\\ \u0026amp;= (\\frac{500}{1000} (1-\\frac{400}{500})) + (\\frac{500}{1000}(1-\\frac{350}{500})) \\\\ \u0026amp;= 0.25 \\end{align*}\\]\nDer Vorhersagefehler unter Kenntnis der unabhängigen Variable beträgt also \\(E_2 = 0.25\\)\n3. Ermittlung des PRE-Maßes Nun können wir \\(\\lambda\\) berechnen: \\[ \\begin{align*} \\lambda \u0026amp;= \\frac{E_1 -E_2}{E_1} \\\\ \u0026amp;= \\frac{0.45 - 0.25}{0.45} \\\\ \u0026amp;= \\frac{0.20}{0.45} \\\\ \u0026amp;\\approx 0.444 \\end{align*}\\]\nErgebnis: Unter Kenntnis des Wohnorts verringert sich der Fehler bei der Prognose der Wahl der CDU um 44 %.\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM3/chapter2/subchapter2/",
	"title": "Ordinales Skalenniveau",
	"tags": ["zusammenhangsmaß", "ordinal"],
	"description": "",
	"content": "Kapitel 2.2 Ordinales Skalenniveau In diesem Kapitel werden Zusammenhangsmaße für Variablen mit ordinalem Skalenniveau vorgestellt.\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM3/chapter2/subchapter2/page-2-2-a/",
	"title": "Spearman&#39;s Rho",
	"tags": ["rho", "spearman", "lernvideo"],
	"description": "",
	"content": " Für den Zusammenhang zweier ordinaler Variablen kann man die Rangkorrelation nach Spearman (\\(\\rho\\)) benutzen. Voraussetzung hierfür sind die Monotonie des Zusammenhangs und mindestens ordinale Variablen des Zusammenhangs. Auch kann ein nicht-linearer, aber monotoner Zusammenhang zweier metrischer Variablen mit Spearmans \\(\\rho\\) analysiert werden.\nWie auch der Korrelationskoeffizient nach Pearson nimmt Spearmans \\(\\rho\\) Werte im Intervall \\([-1;+1]\\) an. Für den perfekt positiven Zusammenhang nimmt er die Maßzahl $+1 $ an, für den perfekt negativen den Wert \\(-1\\).\nFür die Berechnung von Spearmans \\(\\rho\\) müssen zuerst Rangwerte der Variablen \\(x\\) und \\(y\\) erstellt werden (\\(R(x)\\) und \\(R(y)\\)). Die Datenreihen werden der Größe nach sortiert.\nAls Besonderheit gilt bei Spearmans \\(\\rho\\), dass die Abstände äquidistant (wie bei metrischen Variablen) sein müssen. Es muss also gewährleistet sein, dass die Rangdifferenzen tatsächlich die inhaltlichen Differenzen abbilden.\nFür die Berechnung von geteilten Rängen werden Durchschnittsränge ermittelt. Sollte der Wert \\(1\\), die 1., 2. und 3. Position beinhalten, wäre der Rang für die drei Merkmalsausprägungen mit dem Wert \\(1\\) folgender: \\(Rang = \\frac {1} {3} \\ast (1+ 2 +3) = 2\\) . Anstatt des Mittelwertes wird in der Berechnung von Spearmans \\(\\rho\\) der Durchschnittsrang verwendet: \\(\\bar{R(x)}\\) oder \\(\\bar{R(y)}\\)\nFür die Berechnung kann eine vereinfachte Formel herangezogen werden. Hier werden die wahren Werte durch die Rangwerte ersetzt, und mit diesen Rangwerten erfolgt die Berechnung der Rangkorrelation. Die Berechnung von Spearmans \\(\\rho\\) beruht daher nur auf der Ordnung der Daten (\\(\u0026gt;\\), \\(\u0026lt;\\)).\nDie vereinfachte Formel zur Berechnung von Spearman´s \\(\\rho\\):\n\\(\\rho = 1 - \\frac {6 \\ast \\sum\\limits_{i=1}^n d^2_i}{n \\ast (n^2 -1)}\\), wobei \\(d_i = R(x_i) - R(y_i)\\) ist.\nDiese Formel gilt nur, wenn keine Rangbindungen auftreten oder höchstens 20 Prozent der Paare Rangbindung haben.\nBeispiele Sie können nun anhand zweier Beispiele (einmal im Lernvideo und ein weiteres unten im Text) die Berechnung der Spearman-Korrelation nachvollziehen.\nNehmen wir ein weiteres Beispiel: Wir messen die Note von Studierenden einer Statistik-Vorlesung. Wir wollen testen, inwieweit die Endnote (\\(x\\)) mit der Note des Zwischentests (\\(y\\)) korreliert. Da es sich um eine Beispielrechnung handelt, erheben wir zum leichteren Verständnis nur Daten von fünf Studierenden (ohne Rangbindung).\nRangkorrelation Diese Daten übertragen wir nun in eine geordnete Tabelle der Rangreihen und extrahieren daraus die sortierte Rangreihe nach der Endnote (\\(R(x_i)\\)). Hierbei benötigen wir nur noch die Rangreihen und nicht mehr die eigentlichen Notenpunkte im Zwischen- bzw. Endtest.\nRangkorrelation (sortiert nach Rang x) Diese Daten müssen wir nun einfach in unsere vereinfachte Formel einsetzen: \\(\\rho = 1 - \\frac {6 \\ast \\sum\\limits_{i=1}^n d^2_i} {n \\ast (n^2 - 1)}\\), wobei \\(d_i = R(x_i) - R(y_i)\\) ist.\n\\[ \\begin{align*} \\rho \u0026amp;= 1 - \\frac {6 \\ast [(1-2)^2 + (2-1)^2 + (3-3)^2 + (4-5)^2 + (5-4)^2]} {5 \\ast (5^2 -1)} \\\\ \u0026amp;= 1- 0.2 \\\\ \u0026amp;= 0.8 \\end{align*}\\]\nIn unserer Beispielrechnung erhalten wir ein \\(\\rho = 0,8\\). Die Endnote korreliert folglich sehr stark mit der Note des Zwischentests. Interpretativ heißt dies, dass Personen, die bereits im Zwischentest eine gute Note erreicht haben, auch eine gute Endnote erreicht haben.\nSiehe auch: Bortz \u0026amp; Schuster (2010, Kapitel 10.3.6); Behnke \u0026amp; Behnke (2006, Kapitel 14.3.2); Cleff (2011, Kapitel 4.4.1).\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM3/chapter2/subchapter2/page-2-2-b/",
	"title": "Gamma",
	"tags": ["gamma", "pre maß"],
	"description": "",
	"content": " \\(\\gamma\\) ist ein PRE-Maß und wird angewendet, um den Zusammenhang zweier Merkmale, deren Skalenniveau mindestens ordinal ist, in einer Kreuztabelle zu überprüfen. Hierbei werden die Zahl konkordanter Paare \\(C\\) (\\(x_i \u0026lt; x_j\\)) und diskonkordanter Paare \\(D\\) (\\(y_i \u0026lt; y_j\\)) in Relation gesetzt. Im Unterschied zu anderen Zusammenhangsmaßen bleiben sogenannte Ties außen vor. Ties werden auch Rangbindungen genannt. Diese liegen vor, wenn mehrere Fälle denselben Rang in der geordneten Datenstruktur vorliegen. Wenn wir zum Beispiel das Klausurergebnis einer Prüfung nehmen, werden mehrere Personen 8 Notenpunkte erhalten und erhalten dann in der geordneten Auflistung denselben Rang, da unter den Personen mit 8 Notenpunkten keine weitere Rangfolge zu bilden ist. Dies ist eine Schwäche von \\(\\gamma\\), da der Koeffizient so bei Vorliegen von ties einen zu hohen Wert ausgibt.\nDie Gesamtanzahl zweier Variablen wird wie folgt berechnet: $Anzahl , möglicher , Paare = $\nBeispiel Nehmen wir folgendes Beispiel des höchsten Bildungslevels und des gruppierten Einkommens:\nBeispiel Gamma Die konkordanten (\\(N_C\\) und diskonkordanten (\\(N_D\\)) Paare lassen sich aus der Kreuztabelle auslesen.\nDazu nehmen wir diese leicht veränderte Tabelle, in der wir pro Zelle einen Buchstaben eingetragen haben:\nBeispiel Gamma Die Paare sich wie folgt zählen: \\[\\begin{align*}N_C \u0026amp;= a(e + f) +b(f) \\\\ \u0026amp;= 500(400+300) + 200(300) \\\\ \u0026amp;= 410000 \\end{align*}\\]\n\\[\\begin{align*}N_D \u0026amp;= c(d+e) +b(d) \\\\ \u0026amp;= 100(200+400) + 200(200) \\\\ \u0026amp;= 100000 \\end{align*}\\]\nDie Gesamtzahl ergibt sich wie folgt: \\(Anzahl \\, möglicher \\, Paare = \\frac{N(N-1)}{2} = 1444150\\). Hier wird bereits deutlich, dass durch das Nicht-Beachten der Ties viele Wertepaare nicht berücksichtigt werden. Denn \\(N_C + N_D = 510000\\).\nDie Formel für \\(\\gamma\\) lautet wie folgt: \\(\\gamma = \\frac{N_C - N_D}{N_C + N_D}\\)\nIm Beispiel erhalten wir folgende Werte: \\[\\begin{align*}\\gamma \u0026amp;= \\frac{410000 - 100000}{410000+100000} \\\\ \u0026amp;= \\frac{310000}{510000} \\\\ \u0026amp;\\approx 0.608 \\end{align*}\\]\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM3/chapter2/subchapter3/",
	"title": "Metrisches Skalenniveau",
	"tags": ["metrisch", "zusammenhangsmaß"],
	"description": "",
	"content": "Kapitel 2.3 Metrisches Skalenniveau In diesem Kapitel werden Zusammenhangsmaße für Variablen mit metrischem Skalenniveau vorgestellt.\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM3/chapter2/subchapter3/page-2-3-a/",
	"title": "Pearson&#39;s r",
	"tags": ["pearsons r", "lernvideo", "metrisch"],
	"description": "",
	"content": " Die Korrelation nach Pearson ist ein Standardmaß für den Zusammenhang zwischen zwei metrischen Variablen. Eine weitere Voraussetzung für dieses Maß ist, dass der Zusammenhang linear und monoton ist.\nBei der Berechnung einer Korrelation nach Pearson sollte daher immer ein Scatterplot der zwei zu untersuchenden Variablen ausgegeben werden, um diese zwei Annahmen zu überprüfen. Dies wird im folgenden Lernvideo dargestellt. Ein weiteres Beispiel folgt im Text nach dem Lernvideo.\nDie Grafik zeigt einen Scatterplot zwischen Körpergröße in cm und Körpergewicht in kg. Man kann erkennen, dass der Zusammenhang linear und monoton ist.\nDer Korrelationskoeffizient für diese zwei Variablen beträgt \\(r = 0.801\\). Es besteht also ein starker positiver Zusammenhang. Da die Punkte (Fälle) nicht alle auf einer Linie liegen, weicht der Korrelationskoeffizient von \\(1\\) ab.\nWir können den Korrelationskoeffizienten wie folgt interpretieren: Je höher die Körpergröße ist, desto höher ist auch das Gewicht bzw. je höher das Gewicht ist, desto höher ist auch die Körpergröße.\nScatterplot zwischen Körpergröße und Körpergewicht Beispiel Kommen wir nun zur Berechnung des Korrelationskoeffizienten nach Pearson: Die Berechnung des Korrelationskoeffizienten beruht auf der Kovarianz der Variablen. Die Kovarianz ist ähnlich der Varianz, sie umfasst die quadrierten Abstände zum Mittelpunkt. Der Mittelpunkt ist hier nur nicht auf eine Variable bezogen, sondern man nimmt den bivariaten Schwerpunkt \\((\\bar{x}; \\bar{y})\\), der die zwei Mittelwerte beider Variablen darstellt. Die Korrelation gibt an, wie eng die Wertepaare zweier Variablen um eine fiktive Gerade liegen. Sie gibt nicht an, wie steil diese Gerade ist.\nDabei gilt für die Variablen \\(x\\) und \\(y\\):\n\\(X = (x_1, x_2, x_3, ..., x_n)\\) \\(Y = (y_1, y_2, y_3, ..., y_n)\\)\nDie Korrelation berechnet sich für Werte aus einer Grundgesamtheit wie folgt: \\[\\begin{align*}r\u0026amp;= \\frac {cov(x;y)}{s_x \\ast s_y} \\\\ \u0026amp;= \\frac { \\frac{1}{n} \\ast \\sum\\limits_{i=1} ^n (x_i - \\bar{x}) \\ast (y_i - \\bar{y})} {\\sqrt { \\frac {1} {n} \\ast \\sum \\limits_{i=1} ^n (x_i - \\bar{x})^2 ) \\ast \\frac {1} {n} \\ast \\sum \\limits_{i=1} ^n (y_i - \\bar{y})^2})} \\end{align*}\\]\nDabei ist die Kovarianz \\(cov(x;y)\\) als durchschnittliche Abweichung vom bivariaten Schwerpunkt zu verstehen, analog zur Varianz. \\[\\begin{align*}cov(x;y) \u0026amp;= \\frac {1} {n} \\ast \\sum \\limits_{i=1}^n (x_i - \\bar{x}) \\ast (y_i - \\bar{y}) \\\\ \u0026amp;= \\frac {1} {n} \\ast \\sum\\limits_{i=1}^n x_i \\ast y_i - \\bar{x} \\ast \\bar{y} \\end{align*}\\]\nBeispielrechnung: Im Beispiel wollen wir nun die Korrelation zwischen Alter (offene Abfrage) und Wahlpräferenz CDU (Skala 0 (sehr unwahrscheinlich) bis 100 (sehr wahrscheinlich)) berechnen. In diesem Rechenbeispiel verkleinern wir die Stichprobe zu Illustrationszwecken auf 5 Personen. Am Ende der Seite finden Sie die Berechnung auch in einem Lernvideo erklärt.\nDatenmatrix Alter und Präferenz CDU Das Scatterplot für diese Daten sieht wie folgt aus: Die blauen Punkte bilden unsere fünf Datenpunkte und die gestrichelte Linie ist bereits eine Trendlinie der Korrelation. Wir sehen, dass der Zusammenhang annähernd monoton und linear ist.\nScatterplot: Präferenz CDU und Alter Für die Berechnung der Korrelation benötigen wir entsprechend der Formel die Varianz der Variablen und die Kovarianz, wofür wir wiederum die Mittelwerte benötigen.\n1) Berechnung der Mittelwerte Für Alter: \\[\\begin{align*}\\bar{x} \u0026amp;= \\sum\\limits_{i=1}^5 \\frac {x_i} {5} \\\\ \u0026amp;= \\frac {19 + 34 + 27 + 56 + 69}{5} \\\\ \u0026amp;= 41 \\end{align*}\\]\nFür Präferenz CDU: \\[\\begin{align*}\\bar{y} \u0026amp;= \\sum\\limits_{i=1}^5 \\frac {y_i} {5} \\\\ \u0026amp;= \\frac {24 + 41 + 20 + 74 + 81}{5} \\\\ \u0026amp;= 48 \\end{align*}\\]\n2) Berechnung der Varianz und Standardabweichung von x und y: \\[\\begin{align*}s_{x}^2 \u0026amp;= \\frac {1}{n} \\ast \\sum\\limits_{i=1}^n (x_i - \\bar{x})^2 \\\\ \u0026amp;= \\frac {1} {5} \\ast [(19-41)^2 + (34-41)^2 + (27-41)^2 + (56-41)^2 + (69-41)^2] \\\\ \u0026amp;= 347.6 \\end{align*}\\]\n\\[\\begin{align*}s_x \u0026amp;= \\sqrt s^2_{x} \\\\ \u0026amp;= \\sqrt 347.6 \\\\ \u0026amp;\\approx 18.64 \\end{align*}\\]\n\\[\\begin{align*}s_{y}^2 \u0026amp;= \\frac {1}{n} \\ast \\sum\\limits_{i=1}^n (y_i - \\bar{y})^2 \\\\ \u0026amp;= \\frac {1} {5} \\ast [(24-48)^2 + (41-48)^2 + (20-48)^2 + (74-48)^2 + (81-48)^2] \\\\ \u0026amp;= 634.8 \\end{align*}\\]\n\\[\\begin{align*}s_y \u0026amp;= \\sqrt s^2_{y} \\\\ \u0026amp;= \\sqrt 634.8 \\\\ \u0026amp;\\approx 25.20 \\end{align*}\\]\n3) Nun muss noch die Kovarianz berechnet werden: \\[\\begin{align*}cov(x;y) \u0026amp;= \\frac {1} {n} \\sum\\limits_{i=1}^n (x_i - \\bar{x}) \\ast (y_i - \\bar{y}) \\\\ \u0026amp;= \\frac {1}{5} \\ast [((19-41) \\ast (24-48)) + ((34-41) \\ast (41-48)) + ((27-41) \\ast (20-48)) + ((56-41) \\ast (74-48)) + ((69-41) \\ast (81-48))] \\\\ \u0026amp;= \\frac {1}{5} \\ast (528+49+392+390+924) \\\\ \u0026amp;= 456.6 \\end{align*}\\]\n4) Einsetzen in Formel für Pearson’s r \\[\\begin{align*} r \u0026amp;= \\frac {cov(x;y)}{s_x \\ast s_y} \\\\ \u0026amp;= \\frac {456.6} {18.64 \\ast 25.20} \\\\ \u0026amp;\\approx 0.972 \\end{align*}\\]\nIn diesem Beispiel ist also die Korrelation zwischen den Variablen Alter und Präferenz für die CDU positiv und bildet einen sehr starken Zusammenhang aus \\(r=0.972\\).\nSiehe auch: Gehring \u0026amp; Weins (2009, Kapitel 7.6); Behnke \u0026amp; Behnke (2006, Kapitel 14.4); Cleff (2011, Kapitel 4.3.2)\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM3/chapter2/subchapter3/page-2-3-b/",
	"title": "Linearität &amp; Monotonie",
	"tags": ["linearität", "monotonie"],
	"description": "",
	"content": " Bei der Berechnung der Korrelation nach Pearson müssen zwei Bedingungen erfüllt sein: Der Zusammenhang muss linear und monoton sein.\nDie folgende Abbildung zeigt einen Scatterplot eines nicht linearen und nicht monotonen Zusammenhangs zweier Variablen. Beides kann auf Sicht in einem Scatterplot erkannt werden. Das Verfahren der Korrelationsberechnung erkennt dies nicht.\nNicht-monotone Funktion Bis zum \\(x\\)-Wert \\(0\\) steigen die \\(y\\)-Werte und ab dann sinken die \\(y\\)-Werte für steigendes \\(x\\) (Nicht-Monotonie, Nicht-Linearität). Die Art der Beziehung zwischen den Variablen verändert sich mit steigendem \\(x\\) (erst steigt auch \\(y\\), danach sinkt \\(y\\)). Die Grundbedingung der Pearson-Korrelation ist nicht gegeben. Dies wird auch deutlich am Wert der Pearson-Korrelation: Dieser wird in diesem Beispiel mit \\(r=0\\) angegeben. Demnach dürfte kein Zusammenhang zwischen den zwei Variablen vorliegen, aber die Grafik zeigt sehr deutlich, dass ein Zusammenhang vorliegt, der perfekt ist (Punkte bilden eine Linie), aber nicht monoton und nicht linear. So kommt es zu einem falschen Korrelationswert bei der Berechnung der Pearson-Korrelation.\nEin weiteres Beispiel verdeutlicht die Verletzung der Linearität. In der Grafik sehen Sie einen Scatterplot zwischen Nettoeinkommen (offene Abfrage in Euro) und den Arbeitsstunden pro Woche (offene Abfrage). Der ausgegebene Pearson-Korrelationskoeffizient wäre für diese Beispiel bei \\(r=0.388\\). Damit würde ein schwacher Zusammenhang vorliegen.\nBeispiel nicht-linearer Zusammenhang In der Grafik lässt sich deutlich sehen, dass der Zusammenhang der beiden Variablen kurvilinear ist. Mit steigendem \\(x\\) steigt \\(y\\) zu Beginn sehr schnell, mit zunehmenden \\(x\\) nimmt allerdings die Steigung von \\(y\\) ab. Monotonie liegt dagegen vor, da \\(y\\) immer steigt.\nNehmen wir zur Verdeutlichung ein Beispiel: Wir betrachten den Zusammenhang zwischen Einkommen (\\(y\\)) und Wochenarbeitszeit (\\(x\\)). In diesem Beispiel würde bei einer geringen Wochenstundenzahl das Einkommen stärker steigen (bei einer Erhöhung um eine Stunde), als es dies bei einer bereits hohen Wochenstundenzahl tun würde.\nDie Punkte liegen recht eng zusammen, so dass der niedrige Wert des Korrelationskoeffizienten zusätzlich irreführend erscheint. Dies liegt an der Verletzung der Bedingung der Linearität.\nSollte der Zusammenhang zwischen zwei metrischen Variablen nicht linear sein, dann kann zur Berechnung der Korrelation Spearmans \\(\\rho\\) genutzt werden (oder eine der Variable muss transformiert werden, so dass der Zusammenhang linear ist).\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM3/chapter2/subchapter3/page-2-3-c/",
	"title": "Eta-Quadrat",
	"tags": ["eta-qudrat", "pre maß"],
	"description": "",
	"content": " Mit \\(\\eta^2\\) ist ein PRE-Maß und wird angewendet, um den Zusammenhang zweier Merkmale, von denen eine Variable ein metrisches Skalenniveau aufweist und die andere ein nominales Skalenniveau besitzt.\n\\(\\eta^2\\) beruht auf der Zerlegung der Quadratsummen: Wir können die Gesamt-Quadratsumme in zwei Teile zerlegen. Zum einen wird die Quadratsumme innerhalb der Gruppen (der nominalen oder ordinalen Variable) gebildet. Dies ist die Summe der quadrierten Abweichungen der Werte in den einzelnen Gruppen vom Gruppenmittelwert. Zum anderen kann die Summe der quadrierten Abweichungen der Gruppenmittelwerte vom Gesamtmittelwert gebildetet werden. Dies stellt die Quadratsumme zwischen den Gruppen dar.\nFür \\(\\eta^2\\) gilt folgendes: \\(\\eta^2 = \\frac {QS_{zwischen}}{QS_{gesamt}}= \\frac{QS_{zwischen}}{QS_{zwischen} + QS_{in}}\\)\nGibt es keine Unterschiede zwischen den Gruppen, so erreicht man auch bei Kenntnis der Gruppenzugehörigkeit keine Verbesserung der Vorhersage (\\(QS_{zwischen} = 0\\)). Je größer aber die Unterschiede zwischen den Gruppen (\\(QS_{zwischen} \u0026gt; 0\\)) relativ zu den Unterschieden innerhalb der Gruppen sind, desto besser wird die Vorhersage.\nFür die Interpretation von \\(\\eta^2\\) gelten folgende Daumenregeln (Cohen 1998):\n\\(\\eta^2 \u0026lt; 0.01\\) kein Zusammenhang\n\\(0.01 \\leq \\eta^2 \u0026lt; 0.04\\) geringer Zusammenhang\n\\(0.04 \\leq \\eta^2 \u0026lt; 0.16\\) mittlerer Zusammenhang und\n\\(0.16 \\leq \\eta^2\\) starker Zusammenhang\nNehmen wir dazu ein Rechenbeispiel auf: Wir möchten den Zusammenhang zwischen Zufriedenheit mit der Demokratie und dem Geschlecht prüfen. Zu Illustatrionszwecken nehmen wir folgendes Beispiel der Tabelle auf:\nBeispiel Eta Zur Berechnung von \\(\\eta^2\\) gehen wir in vier Schritten vor:\nMittelwerte berechnen\nquadrierte Abweichungen innerhalb der Gruppen berechnen und summieren (\\(QS_{in}\\))\nquadrierte Abweichungen der Gruppenmittelwerte vom Gesamtmittelwert berechnen und summieren (\\(QS_{zwischen}\\))\n\\(\\eta^2\\) berechnen\nMittelwerte berechnen\nFrauen \\(\\bar{x}_w = \\frac{5.5 + 4.5 + 5.2 + 4.9}{4} = 5.025\\) Männer \\(\\bar{x}_m = \\frac{7.2 + 8.1 + 7.8 + 8.4}{4} = 7.875\\) gesamt \\(\\bar{x} = \\frac{5.5+4.5+5.2+4.9+7.2+8.1+7.8+8.4}{8} = 6.45\\) quadrierte Abweichungen innerhalb der Gruppen berechnen und summieren (\\(QS_in\\)) Frauen \\(QS_{in_w}= (5.5 - 5.025)^2 + (4.5 - 5.025)^2 + ( 5.2 - 5.025)^2 + (4.9 - 5.025)^2 = 0.5475\\) Männer \\(QS_{in_m} = (7.2 - 7.875)^2 + (8.1-7.875)^2 +(7.8-7.875)^2 + (8.4-7.875)^2 = 0.7875\\) Summe \\(QS_{in} = QS_{in_w} + QS_{in_m} = 0.5475 + 0.7875 = 1.335\\)\nquadrierte Abweichungen der Gruppenmittelwerte vom Gesamtmittelwert berechnen und summieren (\\(QS_{zwischen}\\)) Frauen \\(n_w * (\\bar{x_w} - \\bar{x})^2= 4 * (5.025-6.45)^2 = 8.1225\\) Männer \\(n_m * (\\bar{x_m} - \\bar{x})^2= 4 * (7.875-6.45)^2 = 8.1225\\) Summe: \\(QS_{zwischen} = 8.1225 + 8.1225 = 16.245\\)\n\\(\\eta^2\\) berechnen \\(\\eta^2 = \\frac {QS_{zwischen}}{QS_{gesamt}} = \\frac {QS_{zwischen}}{QS_{zwischen} + QS_{in}}= \\frac{16.245}{16.245 + 1.335} \\approx 0.924\\) Wir haben also ein \\(\\eta^2 \\approx 0.924\\), das auf einen starken Zusammenhang hinweist. Dies liegt u.a. daran, dass wir in den Gruppen kaum Varianz vorliegen haben.\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM3/chapter3/",
	"title": "Verteilungen",
	"tags": ["verteilungen"],
	"description": "",
	"content": " Kapitel 3 Verteilungen In diesem Kapitel wird in statistische Verteilungen eingeführt. Der Unterschied zu Häufigkeitsverteilungen aus der deskriptiven Statistik liegt darin, dass in der schließenden Statistik die Verteilung für die Wahrscheinlichkeit von einzelnen Werten von Variablen steht. Mathematisch nennt man dies auch Verteilungfunktion. Die Verteilungsfunktion ist für diskrete und stetige Zufallsvariablen definiert, wird aber unterschiedlich berechnet. Bei diskreten Variablen erfolgt die Bildung der Verteilungsfunktion über die Wahrscheinlichkeitsfunktion, bei stetigen Variablen erfolgt die Bildung über die Dichtefunktion. Eine Zufallsvariable \\(X\\) ist dabei eine Funktion, die den Ergebnissen eines Zufallsexperiments (\\(\\omega \\in \\Omega\\)) reelle Zahlen (\\(\\mathbb{R}\\)) zuordnet.\nIm Folgenden werden die einzelnen Schritte zum Berechnen der Wahrscheinlichkeit einer diskreten Variable gezeigt und die Interpretation von Wahrscheinlichkeiten einer stetigen Variable dargestellt. Anschließend wird in die statistischen stetigen Verteilungsformen der Normal-, Standardnormal- und t-Verteilung eingeführt.\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM3/chapter3/page-3-a/",
	"title": "Normalverteilung",
	"tags": ["normalverteilung", "verteilung"],
	"description": "",
	"content": " Die Normalverteilung ist das wichtigste Verteilungsmodell der Statistik und wird für verschiedene Zwecke verwendet. Die Normalverteilung ist eine Verteilungsfunktion stetiger Variablen. Die Normalverteilung ist relevant als Verteilung empirischer Merkmale, als Verteilung von Stichprobenkennwerten und als Approximation anderer theoretischer Verteilungen (zentraler Grenzwertsatz, dazu im nächsten Kapitel mehr).\nDie Normalverteilung wird auch als Gauß-Verteilung beschrieben, da sie auf Carl Friedrich Gauß und Abraham de Moivre zurückgeht. Aufgrund der charakteristischen Form wird sie vereinfacht auch Glockenkurve genannt. Das Maximum einer Normalverteilung liegt dabei immer im Erwartungswert \\(\\mu\\) und die Fläche unterhalb der Kurve der Normalverteilung ist stets 1 (im Intervall \\([-\\infty; +\\infty]\\)). Die Dichtefunktion der Normalverteilung lautet: \\(f(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} e^{-\\frac{1}{2}(\\frac{x-\\mu}{\\sigma})^2}\\).\nEine Normalverteilung mit \\(\\bar{x}=0, \\sigma=1\\) entspricht daher folgender Abbildung:\nNormalverteilung (Mittelwert=0, Standardabweichung=1) Auf der \\(y\\)-Achse ist dabei die Häufigkeit abgetragen und auf der \\(x\\)-Achse die Werte einer Verteilung. Wie zu erkennen ist, teilt der Erwartungswert die Verteilung genau in der Hälfte und stellt auch den Modus, Mittelwert wie Median dar. Die Normalverteilung ist daher unimodal und symmetrisch.\nÄndert sich der Mittelwert der Normalverteilung, verschiebt sich diese nach links (\\(\\bar{x}\u0026lt;0\\)) bzw. nach rechts (\\(\\bar{x}\u0026gt;0\\)). Alle Normalverteilungen im Beispiel haben die selbe Standardabweichung (\\(\\sigma=1\\)).\nNormalverteilung mit unterschiedlichen Mittelwerten Ändern wir nicht den Mittelwert, sondern die Standardabweichung einer Normalverteilung, wird diese flacher (\\(\\sigma\u0026gt;1\\)) bzw. steiler (\\(0\u0026lt;\\sigma\u0026lt;1\\)). Für alle Normalverteilungen in diesem Beispiel gilt \\(\\mu = 0\\).\nNormalverteilung mit unterschiedlichen Standardabweichungen Die Normalverteilung hat in statistischen Tests eine wichtige Stellung. In statistischen Tests wird berechnet, wie wahrscheinlich ein Wert sich innerhalb eines Intervalls befindet und somit als „wahrscheinlich“/signifikant angenommen werden kann. Dazu wird über die Dichtefunktion der Normalverteilung die Verteilungsfunktion gebildet, so dass für Intervalle Wahrscheinlichkeiten berechnet werden können. Dazu ist es wichtig, die Verteilungsregel der Normalverteilung zu verinnerlichen. Für die Normalverteilung sind dabei folgende wichtige Flächen festzuhalten anhand der Standardabweichung festzuhalten.\nFlächen der Normalverteilung Bei einer Normalverteilung sind fast alle Werte innerhalb von drei Standardabweichungen erreicht.\nDies verdeutlicht die obere Grafik: Innerhalb des Intervalls von einer Standardabweichung (\\([-1, 0]\\)) befinden sich 68 % der Verteilung (\\(2*34 %\\)). In einem Intervall von +/- 2 Standardabweichungen (\\([-2, 2]\\)) befinden sich weitere 27 % (\\(2*13,5 %\\)), also insgesamt 95 %.\nInnerhalb eines Intervalls von +/- 3 Standardabweichungen (\\([-3, 3]\\)) befinden sich weitere 4,7 % (\\(2*2,35 %\\)), also insgesamt knapp 99,7 %.\nDas bedeutet, dass die Wahrscheinlichkeit, dass ein Wert \\(x\\) innerhalb des ersten Intervalls \\([\\mu - \\sigma, \\mu + \\sigma]\\) liegt, 0.68 beträgt (\\(P(\\mu - \\sigma \\leq x \\leq \\mu + \\sigma) \\approx 0.68\\)).\nFür das Intervall \\([\\mu - 2\\sigma, \\mu + 2\\sigma]\\) beträgt die Wahrscheinlichkeit eines Wertes \\(x\\) knapp 0,95 (\\(P(\\mu - 2\\sigma \\leq x \\leq \\mu + 2\\sigma) \\approx 0.95\\)).\nUnd innerhalb von drei Standardabweichungen beträgt die Wahrscheinlichkeit, dass ein Wert \\(x\\) innerhalb dieses Intervalls \\([\\mu - 3\\sigma, \\mu + 3\\sigma]\\) liegt, knapp 0,99 (\\(P(\\mu - 3\\sigma \\leq x \\leq \\mu + 3\\sigma) \\approx 0.99\\)). Nahezu alle Fälle liegen bei einer Normalverteilung innerhalb von drei Standardabweichungen.\nSiehe auch: Gehring \u0026amp; Weins (2009, Kapitel 10.3.1); Bortz \u0026amp; Schuster (2010, Kapitel 5.4); Behnke \u0026amp; Behnke (2006, Kapitel 19).\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM3/chapter3/page-3-b/",
	"title": "Standardnormalverteilung",
	"tags": ["standardnormalverteilung", "verteilung"],
	"description": "",
	"content": " Als besondere Form der Normalverteilung gibt es noch die Standardnormalverteilung: Diese liegt vor, wenn Erwartungswert \\(\\mu = 0\\) ist und die Varianz \\(\\sigma^2 = 1\\) und die Standardabweichung \\(\\sigma = 1\\) ist.\nMan erreicht eine Standardnormalverteilung durch eine Transformation der Rohwerte. Diese Transformation wird z-Transformation genannt. Mit Hilfe der z-Transformation kann auf die relative Position innerhalb einer Verteilung geschlossen werden. Der numerische Wert informiert über die Distanz zwischen dem Rohwert und dessen Mittelwert in der Einheit Standardabweichungen. Ein z-Wert von 1.33 bedeutet, dass der Rohwert 1.33 Standardabweichungen vom Mittelwert (\\(z\\)-Wert = 0) abweicht. Das Vorzeichen des z-Wertes gibt dabei an, ob sich der jeweilige Wert unterhalb (\\(-\\)) oder oberhalb (\\(+\\)) des Mittelwertes (\\(\\bar{x}\\)) befindet.\nGleichzeitig gehen aber Informationen über die absoluten Abstände zwischen den Messwerten verloren. Die z-Transformation weist immer die exakt gleiche Form wie die Originalverteilung auf.\nDie z-Transformation erfolgt nach der Formel: \\(z_{i} = \\frac { (x_{i} - \\bar{x})} {s}\\)\nVom entsprechenden \\(x\\)-Wert wird der Mittelwert (\\(\\bar{x}\\)) subtrahiert und das Ergebnis durch die Standardabweichung (\\(s\\)) dividiert. Abschließend erhält man dann eine z-transformierte Verteilung (Standardnormalverteilung) mit Mittelwert von \\(\\bar{x} = 0\\) und Standardabweichung von \\(s=1\\) sowie Varianz von \\(s^2=1\\).\nBei der Transformation wird vereinfacht gesagt, die Differenz eines gemessenen Wertes vom Mittelwert an der Standardabweichung relativiert wird. Dadurch erhält man die relative Position eines Wertes.\nDie Verteilungswerte der Standardnormalverteilung sind in nahezu jedem Statistikbuch abgedruckt und müssen für Hypothesentests (siehe Lernmodul 4) nicht einzeln berechnet werden.\nSiehe auch: Gehring \u0026amp; Weins (2009, Kapitel 10.3.1).\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM3/chapter3/page-3-c/",
	"title": "t-Verteilung",
	"tags": ["t-verteilung", "verteilungen"],
	"description": "",
	"content": " Eine weitere wichtige Verteilungsform neben Normal- und Standardnormalverteilung ist die t-Verteilung.\nDie t-Verteilung ist glockenförmig, aber flacher als die Normalverteilung. Sie wird insbesondere dann genutzt, wenn weniger als 30 Stichproben herangezogen werden oder wir die Varianz (\\(\\sigma^2\\)) nicht kennen. Dies wird insbesondere bei einigen Hypothesentests in der Statistik angewendet. Des Weiteren geht die t-Verteilung bei großen Fallzahlen in eine Normalverteilung über.\nDie t-Verteilung geht zurück auf William S. Gosset und stellt eine „genauere“ z-Verteilung dar. Ein Problem der Standardnormalverteilung liegt darin, dass mehr Annahmen getroffen werden, als in sozialwissenschaftlichen Forschungssituationen oftmals vorhanden sind. So ist die Standardabweichung zur Berechnung des Standardfehlers für die Grundgesamtheit meist nicht bekannt. Die „wahre Streuung“ wird daher aus der Stichprobenstreuung geschätzt, was zu Unsicherheit führt. Bei großen Stichproben ist dies unproblematisch, bei kleineren jedoch birgt dies Probleme. Daher wird bei kleineren Stichproben die t-Verteilung angenommen.\nDie Form der t-Verteilung hängt dabei von der Anzahl der Freiheitsgrade (df = degrees of freedom) ab, die wiederum die Stichprobengröße widerspiegelt. Die Freiheitsgerade berechnen sich aus der Stichprobengröße: \\(df=n - 1\\). Je höher die Anzahl der Freiheitsgrade, desto besser repräsentiert die Stichprobenvarianz die Populationsvarianz und desto besser repräsentiert die \\(t\\)-Statistik den entsprechenden \\(z\\)-Wert.\nDie erste der untenstehenden Grafiken zeigt eine t-Verteilung, bei der sich die Anzahl der Freiheitsgrade verändert. In der zweiten Grafik können Sie sehen, dass die t-Verteilung bereits ab 30 Freiheitsgraden bereits nahezu identisch mit der Standardnormalverteilung ist. Optisch ist kein Unterschied mehr erkennbar.\nt-Verteilung (Freiheitsgrade verändert) t-Verteilung und Standardnormalverteilung Siehe auch: Bortz \u0026amp; Schuster (2010, Kapitel 5.5.2)\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM3/chapter3/subchapter1/",
	"title": "Diskrete &amp; stetige Zufallsvariablen",
	"tags": ["diskret", "stetig", "zufallsvariable"],
	"description": "",
	"content": "In diesem Kapitel werden diskrete und stetige Zufallsvariablen sowie deren Verteilungsfunktionen vorgestellt.\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM3/chapter4/",
	"title": "Inferenzstatistik",
	"tags": ["inferenzstatistik"],
	"description": "",
	"content": "Kapitel 4 Inferenzstatistik In diesem Kapitel wird vorbereitend auf die Vorlesung Statistik für die Sozialwissenschaften II im nächsten Semester in die Inferenzstatistik eingeführt. In diesem Kapitel wird kurz in die Wahrscheinlichkeitsrechnung eingeführt und dann das Gesetz der großen Zahl eingeführt, das für zufallsbasierte Stichproben elementar ist. Ebenso wird in den zentralen Grenzwertsatz eingeführt, der ein Grund dafür ist, dass die Normalverteilung als Verteilungsform so präsent ist.\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM3/glossar/",
	"title": "Glossar",
	"tags": [],
	"description": "",
	"content": "Glossar Hier finden sich alle Begriffe aufgelistet, die im Lernmodul genutzt werden.\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM3/bibliography/",
	"title": "Literatur",
	"tags": [],
	"description": "",
	"content": " Literaturnachweise Behnke, J., \u0026amp; Behnke, N. (2006). Grundlagen der statistischen datenanalyse. Eine einführung für politikwissenschaftler. VS Verlag für Sozialwissenschaften. Bortz, J. (2005). Statistik für human- und sozialwissenschaftler (6th ed.). Springer Medizin Verlag. Bortz, J., \u0026amp; Schuster, C. (2010). Statistik für human- und sozialwissenschaftler (7th ed.). Springer-Lehrbuch. Cleff, T. (2011). Deskriptive statistik und moderne datenanalyse: Eine copmutergestützte einführung mit excel, PASW (SPSS) und STATA (2nd ed.). Springer Fachmedien. Diekmann, A. (2008). Empirische sozialforschung. Grundlagen, methoden, anwendungen (19th ed.). Rowohlt Taschenbuch Verlag. Fromm, S. (2010). Datenanalyse mit SPSS für fortgeschrittene 2: Multivariate verfahren für querschnittsdaten. VS Verlag für Sozialwissenschaften. Gehring, U. W., \u0026amp; Weins, C. (2009). Grundkurs statistik für politologen und soziologen (5th ed.). VS Verlag für Sozialwissenschaften. Janssen, J., \u0026amp; Laatz, W. (2013). Statistische datenanalyse mit SPSS. Eine anwendungsorientierte einführung in das basissystem und das modul exakte tests (8th ed.). Springer Gabler. Kromrey, H., Roose, J., \u0026amp; Strübing, J. (2016). Empirische sozialforschung (13th ed.). UVK Verlagsgesellschaft mbH. Kühnel, S., \u0026amp; Krebs, D. (2010). Statistik für die sozialwissenschaften. Grundlagen, methoden, anwendungen (5th ed.). Rowohlt Taschenbuch Verlag. Schnell, R., Hill, P. B., \u0026amp; Esser, E. (2013). Methoden der empirischen sozialforschung (10th ed.). Oldenbourg Verlag. Urban, D., \u0026amp; Mayerl, J. (2011). Regressionsanalyse: Theorie, technik und anwendung (4th ed.). VS Verlag für Sozialwissenschaften. "
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM3/tags/beobachtete-h%C3%A4ufigkeiten/",
	"title": "beobachtete häufigkeiten",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM3/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM3/tags/chi-quadrat/",
	"title": "chi-quadrat",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM3/tags/cramers-v/",
	"title": "Cramers V",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM3/credits/",
	"title": "Credits",
	"tags": [],
	"description": "",
	"content": "Contributors Web-Based-Training (since v1.0) @B. Philipp Kleer 19 commits Contributors to hugo learner-theme Thanks to them for making Open Source Software a better place!\n@matcornic 158 commits @matalo33 48 commits @coliff 19 commits @lierdakil 16 commits @mdavids 10 commits @ozobi 5 commits @Xipas 5 commits @Alan-Cha 4 commits @pdelaby 4 commits @helfper 4 commits @Chris-Greaves 3 commits @mreithub 3 commits @massimeddu 3 commits @LinuxSuRen 3 commits @dptelecom 3 commits @willwade 3 commits @diemol 2 commits @denisvm 2 commits @hucste 2 commits @ImgBotApp 2 commits @jamesbooker 2 commits @jice-lavocat 2 commits @wikijm 2 commits @lfalin 2 commits @JianLoong 2 commits @armsnyder 1 commits @afilini 1 commits @MrAkaki 1 commits @AmirLavasani 1 commits @afs2015 1 commits @arifpedia 1 commits @berryp 1 commits @MrMoio 1 commits @ChrisLasar 1 commits @DCsunset 1 commits @IEvangelist 1 commits @fritzmg 1 commits @bogaertg 1 commits @geoffreybauduin 1 commits @giuliov 1 commits @haitch 1 commits @zeegin 1 commits @RealOrangeOne 1 commits @jared-stehler 1 commits @JohnBlood 1 commits @JohnAllen2tgt 1 commits @kamilchm 1 commits @gwleclerc 1 commits @lloydbenson 1 commits @massimocireddu 1 commits @sykesm 1 commits @nvasudevan 1 commits @nnja 1 commits @owulveryck 1 commits @654wak654 1 commits @PierreAdam 1 commits @qiwenmin 1 commits @ripienaar 1 commits @stou 1 commits @razonyang 1 commits @HontoNoRoger 1 commits @pocc 1 commits @EnigmaCurry 1 commits @taiidani 1 commits @exKAZUu 1 commits @Oddly 1 commits @sandrogauci 1 commits @shelane 1 commits @mbbx6spp 1 commits @swenzel 1 commits @tedyoung 1 commits @Thiht 1 commits @editicalu 1 commits @fossabot 1 commits @kamar535 1 commits @mtbt03 1 commits @ngocbichdao 1 commits @nonumeros 1 commits @pgorod 1 commits @proelbtn 1 commits And a special thanks to vjeantet for his work on docdock, a fork of hugo-theme-learn. v2.0.0 of this theme is inspired by his work.\nPackages and libraries mermaid - generation of diagram and flowchart from text in a similar manner as markdown font awesome - the iconic font and CSS framework jQuery - The Write Less, Do More, JavaScript Library lunr - Lunr enables you to provide a great search experience without the need for external, server-side, search services\u0026hellip; horsey - Progressive and customizable autocomplete component clipboard.js - copy text to clipboard highlight.js - Javascript syntax highlighter modernizr - A JavaScript toolkit that allows web developers to use new CSS3 and HTML5 features while maintaining a fine level of control over browsers that don\u0026rsquo;t support KaTeX - rendering LaTeX math chunks Tooling Netlify - Continuous deployement and hosting of this documentation Hugo "
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM3/tags/diskret/",
	"title": "diskret",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM3/tags/erwartete-h%C3%A4ufigkeiten/",
	"title": "erwartete häufigkeiten",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM3/tags/eta-quadrat/",
	"title": "eta-quadrat",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM3/tags/eta-qudrat/",
	"title": "eta-qudrat",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM3/tags/gamm/",
	"title": "gamm",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM3/tags/gamma/",
	"title": "gamma",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM3/tags/h%C3%A4ufigkeiten/",
	"title": "häufigkeiten",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM3/tags/inferenzstatistik/",
	"title": "inferenzstatistik",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM3/tags/kontingenzkoeffizient/",
	"title": "kontingenzkoeffizient",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM3/tags/korrelation/",
	"title": "korrelation",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM3/tags/kreuztabellen/",
	"title": "kreuztabellen",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM3/tags/lambd/",
	"title": "lambd",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM3/tags/lambda/",
	"title": "lambda",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM3/",
	"title": "Lernmodul 3",
	"tags": [],
	"description": "",
	"content": "Lernmodul 3: Zusammenhangsmaße, statistische Verteilungen \u0026amp; Inferenzstatistik In diesem Lernmodul werden Ihnen verschiedene Zusammenhangsmaße vorgestellt (Kapitel 2). Des Weiteren wird in die statistischen Verteilungen eingeführt (Kapitel 3) und abschließend erfolgt eine Einführung in die Inferenzstatistik (Kapitel 4).\nInhalt des Lernmoduls: Über diesen Button kannst du Fehler auf Seiten direkt an uns melden. Dazu musst du dich einmalig mit der JLU-Kennung anmelden. Du findest den Button auf jeder Seite.\n"
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM3/tags/lernvideo/",
	"title": "lernvideo",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM3/tags/linearit%C3%A4t/",
	"title": "linearität",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM3/tags/metrisch/",
	"title": "metrisch",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM3/tags/monotonie/",
	"title": "monotonie",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM3/tags/nominal/",
	"title": "nominal",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM3/tags/normalverteilung/",
	"title": "normalverteilung",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM3/tags/ordinal/",
	"title": "ordinal",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM3/tags/pearsons-r/",
	"title": "pearsons r",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM3/tags/phi-koeffizient/",
	"title": "phi-Koeffizient",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM3/tags/pre-ma%C3%9F/",
	"title": "pre maß",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM3/tags/pre-ma%C3%9Fe/",
	"title": "pre-maße",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM3/tags/rho/",
	"title": "rho",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM3/tags/spearman/",
	"title": "spearman",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM3/tags/standardnormalverteilung/",
	"title": "standardnormalverteilung",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM3/tags/stetig/",
	"title": "stetig",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM3/tags/t-verteilung/",
	"title": "t-verteilung",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM3/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM3/tags/unabh%C3%A4ngigkeitstest/",
	"title": "unabhängigkeitstest",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM3/tags/verteilung/",
	"title": "verteilung",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM3/tags/verteilungen/",
	"title": "verteilungen",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM3/tags/zufallsvariable/",
	"title": "zufallsvariable",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM3/tags/zusammenhangsma%C3%9F/",
	"title": "zusammenhangsmaß",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lehre.bpkleer.de/stats101/LM3/tags/zusammenhangsma%C3%9Fe/",
	"title": "zusammenhangsmaße",
	"tags": [],
	"description": "",
	"content": ""
}]